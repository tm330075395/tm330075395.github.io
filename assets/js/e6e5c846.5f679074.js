"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[5755],{88285:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>r,contentTitle:()=>d,default:()=>o,frontMatter:()=>t,metadata:()=>l,toc:()=>p});var i=s(74848),_=s(28453);const t={sidebar_position:6},d="AI Demo\u8bf4\u660e\u6587\u6863",l={id:"CanaanK230/part5/AIDemodocumentation",title:"AI Demo\u8bf4\u660e\u6587\u6863",description:"1. AI Demo\u5f00\u53d1\u6846\u67b6\u4ecb\u7ecd",source:"@site/docs/CanaanK230/part5/06_AIDemodocumentation.md",sourceDirName:"CanaanK230/part5",slug:"/CanaanK230/part5/AIDemodocumentation",permalink:"/docs/CanaanK230/part5/AIDemodocumentation",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/CanaanK230/part5/06_AIDemodocumentation.md",tags:[],version:"current",sidebarPosition:6,frontMatter:{sidebar_position:6},sidebar:"canaanK230Sidebar",previous:{title:"4.ramdisk\u4f7f\u7528\u8bf4\u660e",permalink:"/docs/CanaanK230/part5/ramdiskinstructionsforuse"},next:{title:"2.\u4eba\u8138\u68c0\u6d4b",permalink:"/docs/CanaanK230/part5/Facedetection"}},r={},p=[{value:"1. AI Demo\u5f00\u53d1\u6846\u67b6\u4ecb\u7ecd",id:"1-ai-demo\u5f00\u53d1\u6846\u67b6\u4ecb\u7ecd",level:2},{value:"1.1. AI Demo\u5f00\u53d1\u6846\u67b6",id:"11-ai-demo\u5f00\u53d1\u6846\u67b6",level:3},{value:"1.2. \u63a5\u53e3\u4ecb\u7ecd",id:"12-\u63a5\u53e3\u4ecb\u7ecd",level:3},{value:"1.2.1. PipeLine",id:"121-pipeline",level:4},{value:"1.2.2. Ai2d",id:"122-ai2d",level:4},{value:"1.2.3. AIBase",id:"123-aibase",level:4},{value:"1.2.4. ScopedTiming",id:"124-scopedtiming",level:4},{value:"1.3. \u5e94\u7528\u65b9\u6cd5\u548c\u793a\u4f8b",id:"13-\u5e94\u7528\u65b9\u6cd5\u548c\u793a\u4f8b",level:3},{value:"1.3.1. \u6982\u8ff0",id:"131-\u6982\u8ff0",level:4},{value:"1.3.2. \u5355\u6a21\u578b\u4efb\u52a1",id:"132-\u5355\u6a21\u578b\u4efb\u52a1",level:4},{value:"1.3.3. \u81ea\u5b9a\u4e49\u9884\u5904\u7406\u4efb\u52a1",id:"133-\u81ea\u5b9a\u4e49\u9884\u5904\u7406\u4efb\u52a1",level:4},{value:"1.3.4. \u65e0\u9884\u5904\u7406\u4efb\u52a1",id:"134-\u65e0\u9884\u5904\u7406\u4efb\u52a1",level:4},{value:"1.3.5. \u591a\u6a21\u578b\u4efb\u52a1",id:"135-\u591a\u6a21\u578b\u4efb\u52a1",level:4},{value:"1.4. \u53c2\u8003\u6587\u6863",id:"14-\u53c2\u8003\u6587\u6863",level:3},{value:"1.4.1. k230 canmv\u6587\u6863",id:"141-k230-canmv\u6587\u6863",level:4},{value:"1.4.2. Ulab\u5e93\u652f\u6301",id:"142-ulab\u5e93\u652f\u6301",level:4},{value:"2. AI Demo",id:"2-ai-demo",level:2},{value:"2.1. \u52a8\u6001\u624b\u52bf\u8bc6\u522b",id:"21-\u52a8\u6001\u624b\u52bf\u8bc6\u522b",level:3},{value:"2.2. \u6ce8\u89c6\u4f30\u8ba1",id:"22-\u6ce8\u89c6\u4f30\u8ba1",level:3},{value:"2.3. \u4eba\u8138\u68c0\u6d4b",id:"23-\u4eba\u8138\u68c0\u6d4b",level:3},{value:"2.4. \u4eba\u8138\u5173\u952e\u90e8\u4f4d",id:"24-\u4eba\u8138\u5173\u952e\u90e8\u4f4d",level:3},{value:"2.5. \u4eba\u81383D\u7f51\u7edc",id:"25-\u4eba\u81383d\u7f51\u7edc",level:3},{value:"2.6. \u4eba\u8138\u89e3\u6790",id:"26-\u4eba\u8138\u89e3\u6790",level:3},{value:"2.7. \u4eba\u8138\u59ff\u6001",id:"27-\u4eba\u8138\u59ff\u6001",level:3},{value:"2.8. \u4eba\u8138\u8bc6\u522b",id:"28-\u4eba\u8138\u8bc6\u522b",level:3},{value:"2.9. \u4eba\u8138\u6ce8\u518c",id:"29-\u4eba\u8138\u6ce8\u518c",level:3},{value:"2.10. \u8dcc\u5012\u68c0\u6d4b",id:"210-\u8dcc\u5012\u68c0\u6d4b",level:3},{value:"2.11. \u731c\u62f3\u6e38\u620f",id:"211-\u731c\u62f3\u6e38\u620f",level:3},{value:"2.12. \u624b\u638c\u68c0\u6d4b",id:"212-\u624b\u638c\u68c0\u6d4b",level:3},{value:"2.13. \u624b\u638c\u5173\u952e\u70b9\u5206\u7c7b",id:"213-\u624b\u638c\u5173\u952e\u70b9\u5206\u7c7b",level:3},{value:"2.14. \u624b\u638c\u5173\u952e\u70b9\u68c0\u6d4b",id:"214-\u624b\u638c\u5173\u952e\u70b9\u68c0\u6d4b",level:3},{value:"2.15. \u624b\u52bf\u8bc6\u522b",id:"215-\u624b\u52bf\u8bc6\u522b",level:3},{value:"2.16 \u5173\u952e\u8bcd\u5524\u9192",id:"216-\u5173\u952e\u8bcd\u5524\u9192",level:3},{value:"2.17. \u8f66\u724c\u68c0\u6d4b",id:"217-\u8f66\u724c\u68c0\u6d4b",level:3},{value:"2.18. \u8f66\u724c\u8bc6\u522b",id:"218-\u8f66\u724c\u8bc6\u522b",level:3},{value:"2.19. \u5355\u76ee\u6807\u8ddf\u8e2a",id:"219-\u5355\u76ee\u6807\u8ddf\u8e2a",level:3},{value:"2.20. yolov8n\u76ee\u6807\u68c0\u6d4b",id:"220-yolov8n\u76ee\u6807\u68c0\u6d4b",level:3},{value:"2.21. OCR\u68c0\u6d4b",id:"221-ocr\u68c0\u6d4b",level:3},{value:"2.22. OCR\u8bc6\u522b",id:"222-ocr\u8bc6\u522b",level:3},{value:"2.23. \u4eba\u4f53\u68c0\u6d4b",id:"223-\u4eba\u4f53\u68c0\u6d4b",level:3},{value:"2.24. \u4eba\u4f53\u5173\u952e\u70b9\u68c0\u6d4b",id:"224-\u4eba\u4f53\u5173\u952e\u70b9\u68c0\u6d4b",level:3},{value:"2.25. \u62fc\u56fe\u6e38\u620f",id:"225-\u62fc\u56fe\u6e38\u620f",level:3},{value:"2.26. yolov8\u5206\u5272",id:"226-yolov8\u5206\u5272",level:3},{value:"2.27. \u81ea\u5b66\u4e60",id:"227-\u81ea\u5b66\u4e60",level:3},{value:"2.28. \u5c40\u90e8\u653e\u5927\u5668",id:"228-\u5c40\u90e8\u653e\u5927\u5668",level:3},{value:"2.29. \u6587\u672c\u8f6c\u8bed\u97f3\uff08\u4e2d\u6587\uff09",id:"229-\u6587\u672c\u8f6c\u8bed\u97f3\u4e2d\u6587",level:3}];function a(e){const n={a:"a",blockquote:"blockquote",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",img:"img",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,_.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"ai-demo\u8bf4\u660e\u6587\u6863",children:"AI Demo\u8bf4\u660e\u6587\u6863"})}),"\n",(0,i.jsx)(n.h2,{id:"1-ai-demo\u5f00\u53d1\u6846\u67b6\u4ecb\u7ecd",children:"1. AI Demo\u5f00\u53d1\u6846\u67b6\u4ecb\u7ecd"}),"\n",(0,i.jsx)(n.h3,{id:"11-ai-demo\u5f00\u53d1\u6846\u67b6",children:"1.1. AI Demo\u5f00\u53d1\u6846\u67b6"}),"\n",(0,i.jsx)(n.p,{children:"\u4e3a\u4e86\u5e2e\u52a9\u7528\u6237\u7b80\u5316AI\u90e8\u5206\u7684\u5f00\u53d1\uff0c\u57fa\u4e8eK230_CanMV\u63d0\u4f9b\u7684API\u63a5\u53e3\uff0c\u642d\u5efa\u4e86\u914d\u5957\u7684AI \u5f00\u53d1\u6846\u67b6\u3002\u6846\u67b6\u7ed3\u6784\u5982\u4e0b\u56fe\u6240\u793a\uff1a"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"\u5f00\u53d1\u6846\u67b6",src:s(24357).A+"",width:"1033",height:"495"})}),"\n",(0,i.jsx)(n.p,{children:"Camera\u9ed8\u8ba4\u51fa\u4e24\u8def\u56fe\u50cf\uff0c\u4e00\u8def\u683c\u5f0f\u4e3aYUV420\uff0c\u76f4\u63a5\u7ed9\u5230Display\u663e\u793a\uff1b\u53e6\u4e00\u8def\u683c\u5f0f\u4e3aRGB888\uff0c\u7ed9\u5230AI\u90e8\u5206\u8fdb\u884c\u5904\u7406\u3002AI\u4e3b\u8981\u5b9e\u73b0\u4efb\u52a1\u7684\u524d\u5904\u7406\u3001\u63a8\u7406\u548c\u540e\u5904\u7406\u6d41\u7a0b\uff0c\u5f97\u5230\u540e\u5904\u7406\u7ed3\u679c\u540e\u5c06\u5176\u7ed8\u5236\u5728osd image\u5b9e\u4f8b\u4e0a\uff0c\u5e76\u9001\u7ed9Display\u53e0\u52a0\u663e\u793a\u3002"}),"\n",(0,i.jsx)(n.h3,{id:"12-\u63a5\u53e3\u4ecb\u7ecd",children:"1.2. \u63a5\u53e3\u4ecb\u7ecd"}),"\n",(0,i.jsx)(n.h4,{id:"121-pipeline",children:"1.2.1. PipeLine"}),"\n",(0,i.jsx)(n.p,{children:"\u6211\u4eec\u5c06Media\u90e8\u5206\u7684\u4ee3\u7801\u5c01\u88c5\u5728PipeLine\u7c7b\u578b\u4e2d\uff0c\u901a\u8fc7\u56fa\u5b9a\u7684\u63a5\u53e3\u5b9e\u73b0\u6574\u4e2a\u6d41\u7a0b\u64cd\u4f5c\u3002"}),"\n",(0,i.jsx)(n.p,{children:"\u5176\u4e2dPipeLine\u7c7b\u63d0\u4f9b\u7684\u63a5\u53e3\u5305\u62ec\uff1a"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"\u521d\u59cb\u5316\u53c2\u6570\u5305\u62ec\uff1a"}),"\n",(0,i.jsx)(n.p,{children:"\uff081\uff09rgb888p_size\uff1alist\u7c7b\u578b\uff0c\u9884\u8bbe\u7ed9\u5230AI\u90e8\u5206\u7684\u56fe\u50cf\u5206\u8fa8\u7387\uff1b\u5982rgb888p_size=[1920,1080]\u3002"}),"\n",(0,i.jsx)(n.p,{children:"\uff082\uff09display_size\uff1alist\u7c7b\u578b\uff0c\u663e\u793a\u90e8\u5206Display\u7684\u5206\u8fa8\u7387\uff1b\u5982display_size=[1920,1080]\u3002"}),"\n",(0,i.jsx)(n.p,{children:"\uff083\uff09display_mode\uff1astr\u7c7b\u578b\uff0c\u663e\u793a\u6a21\u5f0f\uff0c\u5305\u62ec\u201dhdmi\u201c\u548c\u201dlcd\u201c\uff1b\u5982display_mode=\u201dhdmi\u201c\u3002"}),"\n",(0,i.jsx)(n.p,{children:"\uff084\uff09debug_mode\uff1aint\u7c7b\u578b\uff0c\u8017\u65f6\u8c03\u8bd5\u6a21\u5f0f\uff0c\u5982\u679c\u5927\u4e8e0\uff0c\u6253\u5370\u64cd\u4f5c\u8017\u65f6\uff1b\u5982debug_mode=0\u3002"}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"create(sensor=None,hmirror=None,vfilp=None)\uff1a"}),"\n",(0,i.jsx)(n.p,{children:"\uff081\uff09sensor\uff1a\u53c2\u6570\u4e3a\u53ef\u9009\u53c2\u6570\uff0c\u7c7b\u578b\u4e3aSensor\u5bf9\u8c61\uff0c\u53ef\u81ea\u4e3b\u914d\u7f6e\u73b0\u6709CanMV\u300101Studio\u548ck230d zero\u5f00\u53d1\u677f\u5b9e\u73b0\u4e86\u81ea\u52a8\u63a2\u6d4b\uff0c\u53ef\u4ee5\u9ed8\u8ba4\u4f7f\u7528create()\u5b9e\u73b0\u3002"}),"\n",(0,i.jsx)(n.p,{children:"\uff082\uff09hmirror\uff1a\u9ed8\u8ba4\u4e3aNone\uff0c\u5f53\u4e3b\u52a8\u8bbe\u7f6e\u65f6\u4e3abool\u7c7b\u578b\uff08True/False\uff09\uff0c\u8868\u793a\u662f\u5426\u5b9e\u73b0\u6c34\u5e73\u65b9\u5411\u955c\u50cf\u663e\u793a\u3002"}),"\n",(0,i.jsx)(n.p,{children:"\uff083\uff09vflip: \u9ed8\u8ba4\u4e3aNone\uff0c\u5f53\u4e3b\u52a8\u8bbe\u7f6e\u65f6\u4e3abool\u7c7b\u578b\uff08True/False\uff09\uff0c\u8868\u793a\u662f\u5426\u5b9e\u73b0\u5782\u76f4\u65b9\u5411\u7ffb\u8f6c\u3002"}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"get_frame()\uff1a\u8fd4\u56de\u4e00\u5e27ulab.numpy.ndarray\u7c7b\u578b\u56fe\u50cf\u6570\u636e\uff0c\u5206\u8fa8\u7387\u4e3argb888p_size\uff0c\u6392\u5e03\u4e3aCHW\u3002"}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"show_image()\uff1aPipeLine\u5b9e\u4f8b\u4e2d\u9884\u8bbe\u4e00\u5e27OSD\u56fe\u50cf\uff0c\u8be5\u63a5\u53e3\u5c06\u6210\u5458\u53d8\u91cfosd_img\u663e\u793a\u5728\u5c4f\u5e55\u4e0a\u3002"}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"destroy()\uff1a\u9500\u6bc1PipeLine\u5b9e\u4f8b\u3002"}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"\u4e0b\u9762\u7ed9\u51fa\u65e0AI\u90e8\u5206\u7684\u793a\u4f8b\u4ee3\u7801\uff1a"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:'from libs.PipeLine import PipeLine, ScopedTiming\nfrom media.media import *\nimport gc\nimport sys,os\n\nif __name__ == "__main__":\n    # \u663e\u793a\u6a21\u5f0f\uff0c\u9ed8\u8ba4"hdmi",\u53ef\u4ee5\u9009\u62e9"hdmi"\u548c"lcd"\n    display_mode="hdmi"\n    if display_mode=="hdmi":\n        display_size=[1920,1080]\n    else:\n        display_size=[800,480]\n\n    # \u521d\u59cb\u5316PipeLine\uff0c\u7528\u4e8e\u56fe\u50cf\u5904\u7406\u6d41\u7a0b\n    pl = PipeLine(rgb888p_size=[1920,1080], display_size=display_size, display_mode=display_mode)\n    pl.create()  # \u521b\u5efaPipeLine\u5b9e\u4f8b\n    try:\n        while True:\n            os.exitpoint()                      # \u68c0\u67e5\u662f\u5426\u6709\u9000\u51fa\u4fe1\u53f7\n            with ScopedTiming("total",1):\n                img = pl.get_frame()            # \u83b7\u53d6\u5f53\u524d\u5e27\u6570\u636e\n                print(img.shape)\n                gc.collect()                    # \u5783\u573e\u56de\u6536\n    except Exception as e:\n        sys.print_exception(e)                  # \u6253\u5370\u5f02\u5e38\u4fe1\u606f\n    finally:\n        pl.destroy()                            # \u9500\u6bc1PipeLine\u5b9e\u4f8b\n'})}),"\n",(0,i.jsxs)(n.p,{children:["\u4e0a\u8ff0\u4ee3\u7801\u4e2d\uff0c\u901a\u8fc7",(0,i.jsx)(n.code,{children:"pl.get_frame()"}),"\u63a5\u53e3\u83b7\u53d6\u4e00\u5e27\u5206\u8fa8\u7387\u4e3argb888p_size\u7684\u56fe\u50cf\uff0c\u7c7b\u578b\u4e3aulab.numpy.ndarray\uff0c\u6392\u5e03\u4e3aCHW\u3002\u57fa\u4e8e\u4e0a\u9762\u7684\u4ee3\u7801\u5f97\u5230\u4e86\u4e00\u5e27\u56fe\u50cf\u7ed9AI\u5904\u7406\uff0c\u60a8\u53ef\u4ee5\u53ea\u5173\u6ce8AI\u63a8\u7406\u90e8\u5206\u7684\u64cd\u4f5c\u3002"]}),"\n",(0,i.jsx)(n.p,{children:"\u56fe\u50cfAI\u5f00\u53d1\u8fc7\u7a0b\u5305\u62ec\uff1a\u56fe\u50cf\u9884\u5904\u7406\u3001\u6a21\u578b\u63a8\u7406\u3001\u8f93\u51fa\u540e\u5904\u7406\u7684\u8fc7\u7a0b\uff0c\u6211\u4eec\u5c06\u6574\u4e2a\u8fc7\u7a0b\u5c01\u88c5\u5728Ai2d\u7c7b\u548cAIBase\u7c7b\u4e2d\u3002"}),"\n",(0,i.jsx)(n.h4,{id:"122-ai2d",children:"1.2.2. Ai2d"}),"\n",(0,i.jsx)(n.p,{children:"\u5bf9\u4e8eAi2d\u7c7b\uff0c\u6211\u4eec\u7ed9\u51fa\u4e86\u5e38\u89c1\u7684\u51e0\u79cd\u9884\u5904\u7406\u65b9\u6cd5\uff0c\u5305\u62eccrop/shift/pad/resize/affine\u3002\u8be5\u7c7b\u522b\u63d0\u4f9b\u7684\u63a5\u53e3\u5305\u62ec\uff1a"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"\u521d\u59cb\u5316\u53c2\u6570\u5305\u62ec\uff1a"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"\uff081\uff09debug_mode\uff1aint\u7c7b\u578b\uff0c\u8017\u65f6\u8c03\u8bd5\u6a21\u5f0f\uff0c\u5982\u679c\u5927\u4e8e0\uff0c\u6253\u5370\u64cd\u4f5c\u8017\u65f6\uff1b\u5982debug_mode=0\u3002"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"set_ai2d_dtype(input_format,output_format,input_type,output_type)"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"\uff081\uff09input_format\uff1aai2d\u9884\u5904\u7406\u8f93\u5165\u683c\u5f0f\u3002"}),"\n",(0,i.jsx)(n.p,{children:"\uff082\uff09output_format\uff1aai2d\u9884\u5904\u7406\u8f93\u51fa\u683c\u5f0f\u3002"}),"\n",(0,i.jsx)(n.p,{children:"\u8f93\u5165\u8f93\u51fa\u683c\u5f0f\u652f\u6301\u5982\u4e0b\u6240\u793a\uff1a"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"enum class ai2d_format\n{\n    YUV420_NV12 = 0,\n    YUV420_NV21 = 1,\n    YUV420_I420 = 2,\n    NCHW_FMT = 3,\n    RGB_packed = 4,\n    RAW16 = 5,\n}\n"})}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"\u8f93\u5165\u683c\u5f0f"}),(0,i.jsx)(n.th,{children:"\u8f93\u51fa\u683c\u5f0f"}),(0,i.jsx)(n.th,{children:"\u5907\u6ce8"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"YUV420_NV12"}),(0,i.jsx)(n.td,{children:"RGB_planar/YUV420_NV12"}),(0,i.jsx)(n.td,{})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"YUV420_NV21"}),(0,i.jsx)(n.td,{children:"RGB_planar/YUV420_NV21"}),(0,i.jsx)(n.td,{})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"YUV420_I420"}),(0,i.jsx)(n.td,{children:"RGB_planar/YUV420_I420"}),(0,i.jsx)(n.td,{})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"YUV400"}),(0,i.jsx)(n.td,{children:"YUV400"}),(0,i.jsx)(n.td,{})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"NCHW(RGB_planar)"}),(0,i.jsx)(n.td,{children:"NCHW(RGB_planar)"}),(0,i.jsx)(n.td,{})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"RGB_packed"}),(0,i.jsx)(n.td,{children:"RGB_planar/RGB_packed"}),(0,i.jsx)(n.td,{})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"RAW16"}),(0,i.jsx)(n.td,{children:"RAW16/8"}),(0,i.jsx)(n.td,{children:"\u6df1\u5ea6\u56fe\uff0c\u6267\u884cshift\u64cd\u4f5c"})]})]})]}),"\n",(0,i.jsx)(n.p,{children:"\uff083\uff09input_type\uff1a\u8f93\u5165\u6570\u636e\u7c7b\u578b\u3002"}),"\n",(0,i.jsx)(n.p,{children:"\uff084\uff09output_type\uff1a\u8f93\u51fa\u6570\u636e\u7c7b\u578b\u3002"}),"\n",(0,i.jsx)(n.p,{children:"\u4e0b\u9762\u662f\u63a5\u53e3\u8c03\u7528\u793a\u4f8b\uff1a"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"from libs.AI2D import Ai2d\nimport nncase_runtime as nn\n\nmy_ai2d=Ai2d(debug_mode=1)\nmy_ai2d.set_ai2d_type(nn.ai2d_format.NCHW_FMT, nn.ai2d_format.NCHW_FMT, np.uint8, np.uint8)\nmy_ai2d.set_ai2d_type(nn.ai2d_format.RGB_packed, nn.ai2d_format.NCHW_FMT, np.uint8, np.uint8)\n"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"crop(start_x,start_y,width,height)\uff1a\u9884\u5904\u7406crop\u51fd\u6570\u3002"}),"\n",(0,i.jsx)(n.p,{children:"\uff081\uff09start_x\uff1a\u5bbd\u5ea6\u65b9\u5411\u7684\u8d77\u59cb\u50cf\u7d20,int\u7c7b\u578b\uff1b"}),"\n",(0,i.jsx)(n.p,{children:"\uff082\uff09start_y: \u9ad8\u5ea6\u65b9\u5411\u7684\u8d77\u59cb\u50cf\u7d20,int\u7c7b\u578b\uff1b"}),"\n",(0,i.jsx)(n.p,{children:"\uff083\uff09width: \u5bbd\u5ea6\u65b9\u5411\u7684crop\u957f\u5ea6,int\u7c7b\u578b\uff1b"}),"\n",(0,i.jsx)(n.p,{children:"\uff084\uff09height: \u9ad8\u5ea6\u65b9\u5411\u7684crop\u957f\u5ea6,int\u7c7b\u578b\uff1b"}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"my_ai2d.crop(0,0,200,300)\n"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"shift(shift_val)\uff1a\u9884\u5904\u7406shift\u51fd\u6570\u3002"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"\uff081\uff09shift_val:\u53f3\u79fb\u7684\u6bd4\u7279\u6570,int\u7c7b\u578b;"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"my_ai2d.shift(2)\n"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"pad(paddings,pad_mode,pad_val)\uff1a\u9884\u5904\u7406padding\u51fd\u6570\u3002"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"\uff081\uff09paddings\uff1alist\u7c7b\u578b\uff0c\u5404\u7ef4\u5ea6\u4e24\u4fa7padding\u7684\u5927\u5c0f\uff0c\u5bf9\u4e8e4\u7ef4\u7684\u56fe\u50cf(NCHW)\uff0c\u8be5\u53c2\u6570\u5305\u542b8\u4e2a\u503c\uff0c\u5206\u522b\u8868\u793aN/C/H/W\u56db\u4e2a\u7ef4\u5ea6\u4e24\u4fa7\u7684padding\u5927\u5c0f\uff0c\u4e00\u822c\u53ea\u5728\u540e\u4e24\u4e2a\u7ef4\u5ea6\u505apadding\uff1b"}),"\n",(0,i.jsx)(n.p,{children:"\uff082\uff09pad_mode\uff1a\u53ea\u652f\u6301constant padding\uff0c\u76f4\u63a5\u8bbe\u4e3a0\uff1b"}),"\n",(0,i.jsx)(n.p,{children:"\uff083\uff09pad_val\uff1alist\u7c7b\u578b\uff0c\u6bcf\u4e2a\u50cf\u7d20\u4f4d\u7f6e\u586b\u5145\u7684\u503c\uff0c\u6bd4\u5982[114,114,114]\u3001[0,0,0]"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"my_ai2d.pad([0,0,0,0,5,5,15,15],0,[114,114,114])\n"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"resize(interp_method,interp_mode)\uff1a\u9884\u5904\u7406resize\u51fd\u6570\u3002"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"\uff081\uff09interp_method\uff1aresize\u63d2\u503c\u65b9\u6cd5\uff0cai2d_interp_method\u7c7b\u578b\uff0c\u5305\u62ec\uff1ann.interp_method.tf_nearest\u3001nn.interp_method.tf_bilinear\u3001nn.interp_method.cv2_nearest\u3001nn.interp_method.cv2_bilinear\uff1b"}),"\n",(0,i.jsx)(n.p,{children:"\uff082\uff09interp_mode\uff1aresize\u6a21\u5f0f\uff0cai2d_interp_mode\u7c7b\u578b\uff0c\u5305\u62ec\uff1ann.interp_mode.none\u3001nn.interp_mode.align_corner\u3001nn.interp_mode.half_pixel\uff1b"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"my_ai2d.resize(nn.interp_method.tf_bilinear, nn.interp_mode.half_pixel)\n"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"affine(interp_method,crop_round,bound_ind,bound_val,bound_smooth,M)\uff1a\u9884\u5904\u7406affine\u51fd\u6570\u3002"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"\uff081\uff09interp_method\uff1aAffine\u91c7\u7528\u7684\u63d2\u503c\u65b9\u6cd5,ai2d_interp_method\u7c7b\u578b\uff0c\u5305\u62ec\uff1ann.interp_method.tf_nearest\u3001nn.interp_method.tf_bilinear\u3001nn.interp_method.cv2_nearest\u3001nn.interp_method.cv2_bilinear\uff1b"}),"\n",(0,i.jsx)(n.p,{children:"\uff082\uff09cord_round:\u6574\u6570\u8fb9\u754c0\u6216\u80051,uint32_t\u7c7b\u578b\uff1b"}),"\n",(0,i.jsx)(n.p,{children:"\uff083\uff09bound_ind:\u8fb9\u754c\u50cf\u7d20\u6a21\u5f0f0\u6216\u80051,uint32_t\u7c7b\u578b\uff1b"}),"\n",(0,i.jsx)(n.p,{children:"\uff084\uff09bound_val:\u8fb9\u754c\u586b\u5145\u503c,uint32_t\u7c7b\u578b\uff1b"}),"\n",(0,i.jsx)(n.p,{children:"\uff085\uff09bound_smooth:\u8fb9\u754c\u5e73\u6ed10\u6216\u80051,uint32_t\u7c7b\u578b\uff1b"}),"\n",(0,i.jsx)(n.p,{children:"\uff086\uff09M:\u4eff\u5c04\u53d8\u6362\u77e9\u9635\u5bf9\u5e94\u7684vector\uff0c\u4eff\u5c04\u53d8\u6362\u4e3aY=[a_0, a_1; a_2, a_3] \\cdot X + [b_0, b_1] $, \u5219 M=[a_0,a_1,b_0,a_2,a_3,b_1 ],list\u7c7b\u578b\u3002"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"affine_matrix=[0.2159457, -0.031286, -59.5312, 0.031286, 0.2159457, -35.30719]\nmy_ai2d.affine(nn.interp_method.cv2_bilinear,0, 0, 127, 1,affine_matrix)\n"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"build(ai2d_input_shape,ai2d_output_shape)\uff1aai2d\u6784\u9020\u51fd\u6570\uff0c\u524d\u9762\u914d\u7f6e\u7684\u9884\u5904\u7406\u65b9\u6cd5\u8d77\u4f5c\u7528\u3002"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"\uff081\uff09ai2d_input_shape\uff1aai2d\u8f93\u5165shape\uff0clist\u7c7b\u578b\uff1b"}),"\n",(0,i.jsx)(n.p,{children:"\uff082\uff09ai2d_output_shape\uff1aai2d\u8f93\u51fashape\uff0clist\u7c7b\u578b\uff1b"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"my_ai2d.build([1,3,224,224],[1,3,512,512])\n"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"run(input_np)\uff1a\u8c03\u7528\u914d\u7f6e\u597d\u7684ai2d\u8fdb\u884c\u9884\u5904\u7406\u7684\u51fd\u6570\uff0c\u8fd4\u56de\u4e00\u4e2atensor\u7c7b\u578b\u6570\u636e\uff0c\u53ef\u4ee5\u76f4\u63a5\u7ed9\u6a21\u578b\u4f7f\u7528\uff0c\u4e5f\u53ef\u4ee5\u901a\u8fc7to_numpy()\u8f6c\u6362\u6210ulab.numpy.ndarray\u7c7b\u578b\u7684\u6570\u636e\u3002"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"\uff081\uff09input_np\uff1aulab.numpy.ndarray\u7c7b\u578b\uff0cai2d\u9884\u5904\u7406\u7684\u8f93\u5165\u6570\u636e\uff0cshape\u548cbuild\u51fd\u6570\u4e2d\u8bbe\u7f6e\u7684ai2d_input_shape\u4e00\u81f4\u3002"}),"\n",(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsx)(n.p,{children:"\u6ce8\u610f\uff1a"}),"\n",(0,i.jsx)(n.p,{children:"(1) Affine\u548cResize\u529f\u80fd\u662f\u4e92\u65a5\u7684\uff0c\u4e0d\u80fd\u540c\u65f6\u5f00\u542f\uff1b (2) Shift\u529f\u80fd\u7684\u8f93\u5165\u683c\u5f0f\u53ea\u80fd\u662fRaw16\uff1b (3) Pad value\u662f\u6309\u901a\u9053\u914d\u7f6e\u7684\uff0c\u5bf9\u5e94\u7684list\u5143\u7d20\u4e2a\u6570\u8981\u4e0echannel\u6570\u76f8\u7b49\uff1b (4) \u5f53\u914d\u7f6e\u4e86\u591a\u4e2a\u529f\u80fd\u65f6\uff0c\u6267\u884c\u987a\u5e8f\u662fCrop->Shift->Resize/Affine->Pad, \u914d\u7f6e\u53c2\u6570\u65f6\u6ce8\u610f\u8981\u5339\u914d\uff1b\u5982\u679c\u4e0d\u7b26\u5408\u8be5\u987a\u5e8f\uff0c\u9700\u8981\u521d\u59cb\u5316\u591a\u4e2aAi2d\u5b9e\u4f8b\u5b9e\u73b0\u9884\u5904\u7406\u8fc7\u7a0b\uff1b"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"\u4e0b\u9762\u662f\u4e00\u4e2a\u5b8c\u6574\u7684\u793a\u4f8b\uff1a"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:'from libs.PipeLine import PipeLine, ScopedTiming\nfrom libs.AI2D import Ai2d\nfrom media.media import *\nimport nncase_runtime as nn\nimport gc\nimport sys,os\n\nif __name__ == "__main__":\n    # \u663e\u793a\u6a21\u5f0f\uff0c\u9ed8\u8ba4"hdmi",\u53ef\u4ee5\u9009\u62e9"hdmi"\u548c"lcd"\n    display_mode="hdmi"\n    if display_mode=="hdmi":\n        display_size=[1920,1080]\n    else:\n        display_size=[800,480]\n\n    # \u521d\u59cb\u5316PipeLine\uff0c\u7528\u4e8e\u56fe\u50cf\u5904\u7406\u6d41\u7a0b\n    pl = PipeLine(rgb888p_size=[512,512], display_size=display_size, display_mode=display_mode)\n    pl.create()  # \u521b\u5efaPipeLine\u5b9e\u4f8b\n    my_ai2d=Ai2d(debug_mode=0) #\u521d\u59cb\u5316Ai2d\u5b9e\u4f8b \n    # \u914d\u7f6eresize\u9884\u5904\u7406\u65b9\u6cd5\n    my_ai2d.resize(nn.interp_method.tf_bilinear, nn.interp_mode.half_pixel)\n    # \u6784\u5efa\u9884\u5904\u7406\u8fc7\u7a0b\n    my_ai2d.build([1,3,512,512],[1,3,640,640])\n    try:\n        while True:\n            os.exitpoint()                      # \u68c0\u67e5\u662f\u5426\u6709\u9000\u51fa\u4fe1\u53f7\n            with ScopedTiming("total",1):\n                img = pl.get_frame()            # \u83b7\u53d6\u5f53\u524d\u5e27\u6570\u636e\n                print(img.shape)                # \u539f\u56feshape\u4e3a[1,3,512,512]\n                ai2d_output_tensor=my_ai2d.run(img) # \u6267\u884cresize\u9884\u5904\u7406\n                ai2d_output_np=ai2d_output_tensor.to_numpy() # \u7c7b\u578b\u8f6c\u6362\n                print(ai2d_output_np.shape)        # \u9884\u5904\u7406\u540e\u7684shape\u4e3a[1,3,640,640]\n                gc.collect()                    # \u5783\u573e\u56de\u6536\n    except Exception as e:\n        sys.print_exception(e)                  # \u6253\u5370\u5f02\u5e38\u4fe1\u606f\n    finally:\n        pl.destroy()                            # \u9500\u6bc1PipeLine\u5b9e\u4f8b\n'})}),"\n",(0,i.jsx)(n.h4,{id:"123-aibase",children:"1.2.3. AIBase"}),"\n",(0,i.jsx)(n.p,{children:"AIBase\u90e8\u5206\u5c01\u88c5\u4e86\u5b9e\u73b0\u6a21\u578b\u63a8\u7406\u7684\u4e3b\u8981\u63a5\u53e3\uff0c\u4e5f\u662f\u8fdb\u884cAI\u5f00\u53d1\u4e3b\u8981\u5173\u6ce8\u7684\u90e8\u5206\u3002\u7528\u6237\u9700\u8981\u6309\u7167\u81ea\u5df1demo\u7684\u8981\u6c42\u5b9e\u73b0\u524d\u5904\u7406\u548c\u540e\u5904\u7406\u90e8\u5206\u3002"}),"\n",(0,i.jsx)(n.p,{children:"AIBase\u63d0\u4f9b\u7684\u63a5\u53e3\u5305\u62ec\uff1a"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"\u521d\u59cb\u5316\u53c2\u6570\u5305\u62ec\uff1a"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"\uff081\uff09kmodel_path\uff1astr\u7c7b\u578b\uff0ckmodel\u8def\u5f84\uff0c\u7528\u4e8e\u521d\u59cb\u5316kpu\u5bf9\u8c61\u5e76\u52a0\u8f7dkmodel\uff1b"}),"\n",(0,i.jsx)(n.p,{children:"\uff082\uff09model_input_size\uff1alist\u7c7b\u578b\uff0c\u53ef\u9009\uff0c\u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\uff0c\u5728\u5355\u8f93\u5165\u65f6\u8d77\u4f5c\u7528\uff0c\u683c\u5f0f\u4e3a[width,height]\uff0c\u5982\uff1amodel_input_size=[512,512];"}),"\n",(0,i.jsx)(n.p,{children:"\uff083\uff09rgb888p_size\uff1alist\u7c7b\u578b\uff0c\u53ef\u9009\uff0cAI\u5f97\u5230\u7684\u56fe\u50cf\u7684\u5206\u8fa8\u7387\uff0c\u5728\u5355\u8f93\u5165\u65f6\u8d77\u4f5c\u7528\uff0c\u683c\u5f0f\u4e3a[width\uff0cheight]\uff0c\u5982\uff1argb888p_size=[640,640];"}),"\n",(0,i.jsx)(n.p,{children:"\uff084\uff09debug_mode\uff1aint\u7c7b\u578b\uff0c\u8017\u65f6\u8c03\u8bd5\u6a21\u5f0f\uff0c\u5982\u679c\u5927\u4e8e0\uff0c\u6253\u5370\u64cd\u4f5c\u8017\u65f6\uff1b\u5982debug_mode=0\u3002"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"get_kmodel_inputs_num()\uff1a\u8fd4\u56de\u5f53\u524d\u6a21\u578b\u7684\u8f93\u5165\u4e2a\u6570\uff1b"}),"\n",(0,i.jsx)(n.li,{children:"get_kmodel_outputs_num()\uff1a\u8fd4\u56de\u5f53\u524d\u6a21\u578b\u7684\u8f93\u51fa\u4e2a\u6570\uff1b"}),"\n",(0,i.jsxs)(n.li,{children:["preprocess(input_np)\uff1a\u4f7f\u7528ai2d\u5bf9input_np\u505a\u9884\u5904\u7406\uff0c",(0,i.jsx)(n.strong,{children:"\u5982\u679c\u4e0d\u4f7f\u7528\u5355\u4e2aai2d\u5b9e\u4f8b\u505a\u9884\u5904\u7406\uff0c\u9700\u8981\u5728\u5b50\u7c7b\u91cd\u5199\u8be5\u51fd\u6570"}),"\u3002"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"\uff081\uff09input_np\uff1aulab.numpy.ndarray\u7c7b\u578b\uff0cai2d\u9884\u5904\u7406\u8f93\u5165\u6570\u636e\uff1b"}),"\n",(0,i.jsxs)(n.p,{children:["\uff082\uff09\u8fd4\u56detensor\u5217\u8868\uff1b",(0,i.jsx)(n.strong,{children:"\u5982\u679c\u8be5\u65b9\u6cd5\u91cd\u5199\uff0c\u8bf7\u6ce8\u610f\u8fd4\u56de\u7c7b\u578b\uff1atensor\u7c7b\u578b\u7684\u5217\u8868\uff1b"})]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"inference(tensors)\uff1a\u5bf9\u9884\u5904\u7406\u540e\u5f97\u5230\u7684kmodel\u7684\u8f93\u5165\uff08\u7c7b\u578b\u4e3atensor\uff09\u8fdb\u884c\u63a8\u7406\uff0c\u5f97\u5230\u591a\u4e2a\u8f93\u51fa\uff08\u7c7b\u578b\u4e3aulab.numpy.ndarray\uff09\uff1b"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"\uff081\uff09tensors\uff1a\u5217\u8868\u7c7b\u578b\uff0c\u6a21\u578b\u7684\u8f93\u5165\uff0c\u53ef\u4ee5\u662f\u4e00\u4e2a\u53ef\u4ee5\u662f\u591a\u4e2a\uff1b"}),"\n",(0,i.jsx)(n.p,{children:"\uff082\uff09\u8fd4\u56deulab.numpy.ndarray\u7c7b\u578b\u7684\u5217\u8868\uff1b"}),"\n",(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Tips:"})}),"\n",(0,i.jsx)(n.p,{children:"Image\u5bf9\u8c61\u8f6culab.numpy.ndarray\uff1a"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"import image\nimg.to_rgb888().to_numpy_ref() #\u8fd4\u56de\u7684array\u662fHWC\u6392\u5e03\n"})}),"\n",(0,i.jsx)(n.p,{children:"ulab.numpy.ndarray\u8f6cImage\u5bf9\u8c61\uff1a"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"import ulab.numpy as np\nimport image\nimg_np = np.zeros((height,width,4),dtype=np.uint8)\nimg = image.Image(width, height, image.ARGB8888, alloc=image.ALLOC_REF,data =img_np)\n"})}),"\n",(0,i.jsx)(n.p,{children:"ulab.numpy.ndarray\u8f6ctensor\u7c7b\u578b\uff1a"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"import ulab.numpy as np\nimport nncase_runtime as nn\nimg_np = np.zeros((height,width,4),dtype=np.uint8)\ntensor = nn.from_numpy(img_np)\n"})}),"\n",(0,i.jsx)(n.p,{children:"tensor \u7c7b\u578b\u8f6culab.numpy.ndarray\uff1a"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"import ulab.numpy as np\nimport nncase_runtime as nn\nimg_np=tensor.to_numpy()\n"})}),"\n"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"postprocess(results)\uff1a\u6a21\u578b\u8f93\u51fa\u540e\u5904\u7406\u51fd\u6570\uff0c\u8be5\u51fd\u6570\u9700\u8981\u7528\u6237\u5728\u4efb\u52a1\u5b50\u7c7b\u91cd\u5199\uff0c\u56e0\u4e3a\u4e0d\u540cAI\u4efb\u52a1\u7684\u540e\u5904\u7406\u662f\u4e0d\u540c\u7684\u3002"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"\uff081\uff09results\uff1alist\u7c7b\u578b\uff0clist\u5143\u7d20\u662fulab.numpy.ndarray\u7c7b\u578b\uff0c\u6a21\u578b\u7684\u63a8\u7406\u8f93\u51fa\u3002"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"run(input_np)\uff1a\u6a21\u578b\u7684\u524d\u5904\u7406\u3001\u63a8\u7406\u3001\u540e\u5904\u7406\u6d41\u7a0b\uff0c\u9002\u7528\u4e8e\u5355ai2d\u5b9e\u4f8b\u80fd\u89e3\u51b3\u7684\u524d\u5904\u7406\u7684AI\u4efb\u52a1\uff0c\u5176\u4ed6\u4efb\u52a1\u9700\u8981\u7528\u6237\u5728\u5b50\u7c7b\u91cd\u5199\u3002"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"\uff081\uff09input_np\uff1aulab.numpy.ndarray\u7c7b\u578b\uff0cai2d\u9884\u5904\u7406\u8f93\u5165\u6570\u636e\uff1b\u8be5\u6570\u636e\u901a\u8fc7ai2d\u9884\u5904\u7406\u8f93\u51fa1\u4e2atensor,tensor\u901a\u8fc7\u6a21\u578b\u63a8\u7406\u5f97\u5230\u8f93\u51fa\u5217\u8868results\uff0cresults\u7ecf\u8fc7\u540e\u5904\u7406\u8fc7\u7a0b\u5f97\u5230AI\u7ed3\u679c\u3002"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"deinit()\uff1aAIBase\u9500\u6bc1\u51fd\u6570\u3002"}),"\n"]}),"\n",(0,i.jsx)(n.h4,{id:"124-scopedtiming",children:"1.2.4. ScopedTiming"}),"\n",(0,i.jsxs)(n.p,{children:["ScopedTiming \u7c7b\u5728PipeLine.py\u6a21\u5757\u5185\uff0c\u662f\u4e00\u4e2a\u7528\u6765\u6d4b\u91cf\u4ee3\u7801\u5757\u6267\u884c\u65f6\u95f4\u7684\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\u3002\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\u901a\u8fc7\u5b9a\u4e49\u5305\u542b ",(0,i.jsx)(n.code,{children:"__enter__"})," \u548c ",(0,i.jsx)(n.code,{children:"__exit__"})," \u65b9\u6cd5\u7684\u7c7b\u6765\u521b\u5efa\u3002\u5f53\u5728 with \u8bed\u53e5\u4e2d\u4f7f\u7528\u8be5\u7c7b\u7684\u5b9e\u4f8b\u65f6\uff0c",(0,i.jsx)(n.code,{children:"__enter__"})," \u5728\u8fdb\u5165 with \u5757\u65f6\u88ab\u8c03\u7528\uff0c",(0,i.jsx)(n.code,{children:"__exit__"})," \u5728\u79bb\u5f00\u65f6\u88ab\u8c03\u7528\u3002"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:'from libs.PipeLine import ScopedTiming\n\ndef test_time():\n    with ScopedTiming("test",1):\n        #####\u4ee3\u7801#####\n        # ...\n        ##############\n'})}),"\n",(0,i.jsx)(n.h3,{id:"13-\u5e94\u7528\u65b9\u6cd5\u548c\u793a\u4f8b",children:"1.3. \u5e94\u7528\u65b9\u6cd5\u548c\u793a\u4f8b"}),"\n",(0,i.jsx)(n.h4,{id:"131-\u6982\u8ff0",children:"1.3.1. \u6982\u8ff0"}),"\n",(0,i.jsx)(n.p,{children:"\u7528\u6237\u53ef\u6839\u636e\u5177\u4f53\u7684AI\u573a\u666f\u81ea\u5199\u4efb\u52a1\u7c7b\u7ee7\u627fAIBase\uff0c\u53ef\u4ee5\u5c06\u4efb\u52a1\u5206\u4e3a\u5982\u4e0b\u56db\u7c7b\uff1a\u5355\u6a21\u578b\u4efb\u52a1\u3001\u591a\u6a21\u578b\u4efb\u52a1\uff0c\u81ea\u5b9a\u4e49\u9884\u5904\u7406\u4efb\u52a1\u3001\u65e0\u9884\u5904\u7406\u4efb\u52a1\u3002\u4e0d\u540c\u4efb\u52a1\u9700\u8981\u7f16\u5199\u4e0d\u540c\u7684\u4ee3\u7801\u5b9e\u73b0\uff0c\u5177\u4f53\u5982\u4e0b\u56fe\u6240\u793a\uff1a"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"\u4e0d\u540c\u4efb\u52a1\u7c7b\u578b",src:s(81546).A+"",width:"983",height:"602"})}),"\n",(0,i.jsx)(n.p,{children:"\u5173\u4e8e\u4e0d\u540c\u4efb\u52a1\u7684\u4ecb\u7ecd\uff1a"}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"\u4efb\u52a1\u7c7b\u578b"}),(0,i.jsx)(n.th,{children:"\u4efb\u52a1\u63cf\u8ff0"}),(0,i.jsx)(n.th,{children:"\u4ee3\u7801\u8bf4\u660e"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"\u5355\u6a21\u578b\u4efb\u52a1"}),(0,i.jsx)(n.td,{children:"\u8be5\u4efb\u52a1\u53ea\u6709\u4e00\u4e2a\u6a21\u578b\uff0c\u53ea\u9700\u8981\u5173\u6ce8\u8be5\u6a21\u578b\u7684\u524d\u5904\u7406\u3001\u63a8\u7406\u3001\u540e\u5904\u7406\u8fc7\u7a0b\uff0c\u6b64\u7c7b\u4efb\u52a1\u7684\u524d\u5904\u7406\u4f7f\u7528Ai2d\u5b9e\u73b0\uff0c\u53ef\u80fd\u4f7f\u7528\u4e00\u4e2aAi2d\u5b9e\u4f8b\uff0c\u4e5f\u53ef\u80fd\u4f7f\u7528\u591a\u4e2aAi2d\u5b9e\u4f8b\uff0c\u540e\u5904\u7406\u57fa\u4e8e\u573a\u666f\u81ea\u5b9a\u4e49\u3002"}),(0,i.jsx)(n.td,{children:"\u7f16\u5199\u81ea\u5b9a\u4e49\u4efb\u52a1\u7c7b\uff0c\u4e3b\u8981\u5173\u6ce8\u4efb\u52a1\u7c7b\u7684config_preprocess\u3001postprocess\u3001\u4ee5\u53ca\u8be5\u4efb\u52a1\u9700\u8981\u7684\u5176\u4ed6\u65b9\u6cd5\u5982\uff1adraw_result\u7b49\u3002 \u5982\u679c\u8be5\u4efb\u52a1\u5305\u542b\u591a\u4e2aAi2d\u5b9e\u4f8b\uff0c\u5219\u9700\u8981\u91cd\u5199preprocess\uff0c\u6309\u7167\u9884\u5904\u7406\u7684\u987a\u5e8f\u8bbe\u7f6e\u9884\u5904\u7406\u9636\u6bb5\u7684\u8ba1\u7b97\u8fc7\u7a0b\u3002"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"\u81ea\u5b9a\u4e49\u9884\u5904\u7406\u4efb\u52a1"}),(0,i.jsx)(n.td,{children:"\u8be5\u4efb\u52a1\u53ea\u6709\u4e00\u4e2a\u6a21\u578b\uff0c\u53ea\u9700\u8981\u5173\u6ce8\u8be5\u6a21\u578b\u7684\u524d\u5904\u7406\u3001\u63a8\u7406\u3001\u540e\u5904\u7406\u8fc7\u7a0b\uff0c\u6b64\u7c7b\u4efb\u52a1\u7684\u524d\u5904\u7406\u4e0d\u4f7f\u7528Ai2d\u5b9e\u73b0\uff0c\u53ef\u4ee5\u4f7f\u7528ulab.numpy\u81ea\u5b9a\u4e49\uff0c\u540e\u5904\u7406\u57fa\u4e8e\u573a\u666f\u81ea\u5b9a\u4e49\u3002"}),(0,i.jsx)(n.td,{children:"\u7f16\u5199\u81ea\u5b9a\u4e49\u4efb\u52a1\u7c7b\uff0c\u4e3b\u8981\u5173\u6ce8\u4efb\u52a1\u7c7b\u7684preprocess\u3001postprocess\u3001\u4ee5\u53ca\u8be5\u4efb\u52a1\u9700\u8981\u7684\u5176\u4ed6\u65b9\u6cd5\u5982\uff1adraw_result\u7b49"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"\u65e0\u9884\u5904\u7406\u4efb\u52a1"}),(0,i.jsx)(n.td,{children:"\u8be5\u4efb\u52a1\u53ea\u6709\u4e00\u4e2a\u6a21\u578b\u4e14\u4e0d\u9700\u8981\u9884\u5904\u7406\uff0c\u53ea\u9700\u8981\u5173\u6ce8\u8be5\u6a21\u578b\u7684\u63a8\u7406\u548c\u540e\u5904\u7406\u8fc7\u7a0b\uff0c\u6b64\u7c7b\u4efb\u52a1\u4e00\u822c\u4f5c\u4e3a\u591a\u6a21\u578b\u4efb\u52a1\u7684\u4e00\u90e8\u5206\uff0c\u76f4\u63a5\u5bf9\u524d\u4e00\u4e2a\u6a21\u578b\u7684\u8f93\u51fa\u505a\u4e3a\u8f93\u5165\u63a8\u7406\uff0c\u540e\u5904\u7406\u57fa\u4e8e\u9700\u6c42\u81ea\u5b9a\u4e49\u3002"}),(0,i.jsx)(n.td,{children:"\u7f16\u5199\u81ea\u5b9a\u4e49\u4efb\u52a1\u7c7b\uff0c\u4e3b\u8981\u5173\u6ce8\u4efb\u52a1\u7c7b\u7684run(\u6a21\u578b\u63a8\u7406\u7684\u6574\u4e2a\u8fc7\u7a0b\uff0c\u5305\u62ecpreprocess\u3001inference\u3001postprocess\u4e2d\u7684\u5168\u90e8\u6216\u67d0\u4e00\u4e9b\u6b65\u9aa4)\u3001postprocess\u3001\u4ee5\u53ca\u8be5\u4efb\u52a1\u9700\u8981\u7684\u5176\u4ed6\u65b9\u6cd5\u5982\uff1adraw_results\u7b49"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"\u591a\u6a21\u578b\u4efb\u52a1"}),(0,i.jsx)(n.td,{children:"\u8be5\u4efb\u52a1\u5305\u542b\u591a\u4e2a\u6a21\u578b\uff0c\u53ef\u80fd\u662f\u4e32\u8054\uff0c\u4e5f\u53ef\u80fd\u662f\u5176\u4ed6\u7ec4\u5408\u65b9\u5f0f\u3002\u5bf9\u4e8e\u6bcf\u4e2a\u6a21\u578b\u57fa\u672c\u4e0a\u5c5e\u4e8e\u524d\u4e09\u79cd\u6a21\u578b\u4e2d\u7684\u4e00\u79cd\uff0c\u6700\u540e\u901a\u8fc7\u4e00\u4e2a\u5b8c\u6574\u7684\u4efb\u52a1\u7c7b\u5c06\u4e0a\u8ff0\u6a21\u578b\u5b50\u4efb\u52a1\u7edf\u4e00\u8d77\u6765\u3002"}),(0,i.jsx)(n.td,{children:"\u7f16\u5199\u591a\u4e2a\u5b50\u6a21\u578b\u4efb\u52a1\u7c7b\uff0c\u4e0d\u540c\u5b50\u6a21\u578b\u4efb\u52a1\u53c2\u7167\u524d\u4e09\u79cd\u4efb\u52a1\u5b9a\u4e49\u3002\u4e0d\u540c\u4efb\u52a1\u5173\u6ce8\u4e0d\u540c\u7684\u65b9\u6cd5\u3002 \u7f16\u5199\u591a\u6a21\u578b\u4efb\u52a1\u7c7b\uff0c\u5c06\u5b50\u6a21\u578b\u4efb\u52a1\u7c7b\u7edf\u4e00\u8d77\u6765\u5b9e\u73b0\u6574\u4e2a\u573a\u666f\u3002"})]})]})]}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{}),(0,i.jsx)(n.th,{}),(0,i.jsx)(n.th,{})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{}),(0,i.jsx)(n.td,{}),(0,i.jsx)(n.td,{})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{}),(0,i.jsx)(n.td,{}),(0,i.jsx)(n.td,{})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{}),(0,i.jsx)(n.td,{}),(0,i.jsx)(n.td,{})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{}),(0,i.jsx)(n.td,{}),(0,i.jsx)(n.td,{})]})]})]}),"\n",(0,i.jsx)(n.h4,{id:"132-\u5355\u6a21\u578b\u4efb\u52a1",children:"1.3.2. \u5355\u6a21\u578b\u4efb\u52a1"}),"\n",(0,i.jsx)(n.p,{children:"\u5355\u6a21\u578b\u4efb\u52a1\u7684\u4f2a\u4ee3\u7801\u7ed3\u6784\u5982\u4e0b\uff1a"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:'from libs.PipeLine import PipeLine, ScopedTiming\nfrom libs.AIBase import AIBase\nfrom libs.AI2D import Ai2d\nimport os\nfrom media.media import *\nimport nncase_runtime as nn\nimport ulab.numpy as np\nimport image\nimport gc\nimport sys\n\n# \u81ea\u5b9a\u4e49AI\u4efb\u52a1\u7c7b\uff0c\u7ee7\u627f\u81eaAIBase\u57fa\u7c7b\nclass MyAIApp(AIBase):\n    def __init__(self, kmodel_path, model_input_size, rgb888p_size=[224,224], display_size=[1920,1080], debug_mode=0):\n        # \u8c03\u7528\u57fa\u7c7b\u7684\u6784\u9020\u51fd\u6570\n        super().__init__(kmodel_path, model_input_size, rgb888p_size, debug_mode)  \n        # \u6a21\u578b\u6587\u4ef6\u8def\u5f84\n        self.kmodel_path = kmodel_path  \n        # \u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.model_input_size = model_input_size  \n        # sensor\u7ed9\u5230AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u5e76\u5bf9\u5bbd\u5ea6\u8fdb\u884c16\u7684\u5bf9\u9f50\n        self.rgb888p_size = [ALIGN_UP(rgb888p_size[0], 16), rgb888p_size[1]]   \n        # \u663e\u793a\u5206\u8fa8\u7387\uff0c\u5e76\u5bf9\u5bbd\u5ea6\u8fdb\u884c16\u7684\u5bf9\u9f50\n        self.display_size = [ALIGN_UP(display_size[0], 16), display_size[1]] \n        # \u662f\u5426\u5f00\u542f\u8c03\u8bd5\u6a21\u5f0f\n        self.debug_mode = debug_mode  \n        # \u5b9e\u4f8b\u5316Ai2d\uff0c\u7528\u4e8e\u5b9e\u73b0\u6a21\u578b\u9884\u5904\u7406\n        self.ai2d = Ai2d(debug_mode)  \n        # \u8bbe\u7f6eAi2d\u7684\u8f93\u5165\u8f93\u51fa\u683c\u5f0f\u548c\u7c7b\u578b\n        self.ai2d.set_ai2d_dtype(nn.ai2d_format.NCHW_FMT, nn.ai2d_format.NCHW_FMT, np.uint8, np.uint8)  \n\n    # \u914d\u7f6e\u9884\u5904\u7406\u64cd\u4f5c\uff0c\u8fd9\u91cc\u4f7f\u7528\u4e86pad\u548cresize\uff0cAi2d\u652f\u6301crop/shift/pad/resize/affine\uff0c\u5177\u4f53\u4ee3\u7801\u8bf7\u6253\u5f00/sdcard/app/libs/AI2D.py\u67e5\u770b\n    def config_preprocess(self, input_image_size=None):\n        with ScopedTiming("set preprocess config", self.debug_mode > 0): \n            # \u521d\u59cb\u5316ai2d\u9884\u5904\u7406\u914d\u7f6e\uff0c\u9ed8\u8ba4\u4e3asensor\u7ed9\u5230AI\u7684\u5c3a\u5bf8\uff0c\u53ef\u4ee5\u901a\u8fc7\u8bbe\u7f6einput_image_size\u81ea\u884c\u4fee\u6539\u8f93\u5165\u5c3a\u5bf8\n            ai2d_input_size = input_image_size if input_image_size else self.rgb888p_size  \n            # \u914d\u7f6eresize\u9884\u5904\u7406\u65b9\u6cd5\n            self.ai2d.resize(nn.interp_method.tf_bilinear, nn.interp_mode.half_pixel) \n            # \u6784\u5efa\u9884\u5904\u7406\u6d41\u7a0b\n            self.ai2d.build([1,3,ai2d_input_size[1],ai2d_input_size[0]],[1,3,self.model_input_size[1],self.model_input_size[0]])  \n\n    # \u81ea\u5b9a\u4e49\u5f53\u524d\u4efb\u52a1\u7684\u540e\u5904\u7406\uff0cresults\u662f\u6a21\u578b\u8f93\u51faarray\u5217\u8868\uff0c\u9700\u8981\u6839\u636e\u5b9e\u9645\u4efb\u52a1\u91cd\u5199\n    def postprocess(self, results):\n        with ScopedTiming("postprocess", self.debug_mode > 0):\n           pass\n\n    # \u7ed8\u5236\u7ed3\u679c\u5230\u753b\u9762\u4e0a\uff0c\u9700\u8981\u6839\u636e\u4efb\u52a1\u81ea\u5df1\u5199\n    def draw_result(self, pl, dets):\n        with ScopedTiming("display_draw", self.debug_mode > 0):\n            pass\n\nif __name__ == "__main__":\n    # \u663e\u793a\u6a21\u5f0f\uff0c\u9ed8\u8ba4"hdmi",\u53ef\u4ee5\u9009\u62e9"hdmi"\u548c"lcd"\n    display_mode="hdmi"\n    if display_mode=="hdmi":\n        display_size=[1920,1080]\n    else:\n        display_size=[800,480]\n    # \u8bbe\u7f6e\u6a21\u578b\u8def\u5f84\uff0c\u8fd9\u91cc\u8981\u66ff\u6362\u6210\u5f53\u524d\u4efb\u52a1\u6a21\u578b\n    kmodel_path = "example_test.kmodel"\n    rgb888p_size = [1920, 1080]\n    ###### \u5176\u5b83\u53c2\u6570########\n    ...\n    ######################\n\n    # \u521d\u59cb\u5316PipeLine\uff0c\u7528\u4e8e\u56fe\u50cf\u5904\u7406\u6d41\u7a0b\n    pl = PipeLine(rgb888p_size=rgb888p_size, display_size=display_size, display_mode=display_mode)\n    pl.create()  # \u521b\u5efaPipeLine\u5b9e\u4f8b\n    # \u521d\u59cb\u5316\u81ea\u5b9a\u4e49AI\u4efb\u52a1\u5b9e\u4f8b\n    my_ai = MyAIApp(kmodel_path, model_input_size=[320, 320],rgb888p_size=rgb888p_size, display_size=display_size, debug_mode=0)\n    my_ai.config_preprocess()  # \u914d\u7f6e\u9884\u5904\u7406\n\n    try:\n        while True:\n            os.exitpoint()                      # \u68c0\u67e5\u662f\u5426\u6709\u9000\u51fa\u4fe1\u53f7\n            with ScopedTiming("total",1):\n                img = pl.get_frame()            # \u83b7\u53d6\u5f53\u524d\u5e27\u6570\u636e\n                res = my_ai.run(img)            # \u63a8\u7406\u5f53\u524d\u5e27\n                my_ai.draw_result(pl, res)      # \u7ed8\u5236\u7ed3\u679c\n                pl.show_image()                 # \u663e\u793a\u7ed3\u679c\n                gc.collect()                    # \u5783\u573e\u56de\u6536\n    except Exception as e:\n        sys.print_exception(e)                  # \u6253\u5370\u5f02\u5e38\u4fe1\u606f\n    finally:\n        my_ai.deinit()                          # \u53cd\u521d\u59cb\u5316\n        pl.destroy()                            # \u9500\u6bc1PipeLine\u5b9e\u4f8b\n                 \n'})}),"\n",(0,i.jsx)(n.p,{children:"\u4e0b\u9762\u4ee5\u4eba\u8138\u68c0\u6d4b\u4e3a\u4f8b\u7ed9\u51fa\u793a\u4f8b\u4ee3\u7801\uff1a"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:'from libs.PipeLine import PipeLine, ScopedTiming\nfrom libs.AIBase import AIBase\nfrom libs.AI2D import Ai2d\nimport os\nimport ujson\nfrom media.media import *\nfrom time import *\nimport nncase_runtime as nn\nimport ulab.numpy as np\nimport time\nimport utime\nimport image\nimport random\nimport gc\nimport sys\nimport aidemo\n\n# \u81ea\u5b9a\u4e49\u4eba\u8138\u68c0\u6d4b\u7c7b\uff0c\u7ee7\u627f\u81eaAIBase\u57fa\u7c7b\nclass FaceDetectionApp(AIBase):\n    def __init__(self, kmodel_path, model_input_size, anchors, confidence_threshold=0.5, nms_threshold=0.2, rgb888p_size=[224,224], display_size=[1920,1080], debug_mode=0):\n        super().__init__(kmodel_path, model_input_size, rgb888p_size, debug_mode)  # \u8c03\u7528\u57fa\u7c7b\u7684\u6784\u9020\u51fd\u6570\n        self.kmodel_path = kmodel_path  # \u6a21\u578b\u6587\u4ef6\u8def\u5f84\n        self.model_input_size = model_input_size  # \u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.confidence_threshold = confidence_threshold  # \u7f6e\u4fe1\u5ea6\u9608\u503c\n        self.nms_threshold = nms_threshold  # NMS\uff08\u975e\u6781\u5927\u503c\u6291\u5236\uff09\u9608\u503c\n        self.anchors = anchors  # \u951a\u70b9\u6570\u636e\uff0c\u7528\u4e8e\u76ee\u6807\u68c0\u6d4b\n        self.rgb888p_size = [ALIGN_UP(rgb888p_size[0], 16), rgb888p_size[1]]  # sensor\u7ed9\u5230AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u5e76\u5bf9\u5bbd\u5ea6\u8fdb\u884c16\u7684\u5bf9\u9f50\n        self.display_size = [ALIGN_UP(display_size[0], 16), display_size[1]]  # \u663e\u793a\u5206\u8fa8\u7387\uff0c\u5e76\u5bf9\u5bbd\u5ea6\u8fdb\u884c16\u7684\u5bf9\u9f50\n        self.debug_mode = debug_mode  # \u662f\u5426\u5f00\u542f\u8c03\u8bd5\u6a21\u5f0f\n        self.ai2d = Ai2d(debug_mode)  # \u5b9e\u4f8b\u5316Ai2d\uff0c\u7528\u4e8e\u5b9e\u73b0\u6a21\u578b\u9884\u5904\u7406\n        self.ai2d.set_ai2d_dtype(nn.ai2d_format.NCHW_FMT, nn.ai2d_format.NCHW_FMT, np.uint8, np.uint8)  # \u8bbe\u7f6eAi2d\u7684\u8f93\u5165\u8f93\u51fa\u683c\u5f0f\u548c\u7c7b\u578b\n\n    # \u914d\u7f6e\u9884\u5904\u7406\u64cd\u4f5c\uff0c\u8fd9\u91cc\u4f7f\u7528\u4e86pad\u548cresize\uff0cAi2d\u652f\u6301crop/shift/pad/resize/affine\uff0c\u5177\u4f53\u4ee3\u7801\u8bf7\u6253\u5f00/sdcard/app/libs/AI2D.py\u67e5\u770b\n    def config_preprocess(self, input_image_size=None):\n        with ScopedTiming("set preprocess config", self.debug_mode > 0):  # \u8ba1\u65f6\u5668\uff0c\u5982\u679cdebug_mode\u5927\u4e8e0\u5219\u5f00\u542f\n            ai2d_input_size = input_image_size if input_image_size else self.rgb888p_size  # \u521d\u59cb\u5316ai2d\u9884\u5904\u7406\u914d\u7f6e\uff0c\u9ed8\u8ba4\u4e3asensor\u7ed9\u5230AI\u7684\u5c3a\u5bf8\uff0c\u53ef\u4ee5\u901a\u8fc7\u8bbe\u7f6einput_image_size\u81ea\u884c\u4fee\u6539\u8f93\u5165\u5c3a\u5bf8\n            top, bottom, left, right = self.get_padding_param()  # \u83b7\u53d6padding\u53c2\u6570\n            self.ai2d.pad([0, 0, 0, 0, top, bottom, left, right], 0, [104, 117, 123])  # \u586b\u5145\u8fb9\u7f18\n            self.ai2d.resize(nn.interp_method.tf_bilinear, nn.interp_mode.half_pixel)  # \u7f29\u653e\u56fe\u50cf\n            self.ai2d.build([1,3,ai2d_input_size[1],ai2d_input_size[0]],[1,3,self.model_input_size[1],self.model_input_size[0]])  # \u6784\u5efa\u9884\u5904\u7406\u6d41\u7a0b\n\n    # \u81ea\u5b9a\u4e49\u5f53\u524d\u4efb\u52a1\u7684\u540e\u5904\u7406\uff0cresults\u662f\u6a21\u578b\u8f93\u51faarray\u5217\u8868\uff0c\u8fd9\u91cc\u4f7f\u7528\u4e86aidemo\u5e93\u7684face_det_post_process\u63a5\u53e3\n    def postprocess(self, results):\n        with ScopedTiming("postprocess", self.debug_mode > 0):\n            post_ret = aidemo.face_det_post_process(self.confidence_threshold, self.nms_threshold, self.model_input_size[1], self.anchors, self.rgb888p_size, results)\n            if len(post_ret) == 0:\n                return post_ret\n            else:\n                return post_ret[0]\n\n    # \u7ed8\u5236\u68c0\u6d4b\u7ed3\u679c\u5230\u753b\u9762\u4e0a\n    def draw_result(self, pl, dets):\n        with ScopedTiming("display_draw", self.debug_mode > 0):\n            if dets:\n                pl.osd_img.clear()  # \u6e05\u9664OSD\u56fe\u50cf\n                for det in dets:\n                    # \u5c06\u68c0\u6d4b\u6846\u7684\u5750\u6807\u8f6c\u6362\u4e3a\u663e\u793a\u5206\u8fa8\u7387\u4e0b\u7684\u5750\u6807\n                    x, y, w, h = map(lambda x: int(round(x, 0)), det[:4])\n                    x = x * self.display_size[0] // self.rgb888p_size[0]\n                    y = y * self.display_size[1] // self.rgb888p_size[1]\n                    w = w * self.display_size[0] // self.rgb888p_size[0]\n                    h = h * self.display_size[1] // self.rgb888p_size[1]\n                    pl.osd_img.draw_rectangle(x, y, w, h, color=(255, 255, 0, 255), thickness=2)  # \u7ed8\u5236\u77e9\u5f62\u6846\n            else:\n                pl.osd_img.clear()\n\n    # \u83b7\u53d6padding\u53c2\u6570\n    def get_padding_param(self):\n        dst_w = self.model_input_size[0]  # \u6a21\u578b\u8f93\u5165\u5bbd\u5ea6\n        dst_h = self.model_input_size[1]  # \u6a21\u578b\u8f93\u5165\u9ad8\u5ea6\n        ratio_w = dst_w / self.rgb888p_size[0]  # \u5bbd\u5ea6\u7f29\u653e\u6bd4\u4f8b\n        ratio_h = dst_h / self.rgb888p_size[1]  # \u9ad8\u5ea6\u7f29\u653e\u6bd4\u4f8b\n        ratio = min(ratio_w, ratio_h)  # \u53d6\u8f83\u5c0f\u7684\u7f29\u653e\u6bd4\u4f8b\n        new_w = int(ratio * self.rgb888p_size[0])  # \u65b0\u5bbd\u5ea6\n        new_h = int(ratio * self.rgb888p_size[1])  # \u65b0\u9ad8\u5ea6\n        dw = (dst_w - new_w) / 2  # \u5bbd\u5ea6\u5dee\n        dh = (dst_h - new_h) / 2  # \u9ad8\u5ea6\u5dee\n        top = int(round(0))\n        bottom = int(round(dh * 2 + 0.1))\n        left = int(round(0))\n        right = int(round(dw * 2 - 0.1))\n        return top, bottom, left, right\n\nif __name__ == "__main__":\n    # \u663e\u793a\u6a21\u5f0f\uff0c\u9ed8\u8ba4"hdmi",\u53ef\u4ee5\u9009\u62e9"hdmi"\u548c"lcd"\n    display_mode="hdmi"\n    if display_mode=="hdmi":\n        display_size=[1920,1080]\n    else:\n        display_size=[800,480]\n    # \u8bbe\u7f6e\u6a21\u578b\u8def\u5f84\u548c\u5176\u4ed6\u53c2\u6570\n    kmodel_path = "/sdcard/app/tests/kmodel/face_detection_320.kmodel"\n    # \u5176\u5b83\u53c2\u6570\n    confidence_threshold = 0.5\n    nms_threshold = 0.2\n    anchor_len = 4200\n    det_dim = 4\n    anchors_path = "/sdcard/app/tests/utils/prior_data_320.bin"\n    anchors = np.fromfile(anchors_path, dtype=np.float)\n    anchors = anchors.reshape((anchor_len, det_dim))\n    rgb888p_size = [1920, 1080]\n\n    # \u521d\u59cb\u5316PipeLine\uff0c\u7528\u4e8e\u56fe\u50cf\u5904\u7406\u6d41\u7a0b\n    pl = PipeLine(rgb888p_size=rgb888p_size, display_size=display_size, display_mode=display_mode)\n    pl.create()  # \u521b\u5efaPipeLine\u5b9e\u4f8b\n    # \u521d\u59cb\u5316\u81ea\u5b9a\u4e49\u4eba\u8138\u68c0\u6d4b\u5b9e\u4f8b\n    face_det = FaceDetectionApp(kmodel_path, model_input_size=[320, 320], anchors=anchors, confidence_threshold=confidence_threshold, nms_threshold=nms_threshold, rgb888p_size=rgb888p_size, display_size=display_size, debug_mode=0)\n    face_det.config_preprocess()  # \u914d\u7f6e\u9884\u5904\u7406\n\n    try:\n        while True:\n            os.exitpoint()                      # \u68c0\u67e5\u662f\u5426\u6709\u9000\u51fa\u4fe1\u53f7\n            with ScopedTiming("total",1):\n                img = pl.get_frame()            # \u83b7\u53d6\u5f53\u524d\u5e27\u6570\u636e\n                res = face_det.run(img)         # \u63a8\u7406\u5f53\u524d\u5e27\n                face_det.draw_result(pl, res)   # \u7ed8\u5236\u7ed3\u679c\n                pl.show_image()                 # \u663e\u793a\u7ed3\u679c\n                gc.collect()                    # \u5783\u573e\u56de\u6536\n    except Exception as e:\n        sys.print_exception(e)                  # \u6253\u5370\u5f02\u5e38\u4fe1\u606f\n    finally:\n        face_det.deinit()                       # \u53cd\u521d\u59cb\u5316\n        pl.destroy()                            # \u9500\u6bc1PipeLine\u5b9e\u4f8b\n'})}),"\n",(0,i.jsx)(n.p,{children:"\u591a\u4e2aAi2d\u5b9e\u4f8b\u65f6\u7684\u4f2a\u4ee3\u7801\u5982\u4e0b\uff1a"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:'from libs.PipeLine import PipeLine, ScopedTiming\nfrom libs.AIBase import AIBase\nfrom libs.AI2D import Ai2d\nimport os\nfrom media.media import *\nimport nncase_runtime as nn\nimport ulab.numpy as np\nimport image\nimport gc\nimport sys\n\n# \u81ea\u5b9a\u4e49AI\u4efb\u52a1\u7c7b\uff0c\u7ee7\u627f\u81eaAIBase\u57fa\u7c7b\nclass MyAIApp(AIBase):\n    def __init__(self, kmodel_path, model_input_size, rgb888p_size=[224,224], display_size=[1920,1080], debug_mode=0):\n        # \u8c03\u7528\u57fa\u7c7b\u7684\u6784\u9020\u51fd\u6570\n        super().__init__(kmodel_path, model_input_size, rgb888p_size, debug_mode)  \n        # \u6a21\u578b\u6587\u4ef6\u8def\u5f84\n        self.kmodel_path = kmodel_path  \n        # \u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.model_input_size = model_input_size  \n        # sensor\u7ed9\u5230AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u5e76\u5bf9\u5bbd\u5ea6\u8fdb\u884c16\u7684\u5bf9\u9f50\n        self.rgb888p_size = [ALIGN_UP(rgb888p_size[0], 16), rgb888p_size[1]]   \n        # \u663e\u793a\u5206\u8fa8\u7387\uff0c\u5e76\u5bf9\u5bbd\u5ea6\u8fdb\u884c16\u7684\u5bf9\u9f50\n        self.display_size = [ALIGN_UP(display_size[0], 16), display_size[1]] \n        # \u662f\u5426\u5f00\u542f\u8c03\u8bd5\u6a21\u5f0f\n        self.debug_mode = debug_mode  \n        # \u5b9e\u4f8b\u5316Ai2d\uff0c\u7528\u4e8e\u5b9e\u73b0\u6a21\u578b\u9884\u5904\u7406\n        self.ai2d_resize = Ai2d(debug_mode)  \n        # \u8bbe\u7f6eAi2d\u7684\u8f93\u5165\u8f93\u51fa\u683c\u5f0f\u548c\u7c7b\u578b\n        self.ai2d_resize.set_ai2d_dtype(nn.ai2d_format.NCHW_FMT, nn.ai2d_format.NCHW_FMT, np.uint8, np.uint8)  \n        # \u5b9e\u4f8b\u5316Ai2d\uff0c\u7528\u4e8e\u5b9e\u73b0\u6a21\u578b\u9884\u5904\u7406\n        self.ai2d_resize = Ai2d(debug_mode)  \n        # \u8bbe\u7f6eAi2d\u7684\u8f93\u5165\u8f93\u51fa\u683c\u5f0f\u548c\u7c7b\u578b\n        self.ai2d_resize.set_ai2d_dtype(nn.ai2d_format.NCHW_FMT, nn.ai2d_format.NCHW_FMT, np.uint8, np.uint8)  \n        # \u5b9e\u4f8b\u5316Ai2d\uff0c\u7528\u4e8e\u5b9e\u73b0\u6a21\u578b\u9884\u5904\u7406\n        self.ai2d_crop = Ai2d(debug_mode)  \n        # \u8bbe\u7f6eAi2d\u7684\u8f93\u5165\u8f93\u51fa\u683c\u5f0f\u548c\u7c7b\u578b\n        self.ai2d_crop.set_ai2d_dtype(nn.ai2d_format.NCHW_FMT, nn.ai2d_format.NCHW_FMT, np.uint8, np.uint8)\n\n    # \u914d\u7f6e\u9884\u5904\u7406\u64cd\u4f5c\uff0c\u8fd9\u91cc\u4f7f\u7528\u4e86pad\u548cresize\uff0cAi2d\u652f\u6301crop/shift/pad/resize/affine\uff0c\u5177\u4f53\u4ee3\u7801\u8bf7\u6253\u5f00/sdcard/app/libs/AI2D.py\u67e5\u770b\n    def config_preprocess(self, input_image_size=None):\n        with ScopedTiming("set preprocess config", self.debug_mode > 0): \n            # \u521d\u59cb\u5316ai2d\u9884\u5904\u7406\u914d\u7f6e\uff0c\u9ed8\u8ba4\u4e3asensor\u7ed9\u5230AI\u7684\u5c3a\u5bf8\uff0c\u53ef\u4ee5\u901a\u8fc7\u8bbe\u7f6einput_image_size\u81ea\u884c\u4fee\u6539\u8f93\u5165\u5c3a\u5bf8\n            ai2d_input_size = input_image_size if input_image_size else self.rgb888p_size  \n            # \u914d\u7f6eresize\u9884\u5904\u7406\u65b9\u6cd5\n            self.ai2d_resize.resize(nn.interp_method.tf_bilinear, nn.interp_mode.half_pixel) \n            # \u6784\u5efa\u9884\u5904\u7406\u6d41\u7a0b\n            self.ai2d_resize.build([1,3,ai2d_input_size[1],ai2d_input_size[0]],[1,3,640,640])  \n            # \u914d\u7f6ecrop\u9884\u5904\u7406\u65b9\u6cd5\n            self.ai2d_crop.crop(0,0,320,320)\n            # \u6784\u5efa\u9884\u5904\u7406\u6d41\u7a0b\n            self.ai2d_crop.build([1,3,640,640],[1,3,320,320])  \n    \n    # \u5047\u8bbe\u8be5\u4efb\u52a1\u9700\u8981crop\u548cresize\u9884\u5904\u7406\uff0c\u987a\u5e8f\u662f\u5148resize\u518dcrop\uff0c\u8be5\u987a\u5e8f\u4e0d\u7b26\u5408ai2d\u7684\u5904\u7406\u987a\u5e8f\uff0c\u56e0\u6b64\u9700\u8981\u8bbe\u7f6e\u4e24\u4e2aAi2d\u5b9e\u4f8b\u5206\u522b\u5904\u7406       \n    def preprocess(self,input_np):\n        resize_tensor=self.ai2d_resize.run(input_np)\n        resize_np=resize_tensor.to_numpy()\n        crop_tensor=self.ai2d_crop.run(resize_np)\n        return [crop_tensor]\n        \n\n    # \u81ea\u5b9a\u4e49\u5f53\u524d\u4efb\u52a1\u7684\u540e\u5904\u7406\uff0cresults\u662f\u6a21\u578b\u8f93\u51faarray\u5217\u8868\uff0c\u9700\u8981\u6839\u636e\u5b9e\u9645\u4efb\u52a1\u91cd\u5199\n    def postprocess(self, results):\n        with ScopedTiming("postprocess", self.debug_mode > 0):\n           pass\n\n    # \u7ed8\u5236\u7ed3\u679c\u5230\u753b\u9762\u4e0a\uff0c\u9700\u8981\u6839\u636e\u4efb\u52a1\u81ea\u5df1\u5199\n    def draw_result(self, pl, dets):\n        with ScopedTiming("display_draw", self.debug_mode > 0):\n            pass\n        \n    # \u91cd\u5199deinit\uff0c\u91ca\u653e\u591a\u4e2aai2d\u8d44\u6e90\n    def deinit(self):\n        with ScopedTiming("deinit",self.debug_mode > 0):\n            del self.ai2d_resize\n            del self.ai2d_crop\n            super().deinit()\n\nif __name__ == "__main__":\n    # \u663e\u793a\u6a21\u5f0f\uff0c\u9ed8\u8ba4"hdmi",\u53ef\u4ee5\u9009\u62e9"hdmi"\u548c"lcd"\n    display_mode="hdmi"\n    if display_mode=="hdmi":\n        display_size=[1920,1080]\n    else:\n        display_size=[800,480]\n    # \u8bbe\u7f6e\u6a21\u578b\u8def\u5f84\uff0c\u8fd9\u91cc\u8981\u66ff\u6362\u6210\u5f53\u524d\u4efb\u52a1\u6a21\u578b\n    kmodel_path = "example_test.kmodel"\n    rgb888p_size = [1920, 1080]\n    ###### \u5176\u5b83\u53c2\u6570########\n    ...\n    ######################\n\n    # \u521d\u59cb\u5316PipeLine\uff0c\u7528\u4e8e\u56fe\u50cf\u5904\u7406\u6d41\u7a0b\n    pl = PipeLine(rgb888p_size=rgb888p_size, display_size=display_size, display_mode=display_mode)\n    pl.create()  # \u521b\u5efaPipeLine\u5b9e\u4f8b\n    # \u521d\u59cb\u5316\u81ea\u5b9a\u4e49AI\u4efb\u52a1\u5b9e\u4f8b\n    my_ai = MyAIApp(kmodel_path, model_input_size=[320, 320],rgb888p_size=rgb888p_size, display_size=display_size, debug_mode=0)\n    my_ai.config_preprocess()  # \u914d\u7f6e\u9884\u5904\u7406\n\n    try:\n        while True:\n            os.exitpoint()                      # \u68c0\u67e5\u662f\u5426\u6709\u9000\u51fa\u4fe1\u53f7\n            with ScopedTiming("total",1):\n                img = pl.get_frame()            # \u83b7\u53d6\u5f53\u524d\u5e27\u6570\u636e\n                res = my_ai.run(img)            # \u63a8\u7406\u5f53\u524d\u5e27\n                my_ai.draw_result(pl, res)      # \u7ed8\u5236\u7ed3\u679c\n                pl.show_image()                 # \u663e\u793a\u7ed3\u679c\n                gc.collect()                    # \u5783\u573e\u56de\u6536\n    except Exception as e:\n        sys.print_exception(e)                  # \u6253\u5370\u5f02\u5e38\u4fe1\u606f\n    finally:\n        my_ai.deinit()                          # \u53cd\u521d\u59cb\u5316\n        pl.destroy()                            # \u9500\u6bc1PipeLine\u5b9e\u4f8b\n'})}),"\n",(0,i.jsx)(n.h4,{id:"133-\u81ea\u5b9a\u4e49\u9884\u5904\u7406\u4efb\u52a1",children:"1.3.3. \u81ea\u5b9a\u4e49\u9884\u5904\u7406\u4efb\u52a1"}),"\n",(0,i.jsx)(n.p,{children:"\u5bf9\u4e8e\u9700\u8981\u91cd\u5199\u524d\u5904\u7406\uff08\u4e0d\u4f7f\u7528\u63d0\u4f9b\u7684ai2d\u7c7b\uff0c\u81ea\u5df1\u624b\u52a8\u5199\u9884\u5904\u7406\uff09\u7684AI\u4efb\u52a1\u4f2a\u4ee3\u7801\u5982\u4e0b\uff1a"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:'from libs.PipeLine import PipeLine, ScopedTiming\nfrom libs.AIBase import AIBase\nfrom libs.AI2D import Ai2d\nimport os\nfrom media.media import *\nimport nncase_runtime as nn\nimport ulab.numpy as np\nimport image\nimport gc\nimport sys\n\n# \u81ea\u5b9a\u4e49AI\u4efb\u52a1\u7c7b\uff0c\u7ee7\u627f\u81eaAIBase\u57fa\u7c7b\nclass MyAIApp(AIBase):\n    def __init__(self, kmodel_path, model_input_size, rgb888p_size=[224,224], display_size=[1920,1080], debug_mode=0):\n        # \u8c03\u7528\u57fa\u7c7b\u7684\u6784\u9020\u51fd\u6570\n        super().__init__(kmodel_path, model_input_size, rgb888p_size, debug_mode)  \n        # \u6a21\u578b\u6587\u4ef6\u8def\u5f84\n        self.kmodel_path = kmodel_path  \n        # \u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.model_input_size = model_input_size  \n        # sensor\u7ed9\u5230AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u5e76\u5bf9\u5bbd\u5ea6\u8fdb\u884c16\u7684\u5bf9\u9f50\n        self.rgb888p_size = [ALIGN_UP(rgb888p_size[0], 16), rgb888p_size[1]]   \n        # \u663e\u793a\u5206\u8fa8\u7387\uff0c\u5e76\u5bf9\u5bbd\u5ea6\u8fdb\u884c16\u7684\u5bf9\u9f50\n        self.display_size = [ALIGN_UP(display_size[0], 16), display_size[1]] \n        # \u662f\u5426\u5f00\u542f\u8c03\u8bd5\u6a21\u5f0f\n        self.debug_mode = debug_mode  \n        # \u5b9e\u4f8b\u5316Ai2d\uff0c\u7528\u4e8e\u5b9e\u73b0\u6a21\u578b\u9884\u5904\u7406\n        self.ai2d = Ai2d(debug_mode)  \n        # \u8bbe\u7f6eAi2d\u7684\u8f93\u5165\u8f93\u51fa\u683c\u5f0f\u548c\u7c7b\u578b\n        self.ai2d.set_ai2d_dtype(nn.ai2d_format.NCHW_FMT, nn.ai2d_format.NCHW_FMT, np.uint8, np.uint8)  \n    \n    # \u5bf9\u4e8e\u4e0d\u4f7f\u7528ai2d\u5b8c\u6210\u9884\u5904\u7406\u7684AI\u4efb\u52a1\uff0c\u4f7f\u7528\u5c01\u88c5\u7684\u63a5\u53e3\u6216\u8005ulab.numpy\u5b9e\u73b0\u9884\u5904\u7406\uff0c\u9700\u8981\u5728\u5b50\u7c7b\u4e2d\u91cd\u5199\u8be5\u51fd\u6570\n    def preprocess(self,input_np):\n        #############\n        #\u6ce8\u610f\u81ea\u5b9a\u4e49\u9884\u5904\u7406\u8fc7\u7a0b\n        #############\n        return [tensor]\n\n    # \u81ea\u5b9a\u4e49\u5f53\u524d\u4efb\u52a1\u7684\u540e\u5904\u7406\uff0cresults\u662f\u6a21\u578b\u8f93\u51faarray\u5217\u8868\uff0c\u9700\u8981\u6839\u636e\u5b9e\u9645\u4efb\u52a1\u91cd\u5199\n    def postprocess(self, results):\n        with ScopedTiming("postprocess", self.debug_mode > 0):\n           pass\n           \n    # \u7ed8\u5236\u7ed3\u679c\u5230\u753b\u9762\u4e0a\uff0c\u9700\u8981\u6839\u636e\u4efb\u52a1\u81ea\u5df1\u5199\n    def draw_result(self, pl, dets):\n        with ScopedTiming("display_draw", self.debug_mode > 0):\n            pass\n\nif __name__ == "__main__":\n    # \u663e\u793a\u6a21\u5f0f\uff0c\u9ed8\u8ba4"hdmi",\u53ef\u4ee5\u9009\u62e9"hdmi"\u548c"lcd"\n    display_mode="hdmi"\n    if display_mode=="hdmi":\n        display_size=[1920,1080]\n    else:\n        display_size=[800,480]\n    # \u8bbe\u7f6e\u6a21\u578b\u8def\u5f84\uff0c\u8fd9\u91cc\u8981\u66ff\u6362\u6210\u5f53\u524d\u4efb\u52a1\u6a21\u578b\n    kmodel_path = "example_test.kmodel"\n    rgb888p_size = [1920, 1080]\n    ###### \u5176\u5b83\u53c2\u6570########\n    ...\n    ######################\n\n    # \u521d\u59cb\u5316PipeLine\uff0c\u7528\u4e8e\u56fe\u50cf\u5904\u7406\u6d41\u7a0b\n    pl = PipeLine(rgb888p_size=rgb888p_size, display_size=display_size, display_mode=display_mode)\n    pl.create()  # \u521b\u5efaPipeLine\u5b9e\u4f8b\n    # \u521d\u59cb\u5316\u81ea\u5b9a\u4e49AI\u4efb\u52a1\u5b9e\u4f8b\n    my_ai = MyAIApp(kmodel_path, model_input_size=[320, 320],rgb888p_size=rgb888p_size, display_size=display_size, debug_mode=0)\n    my_ai.config_preprocess()  # \u914d\u7f6e\u9884\u5904\u7406\n\n    try:\n        while True:\n            os.exitpoint()                      # \u68c0\u67e5\u662f\u5426\u6709\u9000\u51fa\u4fe1\u53f7\n            with ScopedTiming("total",1):\n                img = pl.get_frame()            # \u83b7\u53d6\u5f53\u524d\u5e27\u6570\u636e\n                res = my_ai.run(img)            # \u63a8\u7406\u5f53\u524d\u5e27\n                my_ai.draw_result(pl, res)      # \u7ed8\u5236\u7ed3\u679c\n                pl.show_image()                 # \u663e\u793a\u7ed3\u679c\n                gc.collect()                    # \u5783\u573e\u56de\u6536\n    except Exception as e:\n        sys.print_exception(e)                  # \u6253\u5370\u5f02\u5e38\u4fe1\u606f\n    finally:\n        my_ai.deinit()                          # \u53cd\u521d\u59cb\u5316\n        pl.destroy()                            # \u9500\u6bc1PipeLine\u5b9e\u4f8b\n'})}),"\n",(0,i.jsx)(n.p,{children:"\u4ee5\u5173\u952e\u8bcd\u5524\u9192keyword_spotting\u4e3a\u4f8b\uff1a"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:'from libs.PipeLine import ScopedTiming\nfrom libs.AIBase import AIBase\nfrom libs.AI2D import Ai2d\nfrom media.pyaudio import *                     # \u97f3\u9891\u6a21\u5757\nfrom media.media import *                       # \u8f6f\u4ef6\u62bd\u8c61\u6a21\u5757\uff0c\u4e3b\u8981\u5c01\u88c5\u5a92\u4f53\u6570\u636e\u94fe\u8def\u4ee5\u53ca\u5a92\u4f53\u7f13\u51b2\u533a\nimport media.wave as wave                       # wav\u97f3\u9891\u5904\u7406\u6a21\u5757\nimport nncase_runtime as nn                     # nncase\u8fd0\u884c\u6a21\u5757\uff0c\u5c01\u88c5\u4e86kpu\uff08kmodel\u63a8\u7406\uff09\u548cai2d\uff08\u56fe\u7247\u9884\u5904\u7406\u52a0\u901f\uff09\u64cd\u4f5c\nimport ulab.numpy as np                         # \u7c7b\u4f3cpython numpy\u64cd\u4f5c\uff0c\u4f46\u4e5f\u4f1a\u6709\u4e00\u4e9b\u63a5\u53e3\u4e0d\u540c\nimport aidemo                                   # aidemo\u6a21\u5757\uff0c\u5c01\u88c5ai demo\u76f8\u5173\u524d\u5904\u7406\u3001\u540e\u5904\u7406\u7b49\u64cd\u4f5c\nimport time                                     # \u65f6\u95f4\u7edf\u8ba1\nimport struct                                   # \u5b57\u8282\u5b57\u7b26\u8f6c\u6362\u6a21\u5757\nimport gc                                       # \u5783\u573e\u56de\u6536\u6a21\u5757\nimport os,sys                                   # \u64cd\u4f5c\u7cfb\u7edf\u63a5\u53e3\u6a21\u5757\n\n# \u81ea\u5b9a\u4e49\u5173\u952e\u8bcd\u5524\u9192\u7c7b\uff0c\u7ee7\u627f\u81eaAIBase\u57fa\u7c7b\nclass KWSApp(AIBase):\n    def __init__(self, kmodel_path, threshold, debug_mode=0):\n        super().__init__(kmodel_path)  # \u8c03\u7528\u57fa\u7c7b\u7684\u6784\u9020\u51fd\u6570\n        self.kmodel_path = kmodel_path  # \u6a21\u578b\u6587\u4ef6\u8def\u5f84\n        self.threshold=threshold\n        self.debug_mode = debug_mode  # \u662f\u5426\u5f00\u542f\u8c03\u8bd5\u6a21\u5f0f\n        self.cache_np = np.zeros((1, 256, 105), dtype=np.float)\n\n    # \u81ea\u5b9a\u4e49\u9884\u5904\u7406\uff0c\u8fd4\u56de\u6a21\u578b\u8f93\u5165tensor\u5217\u8868\n    def preprocess(self,pcm_data):\n        pcm_data_list=[]\n        # \u83b7\u53d6\u97f3\u9891\u6d41\u6570\u636e\n        for i in range(0, len(pcm_data), 2):\n            # \u6bcf\u4e24\u4e2a\u5b57\u8282\u7ec4\u7ec7\u6210\u4e00\u4e2a\u6709\u7b26\u53f7\u6574\u6570\uff0c\u7136\u540e\u5c06\u5176\u8f6c\u6362\u4e3a\u6d6e\u70b9\u6570\uff0c\u5373\u4e3a\u4e00\u6b21\u91c7\u6837\u7684\u6570\u636e\uff0c\u52a0\u5165\u5230\u5f53\u524d\u4e00\u5e27\uff080.3s\uff09\u7684\u6570\u636e\u5217\u8868\u4e2d\n            int_pcm_data = struct.unpack("<h", pcm_data[i:i+2])[0]\n            float_pcm_data = float(int_pcm_data)\n            pcm_data_list.append(float_pcm_data)\n        # \u5c06pcm\u6570\u636e\u5904\u7406\u4e3a\u6a21\u578b\u8f93\u5165\u7684\u7279\u5f81\u5411\u91cf\n        mp_feats = aidemo.kws_preprocess(fp, pcm_data_list)[0]\n        mp_feats_np = np.array(mp_feats).reshape((1, 30, 40))\n        audio_input_tensor = nn.from_numpy(mp_feats_np)\n        cache_input_tensor = nn.from_numpy(self.cache_np)\n        return [audio_input_tensor,cache_input_tensor]\n\n    # \u81ea\u5b9a\u4e49\u5f53\u524d\u4efb\u52a1\u7684\u540e\u5904\u7406\uff0cresults\u662f\u6a21\u578b\u8f93\u51faarray\u5217\u8868\n    def postprocess(self, results):\n        with ScopedTiming("postprocess", self.debug_mode > 0):\n            logits_np = results[0]\n            self.cache_np= results[1]\n            max_logits = np.max(logits_np, axis=1)[0]\n            max_p = np.max(max_logits)\n            idx = np.argmax(max_logits)\n            # \u5982\u679c\u5206\u6570\u5927\u4e8e\u9608\u503c\uff0c\u4e14idx==1(\u5373\u5305\u542b\u5524\u9192\u8bcd)\uff0c\u64ad\u653e\u56de\u590d\u97f3\u9891\n            if max_p > self.threshold and idx == 1:\n                return 1\n            else:\n                return 0\n\n\nif __name__ == "__main__":\n    os.exitpoint(os.EXITPOINT_ENABLE)\n    nn.shrink_memory_pool()\n    # \u8bbe\u7f6e\u6a21\u578b\u8def\u5f84\u548c\u5176\u4ed6\u53c2\u6570\n    kmodel_path = "/sdcard/app/tests/kmodel/kws.kmodel"\n    # \u5176\u5b83\u53c2\u6570\n    THRESH = 0.5                # \u68c0\u6d4b\u9608\u503c\n    SAMPLE_RATE = 16000         # \u91c7\u6837\u738716000Hz,\u5373\u6bcf\u79d2\u91c7\u683716000\u6b21\n    CHANNELS = 1                # \u901a\u9053\u6570 1\u4e3a\u5355\u58f0\u9053\uff0c2\u4e3a\u7acb\u4f53\u58f0\n    FORMAT = paInt16            # \u97f3\u9891\u8f93\u5165\u8f93\u51fa\u683c\u5f0f paInt16\n    CHUNK = int(0.3 * 16000)    # \u6bcf\u6b21\u8bfb\u53d6\u97f3\u9891\u6570\u636e\u7684\u5e27\u6570\uff0c\u8bbe\u7f6e\u4e3a0.3s\u7684\u5e27\u657016000*0.3=4800\n    reply_wav_file = "/sdcard/app/tests/utils/wozai.wav"         # kws\u5524\u9192\u8bcd\u56de\u590d\u97f3\u9891\u8def\u5f84\n\n    # \u521d\u59cb\u5316\u97f3\u9891\u9884\u5904\u7406\u63a5\u53e3\n    fp = aidemo.kws_fp_create()\n    # \u521d\u59cb\u5316\u97f3\u9891\u6d41\n    p = PyAudio()\n    p.initialize(CHUNK)\n    MediaManager.init()    #vb buffer\u521d\u59cb\u5316\n    # \u7528\u4e8e\u91c7\u96c6\u5b9e\u65f6\u97f3\u9891\u6570\u636e\n    input_stream = p.open(format=FORMAT,channels=CHANNELS,rate=SAMPLE_RATE,input=True,frames_per_buffer=CHUNK)\n    # \u7528\u4e8e\u64ad\u653e\u56de\u590d\u97f3\u9891\n    output_stream = p.open(format=FORMAT,channels=CHANNELS,rate=SAMPLE_RATE,output=True,frames_per_buffer=CHUNK)\n    # \u521d\u59cb\u5316\u81ea\u5b9a\u4e49\u5173\u952e\u8bcd\u5524\u9192\u5b9e\u4f8b\n    kws = KWSApp(kmodel_path,threshold=THRESH,debug_mode=0)\n\n    try:\n        while True:\n            os.exitpoint()                      # \u68c0\u67e5\u662f\u5426\u6709\u9000\u51fa\u4fe1\u53f7\n            with ScopedTiming("total",1):\n                pcm_data=input_stream.read()\n                res=kws.run(pcm_data)\n                if res:\n                    print("====Detected XiaonanXiaonan!====")\n                    wf = wave.open(reply_wav_file, "rb")\n                    wav_data = wf.read_frames(CHUNK)\n                    while wav_data:\n                        output_stream.write(wav_data)\n                        wav_data = wf.read_frames(CHUNK)\n                    time.sleep(1) # \u65f6\u95f4\u7f13\u51b2\uff0c\u7528\u4e8e\u64ad\u653e\u56de\u590d\u58f0\u97f3\n                    wf.close()\n                else:\n                    print("Deactivated!")\n                gc.collect()                    # \u5783\u573e\u56de\u6536\n    except Exception as e:\n        sys.print_exception(e)                  # \u6253\u5370\u5f02\u5e38\u4fe1\u606f\n    finally:\n        input_stream.stop_stream()\n        output_stream.stop_stream()\n        input_stream.close()\n        output_stream.close()\n        p.terminate()\n        MediaManager.deinit()              #\u91ca\u653evb buffer\n        aidemo.kws_fp_destroy(fp)\n        kws.deinit()                       # \u53cd\u521d\u59cb\u5316\n'})}),"\n",(0,i.jsx)(n.h4,{id:"134-\u65e0\u9884\u5904\u7406\u4efb\u52a1",children:"1.3.4. \u65e0\u9884\u5904\u7406\u4efb\u52a1"}),"\n",(0,i.jsx)(n.p,{children:"\u5bf9\u4e8e\u4e0d\u9700\u8981\u9884\u5904\u7406\uff08\u76f4\u63a5\u8f93\u5165\u63a8\u7406\uff09\u7684AI\u4efb\u52a1\u4f2a\u4ee3\u7801\u5982\u4e0b\uff1a"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:' from libs.PipeLine import PipeLine, ScopedTiming\nfrom libs.AIBase import AIBase\nfrom libs.AI2D import Ai2d\nimport os\nfrom media.media import *\nimport nncase_runtime as nn\nimport ulab.numpy as np\nimport image\nimport gc\nimport sys\n\n# \u81ea\u5b9a\u4e49AI\u4efb\u52a1\u7c7b\uff0c\u7ee7\u627f\u81eaAIBase\u57fa\u7c7b\nclass MyAIApp(AIBase):\n    def __init__(self, kmodel_path, model_input_size, rgb888p_size=[224,224], display_size=[1920,1080], debug_mode=0):\n        # \u8c03\u7528\u57fa\u7c7b\u7684\u6784\u9020\u51fd\u6570\n        super().__init__(kmodel_path, model_input_size, rgb888p_size, debug_mode)  \n        # \u6a21\u578b\u6587\u4ef6\u8def\u5f84\n        self.kmodel_path = kmodel_path  \n        # \u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.model_input_size = model_input_size  \n        # sensor\u7ed9\u5230AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u5e76\u5bf9\u5bbd\u5ea6\u8fdb\u884c16\u7684\u5bf9\u9f50\n        self.rgb888p_size = [ALIGN_UP(rgb888p_size[0], 16), rgb888p_size[1]]   \n        # \u663e\u793a\u5206\u8fa8\u7387\uff0c\u5e76\u5bf9\u5bbd\u5ea6\u8fdb\u884c16\u7684\u5bf9\u9f50\n        self.display_size = [ALIGN_UP(display_size[0], 16), display_size[1]] \n        # \u662f\u5426\u5f00\u542f\u8c03\u8bd5\u6a21\u5f0f\n        self.debug_mode = debug_mode  \n\n    # \u81ea\u5b9a\u4e49\u5f53\u524d\u4efb\u52a1\u7684\u540e\u5904\u7406\uff0cresults\u662f\u6a21\u578b\u8f93\u51faarray\u5217\u8868\uff0c\u9700\u8981\u6839\u636e\u5b9e\u9645\u4efb\u52a1\u91cd\u5199\n    def postprocess(self, results):\n        with ScopedTiming("postprocess", self.debug_mode > 0):\n           pass\n           \n    # \u5bf9\u4e8e\u7528\u9884\u5904\u7406\u7684AI\u4efb\u52a1\uff0c\u9700\u8981\u5728\u5b50\u7c7b\u4e2d\u91cd\u5199\u8be5\u51fd\u6570\n    def run(self,inputs_np):\n        # \u5148\u5c06ulab.numpy.ndarray\u5217\u8868\u8f6c\u6362\u6210tensor\u5217\u8868\n        tensors=[]\n        for input_np in inputs_np:\n            tensors.append(nn.from_numpy(input_np))\n        # \u8c03\u7528AIBase\u5185\u7684inference\u51fd\u6570\u8fdb\u884c\u6a21\u578b\u63a8\u7406\n        results=self.inference(tensors)\n        # \u8c03\u7528\u5f53\u524d\u5b50\u7c7b\u7684postprocess\u65b9\u6cd5\u8fdb\u884c\u81ea\u5b9a\u4e49\u540e\u5904\u7406\n        outputs=self.postprocess(results)\n        return outputs\n\n    # \u7ed8\u5236\u7ed3\u679c\u5230\u753b\u9762\u4e0a\uff0c\u9700\u8981\u6839\u636e\u4efb\u52a1\u81ea\u5df1\u5199\n    def draw_result(self, pl, dets):\n        with ScopedTiming("display_draw", self.debug_mode > 0):\n            pass\n\nif __name__ == "__main__":\n    # \u663e\u793a\u6a21\u5f0f\uff0c\u9ed8\u8ba4"hdmi",\u53ef\u4ee5\u9009\u62e9"hdmi"\u548c"lcd"\n    display_mode="hdmi"\n    if display_mode=="hdmi":\n        display_size=[1920,1080]\n    else:\n        display_size=[800,480]\n    # \u8bbe\u7f6e\u6a21\u578b\u8def\u5f84\uff0c\u8fd9\u91cc\u8981\u66ff\u6362\u6210\u5f53\u524d\u4efb\u52a1\u6a21\u578b\n    kmodel_path = "example_test.kmodel"\n    rgb888p_size = [1920, 1080]\n    ###### \u5176\u5b83\u53c2\u6570########\n    ...\n    ######################\n\n    # \u521d\u59cb\u5316PipeLine\uff0c\u7528\u4e8e\u56fe\u50cf\u5904\u7406\u6d41\u7a0b\n    pl = PipeLine(rgb888p_size=rgb888p_size, display_size=display_size, display_mode=display_mode)\n    pl.create()  # \u521b\u5efaPipeLine\u5b9e\u4f8b\n    # \u521d\u59cb\u5316\u81ea\u5b9a\u4e49AI\u4efb\u52a1\u5b9e\u4f8b\n    my_ai = MyAIApp(kmodel_path, model_input_size=[320, 320],rgb888p_size=rgb888p_size, display_size=display_size, debug_mode=0)\n    my_ai.config_preprocess()  # \u914d\u7f6e\u9884\u5904\u7406\n\n    try:\n        while True:\n            os.exitpoint()                      # \u68c0\u67e5\u662f\u5426\u6709\u9000\u51fa\u4fe1\u53f7\n            with ScopedTiming("total",1):\n                img = pl.get_frame()            # \u83b7\u53d6\u5f53\u524d\u5e27\u6570\u636e\n                res = my_ai.run(img)            # \u63a8\u7406\u5f53\u524d\u5e27\n                my_ai.draw_result(pl, res)      # \u7ed8\u5236\u7ed3\u679c\n                pl.show_image()                 # \u663e\u793a\u7ed3\u679c\n                gc.collect()                    # \u5783\u573e\u56de\u6536\n    except Exception as e:\n        sys.print_exception(e)                  # \u6253\u5370\u5f02\u5e38\u4fe1\u606f\n    finally:\n        my_ai.deinit()                          # \u53cd\u521d\u59cb\u5316\n        pl.destroy()                            # \u9500\u6bc1PipeLine\u5b9e\u4f8b\n'})}),"\n",(0,i.jsx)(n.p,{children:"\u6bd4\u5982\u5355\u76ee\u6807\u8ddf\u8e2a\uff08nanotracker.py\uff09\u4e2d\u7684\u8ffd\u8e2a\u6a21\u5757\uff0c\u53ea\u9700\u8981\u5bf9\u6a21\u7248\u6a21\u578b\u548c\u5b9e\u65f6\u63a8\u7406\u6a21\u578b\u7684\u8f93\u51fa\u4f5c\u4e3a\u8ffd\u8e2a\u6a21\u578b\u7684\u8f93\u5165\uff0c\u4e0d\u9700\u8981\u9884\u5904\u7406\uff1a"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:'class TrackerApp(AIBase):\n    def __init__(self,kmodel_path,crop_input_size,thresh,rgb888p_size=[1280,720],display_size=[1920,1080],debug_mode=0):\n        super().__init__(kmodel_path,rgb888p_size,debug_mode)\n        # kmodel\u8def\u5f84\n        self.kmodel_path=kmodel_path\n        # crop\u6a21\u578b\u7684\u8f93\u5165\u5c3a\u5bf8\n        self.crop_input_size=crop_input_size\n        # \u8ddf\u8e2a\u6846\u9608\u503c\n        self.thresh=thresh\n        # \u8ddf\u8e2a\u6846\u5bbd\u3001\u9ad8\u8c03\u6574\u7cfb\u6570\n        self.CONTEXT_AMOUNT = 0.5\n        # sensor\u7ed9\u5230AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.rgb888p_size=[ALIGN_UP(rgb888p_size[0],16),rgb888p_size[1]]\n        # \u89c6\u9891\u8f93\u51faVO\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.display_size=[ALIGN_UP(display_size[0],16),display_size[1]]\n        # debug\u6a21\u5f0f\n        self.debug_mode=debug_mode\n        # \u53ef\u4ee5\u4e0d\u5b9a\u4e49\n        self.ai2d=Ai2d(debug_mode)\n        self.ai2d.set_ai2d_dtype(nn.ai2d_format.NCHW_FMT,nn.ai2d_format.NCHW_FMT,np.uint8, np.uint8)\n    \n    # \u53ef\u4ee5\u4e0d\u5b9a\u4e49\n    def config_preprocess(self,input_image_size=None):\n        with ScopedTiming("set preprocess config",self.debug_mode > 0):\n            pass\n\n    # \u91cd\u5199run\u51fd\u6570\uff0c\u56e0\u4e3a\u6ca1\u6709\u9884\u5904\u7406\u8fc7\u7a0b\uff0c\u6240\u4ee5\u539f\u6765run\u64cd\u4f5c\u4e2d\u5305\u542b\u7684preprocess->inference->postprocess\u4e0d\u5408\u9002\uff0c\u8fd9\u91cc\u53ea\u5305\u542binference->postprocess\n    def run(self,input_np_1,input_np_2,center_xy_wh):\n        input_tensors=[]\n        input_tensors.append(nn.from_numpy(input_np_1))\n        input_tensors.append(nn.from_numpy(input_np_2))\n        results=self.inference(input_tensors)\n        return self.postprocess(results,center_xy_wh)\n\n\n    # \u81ea\u5b9a\u4e49\u540e\u5904\u7406\uff0cresults\u662f\u6a21\u578b\u8f93\u51faarray\u7684\u5217\u8868,\u8fd9\u91cc\u4f7f\u7528\u4e86aidemo\u7684nanotracker_postprocess\u5217\u8868\n    def postprocess(self,results,center_xy_wh):\n        with ScopedTiming("postprocess",self.debug_mode > 0):\n            det = aidemo.nanotracker_postprocess(results[0],results[1],[self.rgb888p_size[1],self.rgb888p_size[0]],self.thresh,center_xy_wh,self.crop_input_size[0],self.CONTEXT_AMOUNT)\n            return det\n'})}),"\n",(0,i.jsx)(n.h4,{id:"135-\u591a\u6a21\u578b\u4efb\u52a1",children:"1.3.5. \u591a\u6a21\u578b\u4efb\u52a1"}),"\n",(0,i.jsx)(n.p,{children:"\u8fd9\u91cc\u4ee5\u53cc\u6a21\u578b\u4e32\u8054\u63a8\u7406\u4e3a\u4f8b\uff0c\u7ed9\u51fa\u7684\u4f2a\u4ee3\u7801\u5982\u4e0b\uff1a"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:'from libs.PipeLine import PipeLine, ScopedTiming\nfrom libs.AIBase import AIBase\nfrom libs.AI2D import Ai2d\nimport os\nfrom media.media import *\nimport nncase_runtime as nn\nimport ulab.numpy as np\nimport image\nimport gc\nimport sys\n\n# \u81ea\u5b9a\u4e49AI\u4efb\u52a1\u7c7b\uff0c\u7ee7\u627f\u81eaAIBase\u57fa\u7c7b\nclass MyAIApp_1(AIBase):\n    def __init__(self, kmodel_path, model_input_size, rgb888p_size=[224,224], display_size=[1920,1080], debug_mode=0):\n        # \u8c03\u7528\u57fa\u7c7b\u7684\u6784\u9020\u51fd\u6570\n        super().__init__(kmodel_path, model_input_size, rgb888p_size, debug_mode)  \n        # \u6a21\u578b\u6587\u4ef6\u8def\u5f84\n        self.kmodel_path = kmodel_path  \n        # \u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.model_input_size = model_input_size  \n        # sensor\u7ed9\u5230AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u5e76\u5bf9\u5bbd\u5ea6\u8fdb\u884c16\u7684\u5bf9\u9f50\n        self.rgb888p_size = [ALIGN_UP(rgb888p_size[0], 16), rgb888p_size[1]]   \n        # \u663e\u793a\u5206\u8fa8\u7387\uff0c\u5e76\u5bf9\u5bbd\u5ea6\u8fdb\u884c16\u7684\u5bf9\u9f50\n        self.display_size = [ALIGN_UP(display_size[0], 16), display_size[1]] \n        # \u662f\u5426\u5f00\u542f\u8c03\u8bd5\u6a21\u5f0f\n        self.debug_mode = debug_mode  \n        # \u5b9e\u4f8b\u5316Ai2d\uff0c\u7528\u4e8e\u5b9e\u73b0\u6a21\u578b\u9884\u5904\u7406\n        self.ai2d = Ai2d(debug_mode)  \n        # \u8bbe\u7f6eAi2d\u7684\u8f93\u5165\u8f93\u51fa\u683c\u5f0f\u548c\u7c7b\u578b\n        self.ai2d.set_ai2d_dtype(nn.ai2d_format.NCHW_FMT, nn.ai2d_format.NCHW_FMT, np.uint8, np.uint8)  \n\n    # \u914d\u7f6e\u9884\u5904\u7406\u64cd\u4f5c\uff0c\u8fd9\u91cc\u4f7f\u7528\u4e86pad\u548cresize\uff0cAi2d\u652f\u6301crop/shift/pad/resize/affine\uff0c\u5177\u4f53\u4ee3\u7801\u8bf7\u6253\u5f00/sdcard/app/libs/AI2D.py\u67e5\u770b\n    def config_preprocess(self, input_image_size=None):\n        with ScopedTiming("set preprocess config", self.debug_mode > 0): \n            # \u521d\u59cb\u5316ai2d\u9884\u5904\u7406\u914d\u7f6e\uff0c\u9ed8\u8ba4\u4e3asensor\u7ed9\u5230AI\u7684\u5c3a\u5bf8\uff0c\u53ef\u4ee5\u901a\u8fc7\u8bbe\u7f6einput_image_size\u81ea\u884c\u4fee\u6539\u8f93\u5165\u5c3a\u5bf8\n            ai2d_input_size = input_image_size if input_image_size else self.rgb888p_size  \n            # \u914d\u7f6eresize\u9884\u5904\u7406\u65b9\u6cd5\n            self.ai2d.resize(nn.interp_method.tf_bilinear, nn.interp_mode.half_pixel) \n            # \u6784\u5efa\u9884\u5904\u7406\u6d41\u7a0b\n            self.ai2d.build([1,3,ai2d_input_size[1],ai2d_input_size[0]],[1,3,self.model_input_size[1],self.model_input_size[0]])  \n\n    # \u81ea\u5b9a\u4e49\u5f53\u524d\u4efb\u52a1\u7684\u540e\u5904\u7406\uff0cresults\u662f\u6a21\u578b\u8f93\u51faarray\u5217\u8868\uff0c\u9700\u8981\u6839\u636e\u5b9e\u9645\u4efb\u52a1\u91cd\u5199\n    def postprocess(self, results):\n        with ScopedTiming("postprocess", self.debug_mode > 0):\n           pass\n\n            \n# \u81ea\u5b9a\u4e49AI\u4efb\u52a1\u7c7b\uff0c\u7ee7\u627f\u81eaAIBase\u57fa\u7c7b\nclass MyAIApp_2(AIBase):\n    def __init__(self, kmodel_path, model_input_size, rgb888p_size=[224,224], display_size=[1920,1080], debug_mode=0):\n        # \u8c03\u7528\u57fa\u7c7b\u7684\u6784\u9020\u51fd\u6570\n        super().__init__(kmodel_path, model_input_size, rgb888p_size, debug_mode)  \n        # \u6a21\u578b\u6587\u4ef6\u8def\u5f84\n        self.kmodel_path = kmodel_path  \n        # \u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.model_input_size = model_input_size  \n        # sensor\u7ed9\u5230AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u5e76\u5bf9\u5bbd\u5ea6\u8fdb\u884c16\u7684\u5bf9\u9f50\n        self.rgb888p_size = [ALIGN_UP(rgb888p_size[0], 16), rgb888p_size[1]]   \n        # \u663e\u793a\u5206\u8fa8\u7387\uff0c\u5e76\u5bf9\u5bbd\u5ea6\u8fdb\u884c16\u7684\u5bf9\u9f50\n        self.display_size = [ALIGN_UP(display_size[0], 16), display_size[1]] \n        # \u662f\u5426\u5f00\u542f\u8c03\u8bd5\u6a21\u5f0f\n        self.debug_mode = debug_mode  \n        # \u5b9e\u4f8b\u5316Ai2d\uff0c\u7528\u4e8e\u5b9e\u73b0\u6a21\u578b\u9884\u5904\u7406\n        self.ai2d = Ai2d(debug_mode)  \n        # \u8bbe\u7f6eAi2d\u7684\u8f93\u5165\u8f93\u51fa\u683c\u5f0f\u548c\u7c7b\u578b\n        self.ai2d.set_ai2d_dtype(nn.ai2d_format.NCHW_FMT, nn.ai2d_format.NCHW_FMT, np.uint8, np.uint8)  \n\n    # \u914d\u7f6e\u9884\u5904\u7406\u64cd\u4f5c\uff0c\u8fd9\u91cc\u4f7f\u7528\u4e86pad\u548cresize\uff0cAi2d\u652f\u6301crop/shift/pad/resize/affine\uff0c\u5177\u4f53\u4ee3\u7801\u8bf7\u6253\u5f00/sdcard/app/libs/AI2D.py\u67e5\u770b\n    def config_preprocess(self, input_image_size=None):\n        with ScopedTiming("set preprocess config", self.debug_mode > 0): \n            # \u521d\u59cb\u5316ai2d\u9884\u5904\u7406\u914d\u7f6e\uff0c\u9ed8\u8ba4\u4e3asensor\u7ed9\u5230AI\u7684\u5c3a\u5bf8\uff0c\u53ef\u4ee5\u901a\u8fc7\u8bbe\u7f6einput_image_size\u81ea\u884c\u4fee\u6539\u8f93\u5165\u5c3a\u5bf8\n            ai2d_input_size = input_image_size if input_image_size else self.rgb888p_size  \n            # \u914d\u7f6eresize\u9884\u5904\u7406\u65b9\u6cd5\n            self.ai2d.resize(nn.interp_method.tf_bilinear, nn.interp_mode.half_pixel) \n            # \u6784\u5efa\u9884\u5904\u7406\u6d41\u7a0b\n            self.ai2d.build([1,3,ai2d_input_size[1],ai2d_input_size[0]],[1,3,self.model_input_size[1],self.model_input_size[0]])  \n\n    # \u81ea\u5b9a\u4e49\u5f53\u524d\u4efb\u52a1\u7684\u540e\u5904\u7406\uff0cresults\u662f\u6a21\u578b\u8f93\u51faarray\u5217\u8868\uff0c\u9700\u8981\u6839\u636e\u5b9e\u9645\u4efb\u52a1\u91cd\u5199\n    def postprocess(self, results):\n        with ScopedTiming("postprocess", self.debug_mode > 0):\n           pass\n           \n           \nclass MyApp:\n    def __init__(kmodel1_path,kmodel2_path,kmodel1_input_size,kmodel2_input_size,rgb888p_size,display_size,debug_mode):\n        # \u521b\u5efa\u4e24\u4e2a\u6a21\u578b\u63a8\u7406\u7684\u5b9e\u4f8b\n        self.app_1=MyApp_1(kmodel1_path,kmodel1_input_size,rgb888p_size,display_size,debug_mode)\n        self.app_2=MyApp_2(kmodel2_path,kmodel2_input_size\uff0crgb888p_size,display_size\uff0cdebug_mode)\n        self.app_1.config_preprocess()\n    \n    # \u7f16\u5199run\u51fd\u6570\uff0c\u5177\u4f53\u4ee3\u7801\u6839\u636eAI\u4efb\u52a1\u7684\u9700\u6c42\u7f16\u5199\uff0c\u6b64\u5904\u53ea\u662f\u7ed9\u51fa\u4e00\u4e2a\u793a\u4f8b\n    def run(self,input_np):\n        outputs_1=self.app_1.run(input_np)\n        outputs_2=[]\n        for out in outputs_1:\n            self.app_2.config_preprocess(out)\n            out_2=self.app_2.run(input_np)\n            outputs_2.append(out_2)\n        return outputs_1,outputs_2\n            \n    # \u7ed8\u5236\n    def draw_result(self,pl,outputs_1,outputs_2):\n        pass\n        \n    ######\u5176\u4ed6\u51fd\u6570########\n    ...\n    ####################\n        \n\nif __name__ == "__main__":\n    # \u663e\u793a\u6a21\u5f0f\uff0c\u9ed8\u8ba4"hdmi",\u53ef\u4ee5\u9009\u62e9"hdmi"\u548c"lcd"\n    display_mode="hdmi"\n    if display_mode=="hdmi":\n        display_size=[1920,1080]\n    else:\n        display_size=[800,480]\n    rgb888p_size = [1920, 1080]\n    # \u8bbe\u7f6e\u6a21\u578b\u8def\u5f84\uff0c\u8fd9\u91cc\u8981\u66ff\u6362\u6210\u5f53\u524d\u4efb\u52a1\u6a21\u578b\n    kmodel1_path = "test_kmodel1.kmodel"\n    kmdoel1_input_size=[320,320]\n    kmodel2_path = "test_kmodel2.kmodel"\n    kmodel2_input_size=[48,48]\n    \n    ###### \u5176\u5b83\u53c2\u6570########\n    ...\n    ######################\n\n    # \u521d\u59cb\u5316PipeLine\uff0c\u7528\u4e8e\u56fe\u50cf\u5904\u7406\u6d41\u7a0b\n    pl = PipeLine(rgb888p_size=rgb888p_size, display_size=display_size, display_mode=display_mode)\n    pl.create()  # \u521b\u5efaPipeLine\u5b9e\u4f8b\n    # \u521d\u59cb\u5316\u81ea\u5b9a\u4e49AI\u4efb\u52a1\u5b9e\u4f8b\n    my_ai = MyApp(kmodel1_path,kmodel2_path, kmodel1_input_size,kmodel2_input_size,rgb888p_size=rgb888p_size, display_size=display_size, debug_mode=0)\n    my_ai.config_preprocess()  # \u914d\u7f6e\u9884\u5904\u7406\n\n    try:\n        while True:\n            os.exitpoint()                      # \u68c0\u67e5\u662f\u5426\u6709\u9000\u51fa\u4fe1\u53f7\n            with ScopedTiming("total",1):\n                img = pl.get_frame()            # \u83b7\u53d6\u5f53\u524d\u5e27\u6570\u636e\n                outputs_1,outputs_2 = my_ai.run(img)            # \u63a8\u7406\u5f53\u524d\u5e27\n                my_ai.draw_result(pl, outputs_1,outputs_2)      # \u7ed8\u5236\u7ed3\u679c\n                pl.show_image()                 # \u663e\u793a\u7ed3\u679c\n                gc.collect()                    # \u5783\u573e\u56de\u6536\n    except Exception as e:\n        sys.print_exception(e)                  # \u6253\u5370\u5f02\u5e38\u4fe1\u606f\n    finally:\n        my_ai.app_1.deinit()                    # \u53cd\u521d\u59cb\u5316\n        my_ai.app_2.deinit()\n        pl.destroy()                            # \u9500\u6bc1PipeLine\u5b9e\u4f8b\n'})}),"\n",(0,i.jsx)(n.p,{children:"\u4e0b\u9762\u4ee5\u8f66\u724c\u68c0\u6d4b\u4e3a\u4f8b\u7ed9\u51fa\u793a\u4f8b\u4ee3\u7801\uff1a"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:'from libs.PipeLine import PipeLine, ScopedTiming\nfrom libs.AIBase import AIBase\nfrom libs.AI2D import Ai2d\nimport os\nimport ujson\nfrom media.media import *\nfrom time import *\nimport nncase_runtime as nn\nimport ulab.numpy as np\nimport time\nimport image\nimport aidemo\nimport random\nimport gc\nimport sys\n\n# \u81ea\u5b9a\u4e49\u8f66\u724c\u68c0\u6d4b\u7c7b\nclass LicenceDetectionApp(AIBase):\n    # \u521d\u59cb\u5316\u51fd\u6570\uff0c\u8bbe\u7f6e\u8f66\u724c\u68c0\u6d4b\u5e94\u7528\u7684\u53c2\u6570\n    def __init__(self, kmodel_path, model_input_size, confidence_threshold=0.5, nms_threshold=0.2, rgb888p_size=[224,224], display_size=[1920,1080], debug_mode=0):\n        super().__init__(kmodel_path, model_input_size, rgb888p_size, debug_mode)  # \u8c03\u7528\u57fa\u7c7b\u7684\u521d\u59cb\u5316\u51fd\u6570\n        self.kmodel_path = kmodel_path  # \u6a21\u578b\u8def\u5f84\n        # \u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.model_input_size = model_input_size\n        # \u5206\u7c7b\u9608\u503c\n        self.confidence_threshold = confidence_threshold\n        self.nms_threshold = nms_threshold\n        # sensor\u7ed9\u5230AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\n        self.rgb888p_size = [ALIGN_UP(rgb888p_size[0], 16), rgb888p_size[1]]\n        # \u663e\u793a\u5206\u8fa8\u7387\n        self.display_size = [ALIGN_UP(display_size[0], 16), display_size[1]]\n        self.debug_mode = debug_mode\n        # Ai2d\u5b9e\u4f8b\uff0c\u7528\u4e8e\u5b9e\u73b0\u6a21\u578b\u9884\u5904\u7406\n        self.ai2d = Ai2d(debug_mode)\n        # \u8bbe\u7f6eAi2d\u7684\u8f93\u5165\u8f93\u51fa\u683c\u5f0f\u548c\u7c7b\u578b\n        self.ai2d.set_ai2d_dtype(nn.ai2d_format.NCHW_FMT, nn.ai2d_format.NCHW_FMT, np.uint8, np.uint8)\n\n    # \u914d\u7f6e\u9884\u5904\u7406\u64cd\u4f5c\uff0c\u8fd9\u91cc\u4f7f\u7528\u4e86pad\u548cresize\uff0cAi2d\u652f\u6301crop/shift/pad/resize/affine\n    def config_preprocess(self, input_image_size=None):\n        with ScopedTiming("set preprocess config", self.debug_mode > 0):\n            # \u521d\u59cb\u5316ai2d\u9884\u5904\u7406\u914d\u7f6e\uff0c\u9ed8\u8ba4\u4e3asensor\u7ed9\u5230AI\u7684\u5c3a\u5bf8\uff0c\u53ef\u4ee5\u901a\u8fc7\u8bbe\u7f6einput_image_size\u81ea\u884c\u4fee\u6539\u8f93\u5165\u5c3a\u5bf8\n            ai2d_input_size = input_image_size if input_image_size else self.rgb888p_size\n            self.ai2d.resize(nn.interp_method.tf_bilinear, nn.interp_mode.half_pixel)\n            self.ai2d.build([1,3,ai2d_input_size[1],ai2d_input_size[0]],[1,3,self.model_input_size[1],self.model_input_size[0]])\n\n    # \u81ea\u5b9a\u4e49\u5f53\u524d\u4efb\u52a1\u7684\u540e\u5904\u7406\n    def postprocess(self, results):\n        with ScopedTiming("postprocess", self.debug_mode > 0):\n            # \u5bf9\u68c0\u6d4b\u7ed3\u679c\u8fdb\u884c\u540e\u5904\u7406\n            det_res = aidemo.licence_det_postprocess(results, [self.rgb888p_size[1], self.rgb888p_size[0]], self.model_input_size, self.confidence_threshold, self.nms_threshold)\n            return det_res\n\n# \u81ea\u5b9a\u4e49\u8f66\u724c\u8bc6\u522b\u4efb\u52a1\u7c7b\nclass LicenceRecognitionApp(AIBase):\n    def __init__(self,kmodel_path,model_input_size,rgb888p_size=[1920,1080],display_size=[1920,1080],debug_mode=0):\n        super().__init__(kmodel_path,model_input_size,rgb888p_size,debug_mode)\n        # kmodel\u8def\u5f84\n        self.kmodel_path=kmodel_path\n        # \u68c0\u6d4b\u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.model_input_size=model_input_size\n        # sensor\u7ed9\u5230AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.rgb888p_size=[ALIGN_UP(rgb888p_size[0],16),rgb888p_size[1]]\n        # \u89c6\u9891\u8f93\u51faVO\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.display_size=[ALIGN_UP(display_size[0],16),display_size[1]]\n        # debug\u6a21\u5f0f\n        self.debug_mode=debug_mode\n        # \u8f66\u724c\u5b57\u7b26\u5b57\u5178\n        self.dict_rec = ["\u6302", "\u4f7f", "\u9886", "\u6fb3", "\u6e2f", "\u7696", "\u6caa", "\u6d25", "\u6e1d", "\u5180", "\u664b", "\u8499", "\u8fbd", "\u5409", "\u9ed1", "\u82cf", "\u6d59", "\u4eac", "\u95fd", "\u8d63", "\u9c81", "\u8c6b", "\u9102", "\u6e58", "\u7ca4", "\u6842", "\u743c", "\u5ddd", "\u8d35", "\u4e91", "\u85cf", "\u9655", "\u7518", "\u9752", "\u5b81", "\u65b0", "\u8b66", "\u5b66", "0", "1", "2", "3", "4", "5", "6", "7", "8", "9", "A", "B", "C", "D", "E", "F", "G", "H", "J", "K", "L", "M", "N", "P", "Q", "R", "S", "T", "U", "V", "W", "X", "Y", "Z", "_", "-"]\n        self.dict_size = len(self.dict_rec)\n        self.ai2d=Ai2d(debug_mode)\n        self.ai2d.set_ai2d_dtype(nn.ai2d_format.NCHW_FMT,nn.ai2d_format.NCHW_FMT,np.uint8, np.uint8)\n\n    # \u914d\u7f6e\u9884\u5904\u7406\u64cd\u4f5c\uff0c\u8fd9\u91cc\u4f7f\u7528\u4e86resize\uff0cAi2d\u652f\u6301crop/shift/pad/resize/affine\n    def config_preprocess(self,input_image_size=None):\n        with ScopedTiming("set preprocess config",self.debug_mode > 0):\n            ai2d_input_size=input_image_size if input_image_size else self.rgb888p_size\n            self.ai2d.resize(nn.interp_method.tf_bilinear, nn.interp_mode.half_pixel)\n            self.ai2d.build([1,3,ai2d_input_size[1],ai2d_input_size[0]],[1,3,self.model_input_size[1],self.model_input_size[0]])\n\n    # \u81ea\u5b9a\u4e49\u540e\u5904\u7406\uff0cresults\u662f\u6a21\u578b\u8f93\u51fa\u7684array\u5217\u8868\n    def postprocess(self,results):\n        with ScopedTiming("postprocess",self.debug_mode > 0):\n            output_data=results[0].reshape((-1,self.dict_size))\n            max_indices = np.argmax(output_data, axis=1)\n            result_str = ""\n            for i in range(max_indices.shape[0]):\n                index = max_indices[i]\n                if index > 0 and (i == 0 or index != max_indices[i - 1]):\n                    result_str += self.dict_rec[index - 1]\n            return result_str\n\n# \u8f66\u724c\u8bc6\u522b\u4efb\u52a1\u7c7b\nclass LicenceRec:\n    def __init__(self,licence_det_kmodel,licence_rec_kmodel,det_input_size,rec_input_size,confidence_threshold=0.25,nms_threshold=0.3,rgb888p_size=[1920,1080],display_size=[1920,1080],debug_mode=0):\n        # \u8f66\u724c\u68c0\u6d4b\u6a21\u578b\u8def\u5f84\n        self.licence_det_kmodel=licence_det_kmodel\n        # \u8f66\u724c\u8bc6\u522b\u6a21\u578b\u8def\u5f84\n        self.licence_rec_kmodel=licence_rec_kmodel\n        # \u4eba\u8138\u68c0\u6d4b\u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.det_input_size=det_input_size\n        # \u4eba\u8138\u59ff\u6001\u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.rec_input_size=rec_input_size\n        # \u7f6e\u4fe1\u5ea6\u9608\u503c\n        self.confidence_threshold=confidence_threshold\n        # nms\u9608\u503c\n        self.nms_threshold=nms_threshold\n        # sensor\u7ed9\u5230AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.rgb888p_size=[ALIGN_UP(rgb888p_size[0],16),rgb888p_size[1]]\n        # \u89c6\u9891\u8f93\u51faVO\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.display_size=[ALIGN_UP(display_size[0],16),display_size[1]]\n        # debug_mode\u6a21\u5f0f\n        self.debug_mode=debug_mode\n        self.licence_det=LicenceDetectionApp(self.licence_det_kmodel,model_input_size=self.det_input_size,confidence_threshold=self.confidence_threshold,nms_threshold=self.nms_threshold,rgb888p_size=self.rgb888p_size,display_size=self.display_size,debug_mode=0)\n        self.licence_rec=LicenceRecognitionApp(self.licence_rec_kmodel,model_input_size=self.rec_input_size,rgb888p_size=self.rgb888p_size)\n        self.licence_det.config_preprocess()\n\n    # run\u51fd\u6570\n    def run(self,input_np):\n        # \u6267\u884c\u8f66\u724c\u68c0\u6d4b\n        det_boxes=self.licence_det.run(input_np)\n        # \u5c06\u8f66\u724c\u90e8\u5206\u62a0\u51fa\u6765\n        imgs_array_boxes = aidemo.ocr_rec_preprocess(input_np,[self.rgb888p_size[1],self.rgb888p_size[0]],det_boxes)\n        imgs_array = imgs_array_boxes[0]\n        boxes = imgs_array_boxes[1]\n        rec_res = []\n        for img_array in imgs_array:\n            # \u5bf9\u6bcf\u4e00\u4e2a\u68c0\u6d4b\u5230\u7684\u8f66\u724c\u8fdb\u884c\u8bc6\u522b\n            self.licence_rec.config_preprocess(input_image_size=[img_array.shape[3],img_array.shape[2]])\n            licence_str=self.licence_rec.run(img_array)\n            rec_res.append(licence_str)\n            gc.collect()\n        return det_boxes,rec_res\n\n    # \u7ed8\u5236\u8f66\u724c\u68c0\u6d4b\u8bc6\u522b\u6548\u679c\n    def draw_result(self,pl,det_res,rec_res):\n        pl.osd_img.clear()\n        if det_res:\n            point_8 = np.zeros((8),dtype=np.int16)\n            for det_index in range(len(det_res)):\n                for i in range(4):\n                    x = det_res[det_index][i * 2 + 0]/self.rgb888p_size[0]*self.display_size[0]\n                    y = det_res[det_index][i * 2 + 1]/self.rgb888p_size[1]*self.display_size[1]\n                    point_8[i * 2 + 0] = int(x)\n                    point_8[i * 2 + 1] = int(y)\n                for i in range(4):\n                    pl.osd_img.draw_line(point_8[i * 2 + 0],point_8[i * 2 + 1],point_8[(i+1) % 4 * 2 + 0],point_8[(i+1) % 4 * 2 + 1],color=(255, 0, 255, 0),thickness=4)\n                pl.osd_img.draw_string_advanced( point_8[6], point_8[7] + 20, 40,rec_res[det_index] , color=(255,255,153,18))\n\n\nif __name__=="__main__":\n    # \u663e\u793a\u6a21\u5f0f\uff0c\u9ed8\u8ba4"hdmi",\u53ef\u4ee5\u9009\u62e9"hdmi"\u548c"lcd"\n    display_mode="hdmi"\n    if display_mode=="hdmi":\n        display_size=[1920,1080]\n    else:\n        display_size=[800,480]\n    # \u8f66\u724c\u68c0\u6d4b\u6a21\u578b\u8def\u5f84\n    licence_det_kmodel_path="/sdcard/app/tests/kmodel/LPD_640.kmodel"\n    # \u8f66\u724c\u8bc6\u522b\u6a21\u578b\u8def\u5f84\n    licence_rec_kmodel_path="/sdcard/app/tests/kmodel/licence_reco.kmodel"\n    # \u5176\u5b83\u53c2\u6570\n    rgb888p_size=[640,360]\n    licence_det_input_size=[640,640]\n    licence_rec_input_size=[220,32]\n    confidence_threshold=0.2\n    nms_threshold=0.2\n\n    # \u521d\u59cb\u5316PipeLine\uff0c\u53ea\u5173\u6ce8\u4f20\u7ed9AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u663e\u793a\u7684\u5206\u8fa8\u7387\n    pl=PipeLine(rgb888p_size=rgb888p_size,display_size=display_size,display_mode=display_mode)\n    pl.create()\n    lr=LicenceRec(licence_det_kmodel_path,licence_rec_kmodel_path,det_input_size=licence_det_input_size,rec_input_size=licence_rec_input_size,confidence_threshold=confidence_threshold,nms_threshold=nms_threshold,rgb888p_size=rgb888p_size,display_size=display_size)\n    try:\n        while True:\n            os.exitpoint()\n            with ScopedTiming("total",1):\n                img=pl.get_frame()                  # \u83b7\u53d6\u5f53\u524d\u5e27\n                det_res,rec_res=lr.run(img)         # \u63a8\u7406\u5f53\u524d\u5e27\n                lr.draw_result(pl,det_res,rec_res)  # \u7ed8\u5236\u5f53\u524d\u5e27\u63a8\u7406\u7ed3\u679c\n                pl.show_image()                     # \u5c55\u793a\u63a8\u7406\u7ed3\u679c\n                gc.collect()\n    except Exception as e:\n        sys.print_exception(e)\n    finally:\n        lr.licence_det.deinit()\n        lr.licence_rec.deinit()\n        pl.destroy()\n'})}),"\n",(0,i.jsx)(n.h3,{id:"14-\u53c2\u8003\u6587\u6863",children:"1.4. \u53c2\u8003\u6587\u6863"}),"\n",(0,i.jsx)(n.h4,{id:"141-k230-canmv\u6587\u6863",children:"1.4.1. k230 canmv\u6587\u6863"}),"\n",(0,i.jsxs)(n.p,{children:["\u6587\u6863\u94fe\u63a5\uff1a",(0,i.jsx)(n.a,{href:"https://developer.canaan-creative.com/k230_canmv/dev/index.html",children:"Welcome to K230 CanMV\u2019s documentation! \u2014 K230 CanMV \u6587\u6863 (canaan-creative.com)"})]}),"\n",(0,i.jsx)(n.h4,{id:"142-ulab\u5e93\u652f\u6301",children:"1.4.2. Ulab\u5e93\u652f\u6301"}),"\n",(0,i.jsxs)(n.p,{children:["\u94fe\u63a5\uff1a ",(0,i.jsx)(n.a,{href:"https://docs.circuitpython.org/en/latest/shared-bindings/ulab/index.html",children:"ulab \u2013 Manipulate numeric data similar to numpy \u2014 Adafruit CircuitPython 9.1.0-beta.3 documentation"})]}),"\n",(0,i.jsxs)(n.p,{children:["github\u94fe\u63a5\uff1a",(0,i.jsx)(n.a,{href:"https://github.com/v923z/micropython-ulab",children:"v923z/micropython-ulab: a numpy-like fast vector module for micropython, circuitpython, and their derivatives (github.com)"})]}),"\n",(0,i.jsx)(n.h2,{id:"2-ai-demo",children:"2. AI Demo"}),"\n",(0,i.jsx)(n.p,{children:"AI Demo\u5206\u4e3a\u4e24\u79cd\u7c7b\u578b\uff1a\u5355\u6a21\u578b\u3001\u591a\u6a21\u578b\uff0c\u6db5\u76d6\u7269\u4f53\u3001\u4eba\u8138\u3001\u4eba\u624b\u3001\u4eba\u4f53\u3001\u8f66\u724c\u3001OCR\u3001\u97f3\u9891\uff08KWS\u3001TTS\uff09\u7b49\u65b9\u5411\uff1b\u53c2\u8003\u8be5\u6587\u6863\uff0ck230\u7528\u6237\u53ef\u4ee5\u66f4\u5feb\u4e0a\u624bK230 AI\u5e94\u7528\u7684\u5f00\u53d1\uff0c\u5b9e\u73b0\u9884\u671f\u6548\u679c\u3002"}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Demo\u540d\u79f0"}),(0,i.jsx)(n.th,{children:"\u573a\u666f"}),(0,i.jsx)(n.th,{children:"\u4efb\u52a1\u7c7b\u578b"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"dynamic_gesture"}),(0,i.jsx)(n.td,{children:"\u52a8\u6001\u624b\u52bf\u8bc6\u522b"}),(0,i.jsx)(n.td,{children:"\u591a\u6a21\u578b\u4efb\u52a1"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"eye_gaze"}),(0,i.jsx)(n.td,{children:"\u6ce8\u89c6\u4f30\u8ba1"}),(0,i.jsx)(n.td,{children:"\u591a\u6a21\u578b\u4efb\u52a1"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"face_detection"}),(0,i.jsx)(n.td,{children:"\u4eba\u8138\u68c0\u6d4b"}),(0,i.jsx)(n.td,{children:"\u5355\u6a21\u578b\u4efb\u52a1"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"face_landmark"}),(0,i.jsx)(n.td,{children:"\u4eba\u8138\u5173\u952e\u90e8\u4f4d"}),(0,i.jsx)(n.td,{children:"\u591a\u6a21\u578b\u4efb\u52a1"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"face_mesh"}),(0,i.jsx)(n.td,{children:"\u4eba\u81383D\u7f51\u683c"}),(0,i.jsx)(n.td,{children:"\u591a\u6a21\u578b\u4efb\u52a1"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"face_parse"}),(0,i.jsx)(n.td,{children:"\u4eba\u8138\u89e3\u6790"}),(0,i.jsx)(n.td,{children:"\u591a\u6a21\u578b\u4efb\u52a1"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"face_pose"}),(0,i.jsx)(n.td,{children:"\u4eba\u8138\u59ff\u6001"}),(0,i.jsx)(n.td,{children:"\u591a\u6a21\u578b\u4efb\u52a1"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"face_recognition"}),(0,i.jsx)(n.td,{children:"\u4eba\u8138\u8bc6\u522b"}),(0,i.jsx)(n.td,{children:"\u591a\u6a21\u578b\u4efb\u52a1"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"face_registration"}),(0,i.jsx)(n.td,{children:"\u4eba\u8138\u6ce8\u518c"}),(0,i.jsx)(n.td,{children:"\u591a\u6a21\u578b\u4efb\u52a1"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"falldown_detection"}),(0,i.jsx)(n.td,{children:"\u8dcc\u5012\u68c0\u6d4b"}),(0,i.jsx)(n.td,{children:"\u5355\u6a21\u578b\u4efb\u52a1"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"finger_guessing"}),(0,i.jsx)(n.td,{children:"\u731c\u62f3\u6e38\u620f"}),(0,i.jsx)(n.td,{children:"\u591a\u6a21\u578b\u4efb\u52a1"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"hand_detection"}),(0,i.jsx)(n.td,{children:"\u624b\u638c\u68c0\u6d4b"}),(0,i.jsx)(n.td,{children:"\u5355\u6a21\u578b\u4efb\u52a1"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"hand_keypoint_class"}),(0,i.jsx)(n.td,{children:"\u624b\u638c\u5173\u952e\u70b9\u5206\u7c7b"}),(0,i.jsx)(n.td,{children:"\u591a\u6a21\u578b\u4efb\u52a1"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"hand_keypoint_detection"}),(0,i.jsx)(n.td,{children:"\u624b\u638c\u5173\u952e\u70b9\u68c0\u6d4b"}),(0,i.jsx)(n.td,{children:"\u591a\u6a21\u578b\u4efb\u52a1"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"hand_recognition"}),(0,i.jsx)(n.td,{children:"\u624b\u52bf\u8bc6\u522b"}),(0,i.jsx)(n.td,{children:"\u591a\u6a21\u578b\u4efb\u52a1"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"keyword_spotting"}),(0,i.jsx)(n.td,{children:"\u5173\u952e\u8bcd\u5524\u9192"}),(0,i.jsx)(n.td,{children:"\u5355\u6a21\u578b\u4efb\u52a1"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"licence_det"}),(0,i.jsx)(n.td,{children:"\u8f66\u724c\u68c0\u6d4b"}),(0,i.jsx)(n.td,{children:"\u5355\u6a21\u578b\u4efb\u52a1"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"licence_det_rec"}),(0,i.jsx)(n.td,{children:"\u8f66\u724c\u8bc6\u522b"}),(0,i.jsx)(n.td,{children:"\u591a\u6a21\u578b\u4efb\u52a1"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"nanotracker"}),(0,i.jsx)(n.td,{children:"\u5355\u76ee\u6807\u8ddf\u8e2a"}),(0,i.jsx)(n.td,{children:"\u591a\u6a21\u578b\u4efb\u52a1"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"object_detect_yolov8n"}),(0,i.jsx)(n.td,{children:"yolov8n\u76ee\u6807\u68c0\u6d4b"}),(0,i.jsx)(n.td,{children:"\u5355\u6a21\u578b\u4efb\u52a1"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"ocr_det"}),(0,i.jsx)(n.td,{children:"OCR\u68c0\u6d4b"}),(0,i.jsx)(n.td,{children:"\u5355\u6a21\u578b\u4efb\u52a1"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"ocr_rec"}),(0,i.jsx)(n.td,{children:"OCR\u8bc6\u522b"}),(0,i.jsx)(n.td,{children:"\u591a\u6a21\u578b\u4efb\u52a1"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"person_detection"}),(0,i.jsx)(n.td,{children:"\u4eba\u4f53\u68c0\u6d4b"}),(0,i.jsx)(n.td,{children:"\u5355\u6a21\u578b\u4efb\u52a1"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"person_kp_detect"}),(0,i.jsx)(n.td,{children:"\u4eba\u4f53\u5173\u952e\u70b9\u68c0\u6d4b"}),(0,i.jsx)(n.td,{children:"\u591a\u6a21\u578b\u4efb\u52a1"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"puzzle_game"}),(0,i.jsx)(n.td,{children:"\u62fc\u56fe\u6e38\u620f"}),(0,i.jsx)(n.td,{children:"\u591a\u6a21\u578b\u4efb\u52a1"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"segment_yolov8n"}),(0,i.jsx)(n.td,{children:"yolov8\u5206\u5272"}),(0,i.jsx)(n.td,{children:"\u5355\u6a21\u578b\u4efb\u52a1"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"self_learning"}),(0,i.jsx)(n.td,{children:"\u81ea\u5b66\u4e60"}),(0,i.jsx)(n.td,{children:"\u5355\u6a21\u578b\u4efb\u52a1"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"space_resize"}),(0,i.jsx)(n.td,{children:"\u5c40\u90e8\u653e\u5927\u5668"}),(0,i.jsx)(n.td,{children:"\u591a\u6a21\u578b\u4efb\u52a1"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"tts_zh"}),(0,i.jsx)(n.td,{children:"\u4e2d\u6587\u6587\u672c\u8f6c\u8bed\u97f3"}),(0,i.jsx)(n.td,{children:"\u591a\u6a21\u578b\u4efb\u52a1"})]})]})]}),"\n",(0,i.jsx)(n.h3,{id:"21-\u52a8\u6001\u624b\u52bf\u8bc6\u522b",children:"2.1. \u52a8\u6001\u624b\u52bf\u8bc6\u522b"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:'from libs.PipeLine import PipeLine, ScopedTiming\nfrom libs.AIBase import AIBase\nfrom libs.AI2D import Ai2d\nfrom random import randint\nimport os\nimport ujson\nfrom media.media import *\nfrom time import *\nimport nncase_runtime as nn\nimport ulab.numpy as np\nimport time\nimport image\nimport aicube\nimport random\nimport gc\nimport sys\n\n# \u81ea\u5b9a\u4e49\u624b\u638c\u68c0\u6d4b\u4efb\u52a1\u7c7b\nclass HandDetApp(AIBase):\n    def __init__(self,kmodel_path,labels,model_input_size,anchors,confidence_threshold=0.2,nms_threshold=0.5,nms_option=False, strides=[8,16,32],rgb888p_size=[1920,1080],display_size=[1920,1080],debug_mode=0):\n        super().__init__(kmodel_path,model_input_size,rgb888p_size,debug_mode)\n        # kmodel\u8def\u5f84\n        self.kmodel_path=kmodel_path\n        # \u68c0\u6d4b\u6807\u7b7e\n        self.labels=labels\n        # \u68c0\u6d4b\u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.model_input_size=model_input_size\n        # \u7f6e\u4fe1\u5ea6\u9608\u503c\n        self.confidence_threshold=confidence_threshold\n        # nms\u9608\u503c\n        self.nms_threshold=nms_threshold\n        # \u68c0\u6d4b\u951a\u6846\n        self.anchors=anchors\n        self.strides = strides  # \u7279\u5f81\u4e0b\u91c7\u6837\u500d\u6570\n        self.nms_option = nms_option  # NMS\u9009\u9879\uff0c\u5982\u679c\u4e3aTrue\u505a\u7c7b\u95f4NMS,\u5982\u679c\u4e3aFalse\u505a\u7c7b\u5185NMS\n        # sensor\u7ed9\u5230AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.rgb888p_size=[ALIGN_UP(rgb888p_size[0],16),rgb888p_size[1]]\n        # \u89c6\u9891\u8f93\u51faVO\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.display_size=[ALIGN_UP(display_size[0],16),display_size[1]]\n        # debug\u6a21\u5f0f\n        self.debug_mode=debug_mode\n        # Ai2d\u5b9e\u4f8b\uff0c\u7528\u4e8e\u5b9e\u73b0\u6a21\u578b\u9884\u5904\u7406\n        self.ai2d=Ai2d(debug_mode)\n        # \u8bbe\u7f6eAi2d\u7684\u8f93\u5165\u8f93\u51fa\u683c\u5f0f\u548c\u7c7b\u578b\n        self.ai2d.set_ai2d_dtype(nn.ai2d_format.NCHW_FMT,nn.ai2d_format.NCHW_FMT,np.uint8, np.uint8)\n\n    # \u914d\u7f6e\u9884\u5904\u7406\u64cd\u4f5c\uff0c\u8fd9\u91cc\u4f7f\u7528\u4e86padding\u548cresize\uff0cAi2d\u652f\u6301crop/shift/pad/resize/affine\uff0c\u5177\u4f53\u4ee3\u7801\u8bf7\u6253\u5f00/sdcard/app/libs/AI2D.py\u67e5\u770b\n    def config_preprocess(self,input_image_size=None):\n        with ScopedTiming("set preprocess config",self.debug_mode > 0):\n            # \u521d\u59cb\u5316ai2d\u9884\u5904\u7406\u914d\u7f6e\uff0c\u9ed8\u8ba4\u4e3asensor\u7ed9\u5230AI\u7684\u5c3a\u5bf8\uff0c\u53ef\u4ee5\u901a\u8fc7\u8bbe\u7f6einput_image_size\u81ea\u884c\u4fee\u6539\u8f93\u5165\u5c3a\u5bf8\n            ai2d_input_size = input_image_size if input_image_size else self.rgb888p_size\n            # \u8ba1\u7b97padding\u53c2\u6570\u5e76\u5e94\u7528pad\u64cd\u4f5c\uff0c\u4ee5\u786e\u4fdd\u8f93\u5165\u56fe\u50cf\u5c3a\u5bf8\u4e0e\u6a21\u578b\u8f93\u5165\u5c3a\u5bf8\u5339\u914d\n            top, bottom, left, right = self.get_padding_param()\n            self.ai2d.pad([0, 0, 0, 0, top, bottom, left, right], 0, [114, 114, 114])\n            # \u4f7f\u7528\u53cc\u7ebf\u6027\u63d2\u503c\u8fdb\u884cresize\u64cd\u4f5c\uff0c\u8c03\u6574\u56fe\u50cf\u5c3a\u5bf8\u4ee5\u7b26\u5408\u6a21\u578b\u8f93\u5165\u8981\u6c42\n            self.ai2d.resize(nn.interp_method.tf_bilinear, nn.interp_mode.half_pixel)\n            # \u6784\u5efa\u9884\u5904\u7406\u6d41\u7a0b,\u53c2\u6570\u4e3a\u9884\u5904\u7406\u8f93\u5165tensor\u7684shape\u548c\u9884\u5904\u7406\u8f93\u51fa\u7684tensor\u7684shape\n            self.ai2d.build([1,3,ai2d_input_size[1],ai2d_input_size[0]],[1,3,self.model_input_size[1],self.model_input_size[0]])\n\n    # \u81ea\u5b9a\u4e49\u540e\u5904\u7406\u8fc7\u7a0b\uff0c\u8fd9\u91cc\u4f7f\u7528\u4e86aicube\u7684anchorbasedet_post_process\u63a5\u53e3\n    def postprocess(self,results):\n        with ScopedTiming("postprocess",self.debug_mode > 0):\n            dets = aicube.anchorbasedet_post_process(results[0], results[1], results[2], self.model_input_size, self.rgb888p_size, self.strides, len(self.labels), self.confidence_threshold, self.nms_threshold, self.anchors, self.nms_option)\n            # \u8fd4\u56de\u624b\u638c\u68c0\u6d4b\u7ed3\u679c\n            return dets\n\n    # \u8ba1\u7b97padding\u53c2\u6570\uff0c\u786e\u4fdd\u8f93\u5165\u56fe\u50cf\u5c3a\u5bf8\u4e0e\u6a21\u578b\u8f93\u5165\u5c3a\u5bf8\u5339\u914d\n    def get_padding_param(self):\n        # \u6839\u636e\u76ee\u6807\u5bbd\u5ea6\u548c\u9ad8\u5ea6\u8ba1\u7b97\u6bd4\u4f8b\u56e0\u5b50\n        dst_w = self.model_input_size[0]\n        dst_h = self.model_input_size[1]\n        input_width = self.rgb888p_size[0]\n        input_high = self.rgb888p_size[1]\n        ratio_w = dst_w / input_width\n        ratio_h = dst_h / input_high\n        # \u9009\u62e9\u8f83\u5c0f\u7684\u6bd4\u4f8b\u56e0\u5b50\uff0c\u4ee5\u786e\u4fdd\u56fe\u50cf\u5185\u5bb9\u5b8c\u6574\n        if ratio_w < ratio_h:\n            ratio = ratio_w\n        else:\n            ratio = ratio_h\n        # \u8ba1\u7b97\u65b0\u7684\u5bbd\u5ea6\u548c\u9ad8\u5ea6\n        new_w = int(ratio * input_width)\n        new_h = int(ratio * input_high)\n        # \u8ba1\u7b97\u5bbd\u5ea6\u548c\u9ad8\u5ea6\u7684\u5dee\u503c\uff0c\u5e76\u786e\u5b9apadding\u7684\u4f4d\u7f6e\n        dw = (dst_w - new_w) / 2\n        dh = (dst_h - new_h) / 2\n        top = int(round(dh - 0.1))\n        bottom = int(round(dh + 0.1))\n        left = int(round(dw - 0.1))\n        right = int(round(dw + 0.1))\n        return top, bottom, left, right\n\n# \u81ea\u5b9a\u4e49\u624b\u52bf\u5173\u952e\u70b9\u5206\u7c7b\u4efb\u52a1\u7c7b\nclass HandKPClassApp(AIBase):\n    def __init__(self,kmodel_path,model_input_size,rgb888p_size=[1920,1080],display_size=[1920,1080],debug_mode=0):\n        super().__init__(kmodel_path,model_input_size,rgb888p_size,debug_mode)\n        # kmodel\u8def\u5f84\n        self.kmodel_path=kmodel_path\n        # \u624b\u638c\u5173\u952e\u70b9\u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.model_input_size=model_input_size\n        # sensor\u7ed9\u5230AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.rgb888p_size=[ALIGN_UP(rgb888p_size[0],16),rgb888p_size[1]]\n        # \u89c6\u9891\u8f93\u51faVO\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.display_size=[ALIGN_UP(display_size[0],16),display_size[1]]\n        # crop\u53c2\u6570\u5217\u8868\n        self.crop_params=[]\n        # debug\u6a21\u5f0f\n        self.debug_mode=debug_mode\n        # Ai2d\u5b9e\u4f8b\uff0c\u7528\u4e8e\u5b9e\u73b0\u6a21\u578b\u9884\u5904\u7406\n        self.ai2d=Ai2d(debug_mode)\n        # \u8bbe\u7f6eAi2d\u7684\u8f93\u5165\u8f93\u51fa\u683c\u5f0f\u548c\u7c7b\u578b\n        self.ai2d.set_ai2d_dtype(nn.ai2d_format.NCHW_FMT,nn.ai2d_format.NCHW_FMT,np.uint8, np.uint8)\n\n    # \u914d\u7f6e\u9884\u5904\u7406\u64cd\u4f5c\uff0c\u8fd9\u91cc\u4f7f\u7528\u4e86crop\u548cresize\uff0cAi2d\u652f\u6301crop/shift/pad/resize/affine\uff0c\u5177\u4f53\u4ee3\u7801\u8bf7\u6253\u5f00/sdcard/app/libs/AI2D.py\u67e5\u770b\n    def config_preprocess(self,det,input_image_size=None):\n        with ScopedTiming("set preprocess config",self.debug_mode > 0):\n            # \u5982\u679cinput_image_size\u4e3aNone,\u4f7f\u7528\u89c6\u9891\u51fa\u56fe\u5927\u5c0f\uff0c\u5426\u5219\u6309\u7167\u81ea\u5b9a\u4e49\u8bbe\u7f6e\n            ai2d_input_size=input_image_size if input_image_size else self.rgb888p_size\n            # \u8ba1\u7b97crop\u53c2\u6570\n            self.crop_params = self.get_crop_param(det)\n            # \u8bbe\u7f6ecrop\u9884\u5904\u7406\u8fc7\u7a0b\n            self.ai2d.crop(self.crop_params[0],self.crop_params[1],self.crop_params[2],self.crop_params[3])\n            # \u8bbe\u7f6eresize\u9884\u5904\u7406\u8fc7\u7a0b\n            self.ai2d.resize(nn.interp_method.tf_bilinear, nn.interp_mode.half_pixel)\n            # build\u9884\u5904\u7406\u8fc7\u7a0b\uff0c\u53c2\u6570\u4e3a\u8f93\u5165tensor\u7684shape\u548c\u8f93\u51fatensor\u7684shape\n            self.ai2d.build([1,3,ai2d_input_size[1],ai2d_input_size[0]],[1,3,self.model_input_size[1],self.model_input_size[0]])\n\n    # \u81ea\u5b9a\u4e49\u540e\u5904\u7406\uff0cresults\u662f\u6a21\u578b\u8f93\u51fa\u7684array\u5217\u8868\n    def postprocess(self,results):\n        with ScopedTiming("postprocess",self.debug_mode > 0):\n            results=results[0].reshape(results[0].shape[0]*results[0].shape[1])\n            results_show = np.zeros(results.shape,dtype=np.int16)\n            results_show[0::2] = results[0::2] * self.crop_params[3] + self.crop_params[0]\n            results_show[1::2] = results[1::2] * self.crop_params[2] + self.crop_params[1]\n            # \u6839\u636e\u8f93\u51fa\u8ba1\u7b97\u624b\u52bf\n            gesture=self.hk_gesture(results_show)\n            return results_show,gesture\n\n    # \u8ba1\u7b97crop\u53c2\u6570\n    def get_crop_param(self,det_box):\n        x1, y1, x2, y2 = det_box[2],det_box[3],det_box[4],det_box[5]\n        w,h= int(x2 - x1),int(y2 - y1)\n        w_det = int(float(x2 - x1) * self.display_size[0] // self.rgb888p_size[0])\n        h_det = int(float(y2 - y1) * self.display_size[1] // self.rgb888p_size[1])\n        x_det = int(x1*self.display_size[0] // self.rgb888p_size[0])\n        y_det = int(y1*self.display_size[1] // self.rgb888p_size[1])\n        length = max(w, h)/2\n        cx = (x1+x2)/2\n        cy = (y1+y2)/2\n        ratio_num = 1.26*length\n        x1_kp = int(max(0,cx-ratio_num))\n        y1_kp = int(max(0,cy-ratio_num))\n        x2_kp = int(min(self.rgb888p_size[0]-1, cx+ratio_num))\n        y2_kp = int(min(self.rgb888p_size[1]-1, cy+ratio_num))\n        w_kp = int(x2_kp - x1_kp + 1)\n        h_kp = int(y2_kp - y1_kp + 1)\n        return [x1_kp, y1_kp, w_kp, h_kp]\n\n    # \u6c42\u4e24\u4e2avector\u4e4b\u95f4\u7684\u5939\u89d2\n    def hk_vector_2d_angle(self,v1,v2):\n        with ScopedTiming("hk_vector_2d_angle",self.debug_mode > 0):\n            v1_x,v1_y,v2_x,v2_y = v1[0],v1[1],v2[0],v2[1]\n            v1_norm = np.sqrt(v1_x * v1_x+ v1_y * v1_y)\n            v2_norm = np.sqrt(v2_x * v2_x + v2_y * v2_y)\n            dot_product = v1_x * v2_x + v1_y * v2_y\n            cos_angle = dot_product/(v1_norm*v2_norm)\n            angle = np.acos(cos_angle)*180/np.pi\n            return angle\n\n    # \u6839\u636e\u624b\u638c\u5173\u952e\u70b9\u68c0\u6d4b\u7ed3\u679c\u5224\u65ad\u624b\u52bf\u7c7b\u522b\n    def hk_gesture(self,results):\n        with ScopedTiming("hk_gesture",self.debug_mode > 0):\n            angle_list = []\n            for i in range(5):\n                angle = self.hk_vector_2d_angle([(results[0]-results[i*8+4]), (results[1]-results[i*8+5])],[(results[i*8+6]-results[i*8+8]),(results[i*8+7]-results[i*8+9])])\n                angle_list.append(angle)\n            thr_angle,thr_angle_thumb,thr_angle_s,gesture_str = 65.,53.,49.,None\n            if 65535. not in angle_list:\n                if (angle_list[0]>thr_angle_thumb)  and (angle_list[1]>thr_angle) and (angle_list[2]>thr_angle) and (angle_list[3]>thr_angle) and (angle_list[4]>thr_angle):\n                    gesture_str = "fist"\n                elif (angle_list[0]<thr_angle_s)  and (angle_list[1]<thr_angle_s) and (angle_list[2]<thr_angle_s) and (angle_list[3]<thr_angle_s) and (angle_list[4]<thr_angle_s):\n                    gesture_str = "five"\n                elif (angle_list[0]<thr_angle_s)  and (angle_list[1]<thr_angle_s) and (angle_list[2]>thr_angle) and (angle_list[3]>thr_angle) and (angle_list[4]>thr_angle):\n                    gesture_str = "gun"\n                elif (angle_list[0]<thr_angle_s)  and (angle_list[1]<thr_angle_s) and (angle_list[2]>thr_angle) and (angle_list[3]>thr_angle) and (angle_list[4]<thr_angle_s):\n                    gesture_str = "love"\n                elif (angle_list[0]>5)  and (angle_list[1]<thr_angle_s) and (angle_list[2]>thr_angle) and (angle_list[3]>thr_angle) and (angle_list[4]>thr_angle):\n                    gesture_str = "one"\n                elif (angle_list[0]<thr_angle_s)  and (angle_list[1]>thr_angle) and (angle_list[2]>thr_angle) and (angle_list[3]>thr_angle) and (angle_list[4]<thr_angle_s):\n                    gesture_str = "six"\n                elif (angle_list[0]>thr_angle_thumb)  and (angle_list[1]<thr_angle_s) and (angle_list[2]<thr_angle_s) and (angle_list[3]<thr_angle_s) and (angle_list[4]>thr_angle):\n                    gesture_str = "three"\n                elif (angle_list[0]<thr_angle_s)  and (angle_list[1]>thr_angle) and (angle_list[2]>thr_angle) and (angle_list[3]>thr_angle) and (angle_list[4]>thr_angle):\n                    gesture_str = "thumbUp"\n                elif (angle_list[0]>thr_angle_thumb)  and (angle_list[1]<thr_angle_s) and (angle_list[2]<thr_angle_s) and (angle_list[3]>thr_angle) and (angle_list[4]>thr_angle):\n                    gesture_str = "yeah"\n            return gesture_str\n\n# \u81ea\u5b9a\u4e49\u52a8\u6001\u624b\u52bf\u8bc6\u522b\u4efb\u52a1\u7c7b\nclass DynamicGestureApp(AIBase):\n    def __init__(self,kmodel_path,model_input_size,rgb888p_size=[1920,1080],display_size=[1920,1080],debug_mode=0):\n        super().__init__(kmodel_path,model_input_size,rgb888p_size,debug_mode)\n        # kmodel\u8def\u5f84\n        self.kmodel_path=kmodel_path\n        # \u68c0\u6d4b\u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.model_input_size=model_input_size\n        # sensor\u7ed9\u5230AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.rgb888p_size=[ALIGN_UP(rgb888p_size[0],16),rgb888p_size[1]]\n        # \u89c6\u9891\u8f93\u51faVO\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.display_size=[ALIGN_UP(display_size[0],16),display_size[1]]\n        # debug\u6a21\u5f0f\n        self.debug_mode=debug_mode\n        # \u6ce8\u610f\uff1aai2d\u8bbe\u7f6e\u591a\u4e2a\u9884\u5904\u7406\u65f6\u6267\u884c\u7684\u987a\u5e8f\u4e3a\uff1acrop->shift->resize/affine->pad\uff0c\u5982\u679c\u4e0d\u7b26\u5408\u8be5\u987a\u5e8f\uff0c\u9700\u8981\u914d\u7f6e\u591a\u4e2aai2d\u5bf9\u8c61;\n        # \u5982\u4e0b\u6a21\u578b\u9884\u5904\u7406\u8981\u5148\u505aresize\u518d\u505acrop\uff0c\u56e0\u6b64\u8981\u914d\u7f6e\u4e24\u4e2aAi2d\u5bf9\u8c61\n        self.ai2d_resize=Ai2d(debug_mode)\n        self.ai2d_resize.set_ai2d_dtype(nn.ai2d_format.NCHW_FMT,nn.ai2d_format.NCHW_FMT,np.uint8, np.uint8)\n\n        self.ai2d_crop=Ai2d(debug_mode)\n        self.ai2d_crop.set_ai2d_dtype(nn.ai2d_format.NCHW_FMT,nn.ai2d_format.NCHW_FMT,np.uint8, np.uint8)\n\n        # \u52a8\u6001\u624b\u52bf\u8bc6\u522b\u6a21\u578b\u8f93\u5165tensors\u5217\u8868\n        self.input_tensors=[]\n        # \u52a8\u6001\u624b\u52bf\u8bc6\u522b\u6a21\u578b\u7684\u8f93\u5165tensor\u7684shape\n        self.gesture_kmodel_input_shape = [[1, 3, 224, 224],                     # \u52a8\u6001\u624b\u52bf\u8bc6\u522bkmodel\u8f93\u5165\u5206\u8fa8\u7387\n                                           [1,3,56,56],\n                                           [1,4,28,28],\n                                           [1,4,28,28],\n                                           [1,8,14,14],\n                                           [1,8,14,14],\n                                           [1,8,14,14],\n                                           [1,12,14,14],\n                                           [1,12,14,14],\n                                           [1,20,7,7],\n                                           [1,20,7,7]]\n        # \u9884\u5904\u7406\u53c2\u6570\n        self.resize_shape = 256\n        self.mean_values = np.array([0.485, 0.456, 0.406]).reshape((3,1,1))      # \u52a8\u6001\u624b\u52bf\u8bc6\u522b\u9884\u5904\u7406\u5747\u503c\n        self.std_values = np.array([0.229, 0.224, 0.225]).reshape((3,1,1))       # \u52a8\u6001\u624b\u52bf\u8bc6\u522b\u9884\u5904\u7406\u65b9\u5dee\n        self.first_data=None\n        self.max_hist_len=20\n        self.crop_params=self.get_crop_param()\n\n    # \u914d\u7f6e\u9884\u5904\u7406\n    def config_preprocess(self,input_image_size=None):\n        with ScopedTiming("set preprocess config",self.debug_mode > 0):\n            # \u521d\u59cb\u5316ai2d\u9884\u5904\u7406\u914d\u7f6e\n            ai2d_input_size=input_image_size if input_image_size else self.rgb888p_size\n            # \u914d\u7f6eresize\u548ccrop\u9884\u5904\u7406\n            self.ai2d_resize.resize(nn.interp_method.tf_bilinear, nn.interp_mode.half_pixel)\n            self.ai2d_resize.build([1,3,ai2d_input_size[1],ai2d_input_size[0]],[1,3,self.crop_params[1],self.crop_params[0]])\n            self.ai2d_crop.crop(self.crop_params[2],self.crop_params[3],self.crop_params[4],self.crop_params[5])\n            self.ai2d_crop.build([1,3,self.crop_params[1],self.crop_params[0]],[1,3,self.model_input_size[1],self.model_input_size[0]])\n            # \u521d\u59cb\u5316\u52a8\u6001\u624b\u52bf\u8bc6\u522b\u6a21\u578b\u8f93\u5165\u5217\u8868\n            inputs_num=self.get_kmodel_inputs_num()\n            self.first_data = np.ones(self.gesture_kmodel_input_shape[0], dtype=np.float)\n            for i in range(inputs_num):\n                data = np.zeros(self.gesture_kmodel_input_shape[i], dtype=np.float)\n                self.input_tensors.append(nn.from_numpy(data))\n\n    # \u91cd\u5199\u9884\u5904\u7406\uff0c\u56e0\u4e3a\u8be5\u90e8\u5206\u4e0d\u662f\u5355\u7eaf\u7684\u8d70\u4e00\u4e2aai2d\u505a\u9884\u5904\u7406\uff0c\u6240\u4ee5\u8be5\u51fd\u6570\u9700\u8981\u91cd\u5199\n    def preprocess(self,input_np):\n        # \u5148\u8d70resize\uff0c\u518d\u8d70crop\n        resize_tensor=self.ai2d_resize.run(input_np)\n        crop_output_tensor=self.ai2d_crop.run(resize_tensor.to_numpy())\n        ai2d_output = crop_output_tensor.to_numpy()\n        self.first_data[0] = ai2d_output[0].copy()\n        self.first_data[0] = (self.first_data[0]*1.0/255 -self.mean_values)/self.std_values\n        self.input_tensors[0]=nn.from_numpy(self.first_data)\n        return\n\n    # run\u51fd\u6570\u91cd\u5199\n    def run(self,input_np,his_logit,history):\n        # \u9884\u5904\u7406\n        self.preprocess(input_np)\n        # \u63a8\u7406\n        outputs=self.inference(self.input_tensors)\n        # \u4f7f\u7528\u5f53\u524d\u5e27\u7684\u8f93\u51fa\u66f4\u65b0\u4e0b\u4e00\u5e27\u7684\u8f93\u5165\u5217\u8868\n        outputs_num=self.get_kmodel_outputs_num()\n        for i in range(1,outputs_num):\n            self.input_tensors[i]=nn.from_numpy(outputs[i])\n        # \u8fd4\u56de\u540e\u5904\u7406\u7ed3\u679c\n        return self.postprocess(outputs,his_logit,history)\n\n    # \u81ea\u5b9a\u4e49\u540e\u5904\u7406\n    def postprocess(self,results,his_logit, history):\n        with ScopedTiming("postprocess",self.debug_mode > 0):\n            his_logit.append(results[0])\n            avg_logit = sum(np.array(his_logit))\n            idx_ = np.argmax(avg_logit)\n            idx = self.gesture_process_output(idx_, history)\n            if (idx_ != idx):\n                his_logit_last = his_logit[-1]\n                his_logit = []\n                his_logit.append(his_logit_last)\n            return idx, avg_logit\n\n    # \u624b\u52bf\u5904\u7406\u51fd\u6570\n    def gesture_process_output(self,pred,history):\n        if (pred == 7 or pred == 8 or pred == 21 or pred == 22 or pred == 3 ):\n            pred = history[-1]\n        if (pred == 0 or pred == 4 or pred == 6 or pred == 9 or pred == 14 or pred == 1 or pred == 19 or pred == 20 or pred == 23 or pred == 24) :\n            pred = history[-1]\n        if (pred == 0) :\n            pred = 2\n        if (pred != history[-1]) :\n            if (len(history)>= 2) :\n                if (history[-1] != history[len(history)-2]) :\n                    pred = history[-1]\n        history.append(pred)\n        if (len(history) > self.max_hist_len) :\n            history = history[-self.max_hist_len:]\n        return history[-1]\n\n    # \u8ba1\u7b97crop\u53c2\u6570\n    def get_crop_param(self):\n        ori_w = self.rgb888p_size[0]\n        ori_h = self.rgb888p_size[1]\n        width = self.model_input_size[0]\n        height = self.model_input_size[1]\n        ratiow = float(self.resize_shape) / ori_w\n        ratioh = float(self.resize_shape) / ori_h\n        if ratiow < ratioh:\n            ratio = ratioh\n        else:\n            ratio = ratiow\n        new_w = int(ratio * ori_w)\n        new_h = int(ratio * ori_h)\n        top = int((new_h-height)/2)\n        left = int((new_w-width)/2)\n        return new_w,new_h,left,top,width,height\n\n    # \u91cd\u5199\u9006\u521d\u59cb\u5316\n    def deinit(self):\n        with ScopedTiming("deinit",self.debug_mode > 0):\n            del self.kpu\n            del self.ai2d_resize\n            del self.ai2d_crop\n            self.tensors.clear()\n            del self.tensors\n            gc.collect()\n            nn.shrink_memory_pool()\n            os.exitpoint(os.EXITPOINT_ENABLE_SLEEP)\n            time.sleep_ms(100)\n\n# \u81ea\u5b9a\u4e49\u52a8\u6001\u624b\u52bf\u8bc6\u522b\u4efb\u52a1\nclass DynamicGesture:\n    def __init__(self,hand_det_kmodel,hand_kp_kmodel,gesture_kmodel,det_input_size,kp_input_size,gesture_input_size,labels,anchors,confidence_threshold=0.25,nms_threshold=0.3,nms_option=False,strides=[8,16,32],rgb888p_size=[1280,720],display_size=[1920,1080],debug_mode=0):\n        # \u624b\u638c\u68c0\u6d4b\u6a21\u578b\u8def\u5f84\n        self.hand_det_kmodel=hand_det_kmodel\n        # \u624b\u638c\u5173\u952e\u70b9\u6a21\u578b\u8def\u5f84\n        self.hand_kp_kmodel=hand_kp_kmodel\n        # \u52a8\u6001\u624b\u52bf\u8bc6\u522b\u8def\u5f84\n        self.gesture_kmodel=gesture_kmodel\n        # \u624b\u638c\u68c0\u6d4b\u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.det_input_size=det_input_size\n        # \u624b\u638c\u5173\u952e\u70b9\u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.kp_input_size=kp_input_size\n        # \u52a8\u6001\u624b\u52bf\u8bc6\u522b\u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.gesture_input_size=gesture_input_size\n        self.labels=labels\n        # anchors\n        self.anchors=anchors\n        # \u7f6e\u4fe1\u5ea6\u9608\u503c\n        self.confidence_threshold=confidence_threshold\n        # nms\u9608\u503c\n        self.nms_threshold=nms_threshold\n        self.nms_option=nms_option\n        self.strides=strides\n        # sensor\u7ed9\u5230AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.rgb888p_size=[ALIGN_UP(rgb888p_size[0],16),rgb888p_size[1]]\n        # \u89c6\u9891\u8f93\u51faVO\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.display_size=[ALIGN_UP(display_size[0],16),display_size[1]]\n        # \u52a8\u6001\u624b\u52bf\u8bc6\u522b\u8d34\u56fe\n        self.bin_width = 150                                                     # \u52a8\u6001\u624b\u52bf\u8bc6\u522b\u5c4f\u5e55\u5750\u4e0a\u89d2\u6807\u5fd7\u72b6\u6001\u6587\u4ef6\u7684\u77ed\u8fb9\u5c3a\u5bf8\n        self.bin_height = 216                                                    # \u52a8\u6001\u624b\u52bf\u8bc6\u522b\u5c4f\u5e55\u5750\u4e0a\u89d2\u6807\u5fd7\u72b6\u6001\u6587\u4ef6\u7684\u957f\u8fb9\u5c3a\u5bf8\n        shang_argb = np.fromfile("/sdcard/app/tests/utils/shang.bin", dtype=np.uint8)\n        self.shang_argb = shang_argb.reshape((self.bin_height, self.bin_width, 4))\n        xia_argb = np.fromfile("/sdcard/app/tests/utils/xia.bin", dtype=np.uint8)\n        self.xia_argb = xia_argb.reshape((self.bin_height, self.bin_width, 4))\n        zuo_argb = np.fromfile("/sdcard/app/tests/utils/zuo.bin", dtype=np.uint8)\n        self.zuo_argb = zuo_argb.reshape((self.bin_width, self.bin_height, 4))\n        you_argb = np.fromfile("/sdcard/app/tests/utils/you.bin", dtype=np.uint8)\n        self.you_argb = you_argb.reshape((self.bin_width, self.bin_height, 4))\n        #\u5176\u4ed6\u53c2\u6570\n        self.TRIGGER = 0                                                         # \u52a8\u6001\u624b\u52bf\u8bc6\u522b\u5e94\u7528\u7684\u7ed3\u679c\u72b6\u6001\n        self.MIDDLE = 1\n        self.UP = 2\n        self.DOWN = 3\n        self.LEFT = 4\n        self.RIGHT = 5\n        self.max_hist_len = 20                                                   # \u6700\u591a\u5b58\u50a8\u591a\u5c11\u5e27\u7684\u7ed3\u679c\n        # debug_mode\u6a21\u5f0f\n        self.debug_mode=debug_mode\n        self.cur_state = self.TRIGGER\n        self.pre_state = self.TRIGGER\n        self.draw_state = self.TRIGGER\n        self.vec_flag = []\n        self.his_logit = []\n        self.history = [2]\n        self.s_start = time.time_ns()\n        self.m_start=None\n        self.hand_det=HandDetApp(self.hand_det_kmodel,self.labels,model_input_size=self.det_input_size,anchors=self.anchors,confidence_threshold=self.confidence_threshold,nms_threshold=self.nms_threshold,nms_option=self.nms_option,strides=self.strides,rgb888p_size=self.rgb888p_size,display_size=self.display_size,debug_mode=0)\n        self.hand_kp=HandKPClassApp(self.hand_kp_kmodel,model_input_size=self.kp_input_size,rgb888p_size=self.rgb888p_size,display_size=self.display_size)\n        self.dg=DynamicGestureApp(self.gesture_kmodel,model_input_size=self.gesture_input_size,rgb888p_size=self.rgb888p_size,display_size=self.display_size)\n        self.hand_det.config_preprocess()\n        self.dg.config_preprocess()\n\n    # run\u51fd\u6570\n    def run(self,input_np):\n        if self.cur_state == self.TRIGGER:\n            # \u624b\u638c\u68c0\u6d4b\n            det_boxes=self.hand_det.run(input_np)\n            boxes=[]\n            gesture_res=[]\n            for det_box in det_boxes:\n                # \u7b5b\u9009\u68c0\u6d4b\u6846\n                x1, y1, x2, y2 = det_box[2],det_box[3],det_box[4],det_box[5]\n                w,h= int(x2 - x1),int(y2 - y1)\n                if (h<(0.1*self.rgb888p_size[1])):\n                    continue\n                if (w<(0.25*self.rgb888p_size[0]) and ((x1<(0.03*self.rgb888p_size[0])) or (x2>(0.97*self.rgb888p_size[0])))):\n                    continue\n                if (w<(0.15*self.rgb888p_size[0]) and ((x1<(0.01*self.rgb888p_size[0])) or (x2>(0.99*self.rgb888p_size[0])))):\n                    continue\n                # \u624b\u638c\u5173\u952e\u70b9\u9884\u5904\u7406\u914d\u7f6e\n                self.hand_kp.config_preprocess(det_box)\n                # \u624b\u638c\u5173\u952e\u70b9\u68c0\u6d4b\n                hk_results,gesture_str=self.hand_kp.run(input_np)\n                boxes.append(det_box)\n                gesture_res.append((hk_results,gesture_str))\n            return boxes,gesture_res\n        else:\n            # \u52a8\u6001\u624b\u52bf\u8bc6\u522b\n            idx, avg_logit = self.dg.run(input_np, self.his_logit, self.history)\n            return idx,avg_logit\n\n    # \u6839\u636e\u8f93\u51fa\u7ed3\u679c\u7ed8\u5236\u6548\u679c\n    def draw_result(self,pl,output1,output2):\n        pl.osd_img.clear()\n        draw_img_np = np.zeros((self.display_size[1],self.display_size[0],4),dtype=np.uint8)\n        draw_img=image.Image(self.display_size[0], self.display_size[1], image.ARGB8888,alloc=image.ALLOC_REF,data=draw_img_np)\n        if self.cur_state == self.TRIGGER:\n            for i in range(len(output1)):\n                hk_results,gesture=output2[i][0],output2[i][1]\n                if ((gesture == "five") or (gesture == "yeah")):\n                    v_x = hk_results[24]-hk_results[0]\n                    v_y = hk_results[25]-hk_results[1]\n                    angle = self.hand_kp.hk_vector_2d_angle([v_x,v_y],[1.0,0.0])\n                    if (v_y>0):\n                        angle = 360-angle\n                    if ((70.0<=angle) and (angle<110.0)):\n                        if ((self.pre_state != self.UP) or (self.pre_state != self.MIDDLE)):\n                            self.vec_flag.append(self.pre_state)\n                        if ((len(self.vec_flag)>10)or(self.pre_state == self.UP) or (self.pre_state == self.MIDDLE) or(self.pre_state == self.TRIGGER)):\n                            draw_img_np[:self.bin_height,:self.bin_width,:] = self.shang_argb\n                            self.cur_state = self.UP\n                    elif ((110.0<=angle) and (angle<225.0)):                                                # \u624b\u6307\u5411\u53f3(\u5b9e\u9645\u65b9\u5411)\n                        if (self.pre_state != self.RIGHT):\n                            self.vec_flag.append(self.pre_state)\n                        if ((len(self.vec_flag)>10)or(self.pre_state == self.RIGHT)or(self.pre_state == self.TRIGGER)):\n                            draw_img_np[:self.bin_width,:self.bin_height,:] = self.you_argb\n                            self.cur_state = self.RIGHT\n                    elif((225.0<=angle) and (angle<315.0)):                                                 # \u624b\u6307\u5411\u4e0b\n                        if (self.pre_state != self.DOWN):\n                            self.vec_flag.append(self.pre_state)\n                        if ((len(self.vec_flag)>10)or(self.pre_state == self.DOWN)or(self.pre_state == self.TRIGGER)):\n                            draw_img_np[:self.bin_height,:self.bin_width,:] = self.xia_argb\n                            self.cur_state = self.DOWN\n                    else:                                                                                   # \u624b\u6307\u5411\u5de6(\u5b9e\u9645\u65b9\u5411)\n                        if (self.pre_state != self.LEFT):\n                            self.vec_flag.append(self.pre_state)\n                        if ((len(self.vec_flag)>10)or(self.pre_state == self.LEFT)or(self.pre_state == self.TRIGGER)):\n                            draw_img_np[:self.bin_width,:self.bin_height,:] = self.zuo_argb\n                            self.cur_state = self.LEFT\n                    self.m_start = time.time_ns()\n            self.his_logit = []\n        else:\n            idx,avg_logit=output1,output2[0]\n            if (self.cur_state == self.UP):\n                draw_img_np[:self.bin_height,:self.bin_width,:] = self.shang_argb\n                if ((idx==15) or (idx==10)):\n                    self.vec_flag.clear()\n                    if (((avg_logit[idx] >= 0.7) and (len(self.his_logit) >= 2)) or ((avg_logit[idx] >= 0.3) and (len(self.his_logit) >= 4))):\n                        self.s_start = time.time_ns()\n                        self.cur_state = self.TRIGGER\n                        self.draw_state = self.DOWN\n                        self.history = [2]\n                    self.pre_state = self.UP\n                elif ((idx==25)or(idx==26)) :\n                    self.vec_flag.clear()\n                    if (((avg_logit[idx] >= 0.4) and (len(self.his_logit) >= 2)) or ((avg_logit[idx] >= 0.3) and (len(self.his_logit) >= 3))):\n                        self.s_start = time.time_ns()\n                        self.cur_state = self.TRIGGER\n                        self.draw_state = self.MIDDLE\n                        self.history = [2]\n                    self.pre_state = self.MIDDLE\n                else:\n                    self.his_logit.clear()\n            elif (self.cur_state == self.RIGHT):\n                draw_img_np[:self.bin_width,:self.bin_height,:] = self.you_argb\n                if  ((idx==16)or(idx==11)) :\n                    self.vec_flag.clear()\n                    if (((avg_logit[idx] >= 0.4) and (len(self.his_logit) >= 2)) or ((avg_logit[idx] >= 0.3) and (len(self.his_logit) >= 3))):\n                        self.s_start = time.time_ns()\n                        self.cur_state = self.TRIGGER\n                        self.draw_state = self.RIGHT\n                        self.history = [2]\n                    self.pre_state = self.RIGHT\n                else:\n                    self.his_logit.clear()\n            elif (self.cur_state == self.DOWN):\n                draw_img_np[:self.bin_height,:self.bin_width,:] = self.xia_argb\n                if  ((idx==18)or(idx==13)):\n                    self.vec_flag.clear()\n                    if (((avg_logit[idx] >= 0.4) and (len(self.his_logit) >= 2)) or ((avg_logit[idx] >= 0.3) and (len(self.his_logit) >= 3))):\n                        self.s_start = time.time_ns()\n                        self.cur_state = self.TRIGGER\n                        self.draw_state = self.UP\n                        self.history = [2]\n                    self.pre_state = self.DOWN\n                else:\n                    self.his_logit.clear()\n            elif (self.cur_state == self.LEFT):\n                draw_img_np[:self.bin_width,:self.bin_height,:] = self.zuo_argb\n                if ((idx==17)or(idx==12)):\n                    self.vec_flag.clear()\n                    if (((avg_logit[idx] >= 0.4) and (len(self.his_logit) >= 2)) or ((avg_logit[idx] >= 0.3) and (len(self.his_logit) >= 3))):\n                        self.s_start = time.time_ns()\n                        self.cur_state = self.TRIGGER\n                        self.draw_state = self.LEFT\n                        self.history = [2]\n                    self.pre_state = self.LEFT\n                else:\n                    self.his_logit.clear()\n\n            self.elapsed_time = round((time.time_ns() - self.m_start)/1000000)\n\n            if ((self.cur_state != self.TRIGGER) and (self.elapsed_time>2000)):\n                self.cur_state = self.TRIGGER\n                self.pre_state = self.TRIGGER\n\n        self.elapsed_ms_show = round((time.time_ns()-self.s_start)/1000000)\n        if (self.elapsed_ms_show<1000):\n            if (self.draw_state == self.UP):\n                draw_img.draw_arrow(1068,330,1068,130, (255,170,190,230), thickness=13)                             # \u5224\u65ad\u4e3a\u5411\u4e0a\u6325\u52a8\u65f6\uff0c\u753b\u4e00\u4e2a\u5411\u4e0a\u7684\u7bad\u5934\n                draw_img.draw_string_advanced(self.display_size[0]//2-50,self.display_size[1]//2-50,32,"\u5411\u4e0a")\n            elif (self.draw_state == self.RIGHT):\n                draw_img.draw_arrow(1290,540,1536,540, (255,170,190,230), thickness=13)                             # \u5224\u65ad\u4e3a\u5411\u53f3\u6325\u52a8\u65f6\uff0c\u753b\u4e00\u4e2a\u5411\u53f3\u7684\u7bad\u5934\n                draw_img.draw_string_advanced(self.display_size[0]//2-50,self.display_size[1]//2-50,32,"\u5411\u53f3")\n            elif (self.draw_state == self.DOWN):\n                draw_img.draw_arrow(1068,750,1068,950, (255,170,190,230), thickness=13)                             # \u5224\u65ad\u4e3a\u5411\u4e0b\u6325\u52a8\u65f6\uff0c\u753b\u4e00\u4e2a\u5411\u4e0b\u7684\u7bad\u5934\n                draw_img.draw_string_advanced(self.display_size[0]//2-50,self.display_size[1]//2-50,32,"\u5411\u4e0b")\n            elif (self.draw_state == self.LEFT):\n                draw_img.draw_arrow(846,540,600,540, (255,170,190,230), thickness=13)                               # \u5224\u65ad\u4e3a\u5411\u5de6\u6325\u52a8\u65f6\uff0c\u753b\u4e00\u4e2a\u5411\u5de6\u7684\u7bad\u5934\n                draw_img.draw_string_advanced(self.display_size[0]//2-50,self.display_size[1]//2-50,32,"\u5411\u5de6")\n            elif (self.draw_state == self.MIDDLE):\n                draw_img.draw_circle(1068,540,100, (255,170,190,230), thickness=2, fill=True)                       # \u5224\u65ad\u4e3a\u4e94\u6307\u634f\u5408\u624b\u52bf\u65f6\uff0c\u753b\u4e00\u4e2a\u5b9e\u5fc3\u5706\n                draw_img.draw_string_advanced(self.display_size[0]//2-50,self.display_size[1]//2-50,32,"\u4e2d\u95f4")\n        else:\n            self.draw_state = self.TRIGGER\n        pl.osd_img.copy_from(draw_img)\n\n\nif __name__=="__main__":\n    # \u663e\u793a\u6a21\u5f0f\uff0c\u9ed8\u8ba4"hdmi",\u53ef\u4ee5\u9009\u62e9"hdmi"\u548c"lcd"\n    display_mode="hdmi"\n    if display_mode=="hdmi":\n        display_size=[1920,1080]\n    else:\n        display_size=[800,480]\n    # \u624b\u638c\u68c0\u6d4b\u6a21\u578b\u8def\u5f84\n    hand_det_kmodel_path="/sdcard/app/tests/kmodel/hand_det.kmodel"\n    # \u624b\u90e8\u5173\u952e\u70b9\u6a21\u578b\u8def\u5f84\n    hand_kp_kmodel_path="/sdcard/app/tests/kmodel/handkp_det.kmodel"\n    # \u52a8\u6001\u624b\u52bf\u8bc6\u522b\u6a21\u578b\u8def\u5f84\n    gesture_kmodel_path="/sdcard/app/tests/kmodel/gesture.kmodel"\n    # \u5176\u4ed6\u53c2\u6570\n    rgb888p_size=[1920,1080]\n    hand_det_input_size=[512,512]\n    hand_kp_input_size=[256,256]\n    gesture_input_size=[224,224]\n    confidence_threshold=0.2\n    nms_threshold=0.5\n    labels=["hand"]\n    anchors = [26,27, 53,52, 75,71, 80,99, 106,82, 99,134, 140,113, 161,172, 245,276]\n\n    # \u521d\u59cb\u5316PipeLine\uff0c\u53ea\u5173\u6ce8\u4f20\u7ed9AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u663e\u793a\u7684\u5206\u8fa8\u7387\n    pl=PipeLine(rgb888p_size=rgb888p_size,display_size=display_size,display_mode=display_mode)\n    pl.create()\n    # \u81ea\u5b9a\u4e49\u52a8\u6001\u624b\u52bf\u8bc6\u522b\u4efb\u52a1\u5b9e\u4f8b\n    dg=DynamicGesture(hand_det_kmodel_path,hand_kp_kmodel_path,gesture_kmodel_path,det_input_size=hand_det_input_size,kp_input_size=hand_kp_input_size,gesture_input_size=gesture_input_size,labels=labels,anchors=anchors,confidence_threshold=confidence_threshold,nms_threshold=nms_threshold,nms_option=False,strides=[8,16,32],rgb888p_size=rgb888p_size,display_size=display_size)\n    try:\n        while True:\n            os.exitpoint()\n            with ScopedTiming("total",1):\n                img=pl.get_frame()                 # \u83b7\u53d6\u5f53\u524d\u5e27\n                output1,output2=dg.run(img)        # \u63a8\u7406\u5f53\u524d\u5e27\n                dg.draw_result(pl,output1,output2) # \u7ed8\u5236\u63a8\u7406\u7ed3\u679c\n                pl.show_image()                    # \u5c55\u793a\u63a8\u7406\u7ed3\u679c\n                gc.collect()\n    except Exception as e:\n        sys.print_exception(e)\n    finally:\n        dg.hand_det.deinit()\n        dg.hand_kp.deinit()\n        dg.dg.deinit()\n        pl.destroy()\n'})}),"\n",(0,i.jsx)(n.h3,{id:"22-\u6ce8\u89c6\u4f30\u8ba1",children:"2.2. \u6ce8\u89c6\u4f30\u8ba1"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:'from libs.PipeLine import PipeLine, ScopedTiming\nfrom libs.AIBase import AIBase\nfrom libs.AI2D import Ai2d\nimport os\nimport ujson\nfrom media.media import *\nfrom time import *\nimport nncase_runtime as nn\nimport ulab.numpy as np\nimport time\nimport image\nimport aidemo\nimport random\nimport gc\nimport sys\nimport math\n\n# \u81ea\u5b9a\u4e49\u4eba\u8138\u68c0\u6d4b\u4efb\u52a1\u7c7b\nclass FaceDetApp(AIBase):\n    def __init__(self,kmodel_path,model_input_size,anchors,confidence_threshold=0.25,nms_threshold=0.3,rgb888p_size=[1280,720],display_size=[1920,1080],debug_mode=0):\n        super().__init__(kmodel_path,model_input_size,rgb888p_size,debug_mode)\n        # kmodel\u8def\u5f84\n        self.kmodel_path=kmodel_path\n        # \u68c0\u6d4b\u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.model_input_size=model_input_size\n        # \u7f6e\u4fe1\u5ea6\u9608\u503c\n        self.confidence_threshold=confidence_threshold\n        # nms\u9608\u503c\n        self.nms_threshold=nms_threshold\n        self.anchors=anchors\n        # sensor\u7ed9\u5230AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.rgb888p_size=[ALIGN_UP(rgb888p_size[0],16),rgb888p_size[1]]\n        # \u89c6\u9891\u8f93\u51faVO\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.display_size=[ALIGN_UP(display_size[0],16),display_size[1]]\n        # debug\u6a21\u5f0f\n        self.debug_mode=debug_mode\n        # Ai2d\u5b9e\u4f8b\uff0c\u7528\u4e8e\u5b9e\u73b0\u6a21\u578b\u9884\u5904\u7406\n        self.ai2d=Ai2d(debug_mode)\n        # \u8bbe\u7f6eAi2d\u7684\u8f93\u5165\u8f93\u51fa\u683c\u5f0f\u548c\u7c7b\u578b\n        self.ai2d.set_ai2d_dtype(nn.ai2d_format.NCHW_FMT,nn.ai2d_format.NCHW_FMT,np.uint8, np.uint8)\n\n    # \u914d\u7f6e\u9884\u5904\u7406\u64cd\u4f5c\uff0c\u8fd9\u91cc\u4f7f\u7528\u4e86padding\u548cresize\uff0cAi2d\u652f\u6301crop/shift/pad/resize/affine\uff0c\u5177\u4f53\u4ee3\u7801\u8bf7\u6253\u5f00/sdcard/app/libs/AI2D.py\u67e5\u770b\n    def config_preprocess(self,input_image_size=None):\n        with ScopedTiming("set preprocess config",self.debug_mode > 0):\n            # \u521d\u59cb\u5316ai2d\u9884\u5904\u7406\u914d\u7f6e\uff0c\u9ed8\u8ba4\u4e3asensor\u7ed9\u5230AI\u7684\u5c3a\u5bf8\uff0c\u53ef\u4ee5\u901a\u8fc7\u8bbe\u7f6einput_image_size\u81ea\u884c\u4fee\u6539\u8f93\u5165\u5c3a\u5bf8\n            ai2d_input_size=input_image_size if input_image_size else self.rgb888p_size\n            # \u8bbe\u7f6epadding\u9884\u5904\u7406\n            self.ai2d.pad(self.get_pad_param(), 0, [104,117,123])\n            # \u8bbe\u7f6eresize\u9884\u5904\u7406\n            self.ai2d.resize(nn.interp_method.tf_bilinear, nn.interp_mode.half_pixel)\n            # \u6784\u5efa\u9884\u5904\u7406\u6d41\u7a0b,\u53c2\u6570\u4e3a\u9884\u5904\u7406\u8f93\u5165tensor\u7684shape\u548c\u9884\u5904\u7406\u8f93\u51fa\u7684tensor\u7684shape\n            self.ai2d.build([1,3,ai2d_input_size[1],ai2d_input_size[0]],[1,3,self.model_input_size[1],self.model_input_size[0]])\n\n    # \u81ea\u5b9a\u4e49\u4efb\u52a1\u540e\u5904\u7406,\u8fd9\u91cc\u4f7f\u7528\u4e86aidemo\u5e93\u7684face_det_post_process\u63a5\u53e3\n    def postprocess(self,results):\n        with ScopedTiming("postprocess",self.debug_mode > 0):\n            res = aidemo.face_det_post_process(self.confidence_threshold,self.nms_threshold,self.model_input_size[0],self.anchors,self.rgb888p_size,results)\n            if len(res)==0:\n                return res\n            else:\n                return res[0]\n\n    # \u8ba1\u7b97padding\u53c2\u6570\n    def get_pad_param(self):\n        dst_w = self.model_input_size[0]\n        dst_h = self.model_input_size[1]\n        # \u8ba1\u7b97\u6700\u5c0f\u7684\u7f29\u653e\u6bd4\u4f8b\uff0c\u7b49\u6bd4\u4f8b\u7f29\u653e\n        ratio_w = dst_w / self.rgb888p_size[0]\n        ratio_h = dst_h / self.rgb888p_size[1]\n        if ratio_w < ratio_h:\n            ratio = ratio_w\n        else:\n            ratio = ratio_h\n        new_w = (int)(ratio * self.rgb888p_size[0])\n        new_h = (int)(ratio * self.rgb888p_size[1])\n        dw = (dst_w - new_w) / 2\n        dh = (dst_h - new_h) / 2\n        top = (int)(round(0))\n        bottom = (int)(round(dh * 2 + 0.1))\n        left = (int)(round(0))\n        right = (int)(round(dw * 2 - 0.1))\n        return [0,0,0,0,top, bottom, left, right]\n\n# \u81ea\u5b9a\u4e49\u6ce8\u89c6\u4f30\u8ba1\u4efb\u52a1\u7c7b\nclass EyeGazeApp(AIBase):\n    def __init__(self,kmodel_path,model_input_size,rgb888p_size=[1920,1080],display_size=[1920,1080],debug_mode=0):\n        super().__init__(kmodel_path,model_input_size,rgb888p_size,debug_mode)\n        # kmodel\u8def\u5f84\n        self.kmodel_path=kmodel_path\n        # \u6ce8\u89c6\u4f30\u8ba1\u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.model_input_size=model_input_size\n        # sensor\u7ed9\u5230AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.rgb888p_size=[ALIGN_UP(rgb888p_size[0],16),rgb888p_size[1]]\n        # \u89c6\u9891\u8f93\u51faVO\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.display_size=[ALIGN_UP(display_size[0],16),display_size[1]]\n        # debug\u6a21\u5f0f\n        self.debug_mode=debug_mode\n        # Ai2d\u5b9e\u4f8b\uff0c\u7528\u4e8e\u5b9e\u73b0\u6a21\u578b\u9884\u5904\u7406\n        self.ai2d=Ai2d(debug_mode)\n        # \u8bbe\u7f6eAi2d\u7684\u8f93\u5165\u8f93\u51fa\u683c\u5f0f\u548c\u7c7b\u578b\n        self.ai2d.set_ai2d_dtype(nn.ai2d_format.NCHW_FMT,nn.ai2d_format.NCHW_FMT,np.uint8, np.uint8)\n\n    # \u914d\u7f6e\u9884\u5904\u7406\u64cd\u4f5c\uff0c\u8fd9\u91cc\u4f7f\u7528\u4e86crop\u548cresize\uff0cAi2d\u652f\u6301crop/shift/pad/resize/affine\uff0c\u5177\u4f53\u4ee3\u7801\u8bf7\u6253\u5f00/sdcard/app/libs/AI2D.py\u67e5\u770b\n    def config_preprocess(self,det,input_image_size=None):\n        with ScopedTiming("set preprocess config",self.debug_mode > 0):\n            # \u521d\u59cb\u5316ai2d\u9884\u5904\u7406\u914d\u7f6e\n            ai2d_input_size=input_image_size if input_image_size else self.rgb888p_size\n            # \u8ba1\u7b97crop\u9884\u5904\u7406\u53c2\u6570\n            x, y, w, h = map(lambda x: int(round(x, 0)), det[:4])\n            # \u8bbe\u7f6ecrop\u9884\u5904\u7406\n            self.ai2d.crop(x,y,w,h)\n            # \u8bbe\u7f6eresize\u9884\u5904\u7406\n            self.ai2d.resize(nn.interp_method.tf_bilinear, nn.interp_mode.half_pixel)\n            # \u6784\u5efa\u9884\u5904\u7406\u6d41\u7a0b,\u53c2\u6570\u4e3a\u9884\u5904\u7406\u8f93\u5165tensor\u7684shape\u548c\u9884\u5904\u7406\u8f93\u51fa\u7684tensor\u7684shape\n            self.ai2d.build([1,3,ai2d_input_size[1],ai2d_input_size[0]],[1,3,self.model_input_size[1],self.model_input_size[0]])\n\n    # \u81ea\u5b9a\u4e49\u540e\u5904\u7406\uff0cresults\u662f\u6a21\u578b\u8f93\u51fa\u7684array\u5217\u8868\uff0c\u8fd9\u91cc\u8c03\u7528\u4e86aidemo\u5e93\u7684eye_gaze_post_process\u63a5\u53e3\n    def postprocess(self,results):\n        with ScopedTiming("postprocess",self.debug_mode > 0):\n            post_ret = aidemo.eye_gaze_post_process(results)\n            return post_ret[0],post_ret[1]\n\n# \u81ea\u5b9a\u4e49\u6ce8\u89c6\u4f30\u8ba1\u7c7b\nclass EyeGaze:\n    def __init__(self,face_det_kmodel,eye_gaze_kmodel,det_input_size,eye_gaze_input_size,anchors,confidence_threshold=0.25,nms_threshold=0.3,rgb888p_size=[1920,1080],display_size=[1920,1080],debug_mode=0):\n        # \u4eba\u8138\u68c0\u6d4b\u6a21\u578b\u8def\u5f84\n        self.face_det_kmodel=face_det_kmodel\n        # \u4eba\u8138\u6ce8\u89c6\u4f30\u8ba1\u6a21\u578b\u8def\u5f84\n        self.eye_gaze_kmodel=eye_gaze_kmodel\n        # \u4eba\u8138\u68c0\u6d4b\u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.det_input_size=det_input_size\n        # \u4eba\u8138\u6ce8\u89c6\u4f30\u8ba1\u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.eye_gaze_input_size=eye_gaze_input_size\n        # anchors\n        self.anchors=anchors\n        # \u7f6e\u4fe1\u5ea6\u9608\u503c\n        self.confidence_threshold=confidence_threshold\n        # nms\u9608\u503c\n        self.nms_threshold=nms_threshold\n        # sensor\u7ed9\u5230AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.rgb888p_size=[ALIGN_UP(rgb888p_size[0],16),rgb888p_size[1]]\n        # \u89c6\u9891\u8f93\u51faVO\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.display_size=[ALIGN_UP(display_size[0],16),display_size[1]]\n        # debug_mode\u6a21\u5f0f\n        self.debug_mode=debug_mode\n        # \u4eba\u8138\u68c0\u6d4b\u5b9e\u4f8b\n        self.face_det=FaceDetApp(self.face_det_kmodel,model_input_size=self.det_input_size,anchors=self.anchors,confidence_threshold=self.confidence_threshold,nms_threshold=self.nms_threshold,rgb888p_size=self.rgb888p_size,display_size=self.display_size,debug_mode=0)\n        # \u6ce8\u89c6\u4f30\u8ba1\u5b9e\u4f8b\n        self.eye_gaze=EyeGazeApp(self.eye_gaze_kmodel,model_input_size=self.eye_gaze_input_size,rgb888p_size=self.rgb888p_size,display_size=self.display_size)\n        # \u4eba\u8138\u68c0\u6d4b\u914d\u7f6e\u9884\u5904\u7406\n        self.face_det.config_preprocess()\n\n    #run\u65b9\u6cd5\n    def run(self,input_np):\n        # \u5148\u8fdb\u884c\u4eba\u8138\u68c0\u6d4b\n        det_boxes=self.face_det.run(input_np)\n        eye_gaze_res=[]\n        for det_box in det_boxes:\n            # \u5bf9\u6bcf\u4e00\u4e2a\u68c0\u6d4b\u5230\u7684\u4eba\u8138\u505a\u6ce8\u89c6\u4f30\u8ba1\n            self.eye_gaze.config_preprocess(det_box)\n            pitch,yaw=self.eye_gaze.run(input_np)\n            eye_gaze_res.append((pitch,yaw))\n        return det_boxes,eye_gaze_res\n\n    # \u7ed8\u5236\u6ce8\u89c6\u4f30\u8ba1\u6548\u679c\n    def draw_result(self,pl,dets,eye_gaze_res):\n        pl.osd_img.clear()\n        if dets:\n            for det,gaze_ret in zip(dets,eye_gaze_res):\n                pitch , yaw = gaze_ret\n                length = self.display_size[0]/ 2\n                x, y, w, h = map(lambda x: int(round(x, 0)), det[:4])\n                x = x * self.display_size[0] // self.rgb888p_size[0]\n                y = y * self.display_size[1] // self.rgb888p_size[1]\n                w = w * self.display_size[0] // self.rgb888p_size[0]\n                h = h * self.display_size[1] // self.rgb888p_size[1]\n                center_x = (x + w / 2.0)\n                center_y = (y + h / 2.0)\n                dx = -length * math.sin(pitch) * math.cos(yaw)\n                target_x = int(center_x + dx)\n                dy = -length * math.sin(yaw)\n                target_y = int(center_y + dy)\n                pl.osd_img.draw_arrow(int(center_x), int(center_y), target_x, target_y, color = (255,255,0,0), size = 30, thickness = 2)\n\n\nif __name__=="__main__":\n    # \u663e\u793a\u6a21\u5f0f\uff0c\u9ed8\u8ba4"hdmi",\u53ef\u4ee5\u9009\u62e9"hdmi"\u548c"lcd"\n    display_mode="hdmi"\n    if display_mode=="hdmi":\n        display_size=[1920,1080]\n    else:\n        display_size=[800,480]\n    # \u4eba\u8138\u68c0\u6d4b\u6a21\u578b\u8def\u5f84\n    face_det_kmodel_path="/sdcard/app/tests/kmodel/face_detection_320.kmodel"\n    # \u4eba\u8138\u6ce8\u89c6\u4f30\u8ba1\u6a21\u578b\u8def\u5f84\n    eye_gaze_kmodel_path="/sdcard/app/tests/kmodel/eye_gaze.kmodel"\n    # \u5176\u4ed6\u53c2\u6570\n    anchors_path="/sdcard/app/tests/utils/prior_data_320.bin"\n    rgb888p_size=[1920,1080]\n    face_det_input_size=[320,320]\n    eye_gaze_input_size=[448,448]\n    confidence_threshold=0.5\n    nms_threshold=0.2\n    anchor_len=4200\n    det_dim=4\n    anchors = np.fromfile(anchors_path, dtype=np.float)\n    anchors = anchors.reshape((anchor_len,det_dim))\n\n    # \u521d\u59cb\u5316PipeLine\uff0c\u53ea\u5173\u6ce8\u4f20\u7ed9AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u663e\u793a\u7684\u5206\u8fa8\u7387\n    pl=PipeLine(rgb888p_size=rgb888p_size,display_size=display_size,display_mode=display_mode)\n    pl.create()\n    eg=EyeGaze(face_det_kmodel_path,eye_gaze_kmodel_path,det_input_size=face_det_input_size,eye_gaze_input_size=eye_gaze_input_size,anchors=anchors,confidence_threshold=confidence_threshold,nms_threshold=nms_threshold,rgb888p_size=rgb888p_size,display_size=display_size)\n    try:\n        while True:\n            os.exitpoint()\n            with ScopedTiming("total",1):\n                img=pl.get_frame()                          # \u83b7\u53d6\u5f53\u524d\u5e27\n                det_boxes,eye_gaze_res=eg.run(img)          # \u63a8\u7406\u5f53\u524d\u5e27\n                eg.draw_result(pl,det_boxes,eye_gaze_res)   # \u7ed8\u5236\u63a8\u7406\u6548\u679c\n                pl.show_image()                             # \u5c55\u793a\u63a8\u7406\u6548\u679c\n                gc.collect()\n    except Exception as e:\n        sys.print_exception(e)\n    finally:\n        eg.face_det.deinit()\n        eg.eye_gaze.deinit()\n        pl.destroy()\n'})}),"\n",(0,i.jsx)(n.h3,{id:"23-\u4eba\u8138\u68c0\u6d4b",children:"2.3. \u4eba\u8138\u68c0\u6d4b"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:'from libs.PipeLine import PipeLine, ScopedTiming\nfrom libs.AIBase import AIBase\nfrom libs.AI2D import Ai2d\nimport os\nimport ujson\nfrom media.media import *\nfrom time import *\nimport nncase_runtime as nn\nimport ulab.numpy as np\nimport time\nimport utime\nimport image\nimport random\nimport gc\nimport sys\nimport aidemo\n\n# \u81ea\u5b9a\u4e49\u4eba\u8138\u68c0\u6d4b\u7c7b\uff0c\u7ee7\u627f\u81eaAIBase\u57fa\u7c7b\nclass FaceDetectionApp(AIBase):\n    def __init__(self, kmodel_path, model_input_size, anchors, confidence_threshold=0.5, nms_threshold=0.2, rgb888p_size=[224,224], display_size=[1920,1080], debug_mode=0):\n        super().__init__(kmodel_path, model_input_size, rgb888p_size, debug_mode)  # \u8c03\u7528\u57fa\u7c7b\u7684\u6784\u9020\u51fd\u6570\n        self.kmodel_path = kmodel_path  # \u6a21\u578b\u6587\u4ef6\u8def\u5f84\n        self.model_input_size = model_input_size  # \u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.confidence_threshold = confidence_threshold  # \u7f6e\u4fe1\u5ea6\u9608\u503c\n        self.nms_threshold = nms_threshold  # NMS\uff08\u975e\u6781\u5927\u503c\u6291\u5236\uff09\u9608\u503c\n        self.anchors = anchors  # \u951a\u70b9\u6570\u636e\uff0c\u7528\u4e8e\u76ee\u6807\u68c0\u6d4b\n        self.rgb888p_size = [ALIGN_UP(rgb888p_size[0], 16), rgb888p_size[1]]  # sensor\u7ed9\u5230AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u5e76\u5bf9\u5bbd\u5ea6\u8fdb\u884c16\u7684\u5bf9\u9f50\n        self.display_size = [ALIGN_UP(display_size[0], 16), display_size[1]]  # \u663e\u793a\u5206\u8fa8\u7387\uff0c\u5e76\u5bf9\u5bbd\u5ea6\u8fdb\u884c16\u7684\u5bf9\u9f50\n        self.debug_mode = debug_mode  # \u662f\u5426\u5f00\u542f\u8c03\u8bd5\u6a21\u5f0f\n        self.ai2d = Ai2d(debug_mode)  # \u5b9e\u4f8b\u5316Ai2d\uff0c\u7528\u4e8e\u5b9e\u73b0\u6a21\u578b\u9884\u5904\u7406\n        self.ai2d.set_ai2d_dtype(nn.ai2d_format.NCHW_FMT, nn.ai2d_format.NCHW_FMT, np.uint8, np.uint8)  # \u8bbe\u7f6eAi2d\u7684\u8f93\u5165\u8f93\u51fa\u683c\u5f0f\u548c\u7c7b\u578b\n\n    # \u914d\u7f6e\u9884\u5904\u7406\u64cd\u4f5c\uff0c\u8fd9\u91cc\u4f7f\u7528\u4e86pad\u548cresize\uff0cAi2d\u652f\u6301crop/shift/pad/resize/affine\uff0c\u5177\u4f53\u4ee3\u7801\u8bf7\u6253\u5f00/sdcard/app/libs/AI2D.py\u67e5\u770b\n    def config_preprocess(self, input_image_size=None):\n        with ScopedTiming("set preprocess config", self.debug_mode > 0):  # \u8ba1\u65f6\u5668\uff0c\u5982\u679cdebug_mode\u5927\u4e8e0\u5219\u5f00\u542f\n            ai2d_input_size = input_image_size if input_image_size else self.rgb888p_size  # \u521d\u59cb\u5316ai2d\u9884\u5904\u7406\u914d\u7f6e\uff0c\u9ed8\u8ba4\u4e3asensor\u7ed9\u5230AI\u7684\u5c3a\u5bf8\uff0c\u53ef\u4ee5\u901a\u8fc7\u8bbe\u7f6einput_image_size\u81ea\u884c\u4fee\u6539\u8f93\u5165\u5c3a\u5bf8\n            top, bottom, left, right = self.get_padding_param()  # \u83b7\u53d6padding\u53c2\u6570\n            self.ai2d.pad([0, 0, 0, 0, top, bottom, left, right], 0, [104, 117, 123])  # \u586b\u5145\u8fb9\u7f18\n            self.ai2d.resize(nn.interp_method.tf_bilinear, nn.interp_mode.half_pixel)  # \u7f29\u653e\u56fe\u50cf\n            self.ai2d.build([1,3,ai2d_input_size[1],ai2d_input_size[0]],[1,3,self.model_input_size[1],self.model_input_size[0]])  # \u6784\u5efa\u9884\u5904\u7406\u6d41\u7a0b\n\n    # \u81ea\u5b9a\u4e49\u5f53\u524d\u4efb\u52a1\u7684\u540e\u5904\u7406\uff0cresults\u662f\u6a21\u578b\u8f93\u51faarray\u5217\u8868\uff0c\u8fd9\u91cc\u4f7f\u7528\u4e86aidemo\u5e93\u7684face_det_post_process\u63a5\u53e3\n    def postprocess(self, results):\n        with ScopedTiming("postprocess", self.debug_mode > 0):\n            post_ret = aidemo.face_det_post_process(self.confidence_threshold, self.nms_threshold, self.model_input_size[1], self.anchors, self.rgb888p_size, results)\n            if len(post_ret) == 0:\n                return post_ret\n            else:\n                return post_ret[0]\n\n    # \u7ed8\u5236\u68c0\u6d4b\u7ed3\u679c\u5230\u753b\u9762\u4e0a\n    def draw_result(self, pl, dets):\n        with ScopedTiming("display_draw", self.debug_mode > 0):\n            if dets:\n                pl.osd_img.clear()  # \u6e05\u9664OSD\u56fe\u50cf\n                for det in dets:\n                    # \u5c06\u68c0\u6d4b\u6846\u7684\u5750\u6807\u8f6c\u6362\u4e3a\u663e\u793a\u5206\u8fa8\u7387\u4e0b\u7684\u5750\u6807\n                    x, y, w, h = map(lambda x: int(round(x, 0)), det[:4])\n                    x = x * self.display_size[0] // self.rgb888p_size[0]\n                    y = y * self.display_size[1] // self.rgb888p_size[1]\n                    w = w * self.display_size[0] // self.rgb888p_size[0]\n                    h = h * self.display_size[1] // self.rgb888p_size[1]\n                    pl.osd_img.draw_rectangle(x, y, w, h, color=(255, 255, 0, 255), thickness=2)  # \u7ed8\u5236\u77e9\u5f62\u6846\n            else:\n                pl.osd_img.clear()\n\n    # \u83b7\u53d6padding\u53c2\u6570\n    def get_padding_param(self):\n        dst_w = self.model_input_size[0]  # \u6a21\u578b\u8f93\u5165\u5bbd\u5ea6\n        dst_h = self.model_input_size[1]  # \u6a21\u578b\u8f93\u5165\u9ad8\u5ea6\n        ratio_w = dst_w / self.rgb888p_size[0]  # \u5bbd\u5ea6\u7f29\u653e\u6bd4\u4f8b\n        ratio_h = dst_h / self.rgb888p_size[1]  # \u9ad8\u5ea6\u7f29\u653e\u6bd4\u4f8b\n        ratio = min(ratio_w, ratio_h)  # \u53d6\u8f83\u5c0f\u7684\u7f29\u653e\u6bd4\u4f8b\n        new_w = int(ratio * self.rgb888p_size[0])  # \u65b0\u5bbd\u5ea6\n        new_h = int(ratio * self.rgb888p_size[1])  # \u65b0\u9ad8\u5ea6\n        dw = (dst_w - new_w) / 2  # \u5bbd\u5ea6\u5dee\n        dh = (dst_h - new_h) / 2  # \u9ad8\u5ea6\u5dee\n        top = int(round(0))\n        bottom = int(round(dh * 2 + 0.1))\n        left = int(round(0))\n        right = int(round(dw * 2 - 0.1))\n        return top, bottom, left, right\n\nif __name__ == "__main__":\n    # \u663e\u793a\u6a21\u5f0f\uff0c\u9ed8\u8ba4"hdmi",\u53ef\u4ee5\u9009\u62e9"hdmi"\u548c"lcd"\n    display_mode="hdmi"\n    if display_mode=="hdmi":\n        display_size=[1920,1080]\n    else:\n        display_size=[800,480]\n    # \u8bbe\u7f6e\u6a21\u578b\u8def\u5f84\u548c\u5176\u4ed6\u53c2\u6570\n    kmodel_path = "/sdcard/app/tests/kmodel/face_detection_320.kmodel"\n    # \u5176\u5b83\u53c2\u6570\n    confidence_threshold = 0.5\n    nms_threshold = 0.2\n    anchor_len = 4200\n    det_dim = 4\n    anchors_path = "/sdcard/app/tests/utils/prior_data_320.bin"\n    anchors = np.fromfile(anchors_path, dtype=np.float)\n    anchors = anchors.reshape((anchor_len, det_dim))\n    rgb888p_size = [1920, 1080]\n\n    # \u521d\u59cb\u5316PipeLine\uff0c\u7528\u4e8e\u56fe\u50cf\u5904\u7406\u6d41\u7a0b\n    pl = PipeLine(rgb888p_size=rgb888p_size, display_size=display_size, display_mode=display_mode)\n    pl.create()  # \u521b\u5efaPipeLine\u5b9e\u4f8b\n    # \u521d\u59cb\u5316\u81ea\u5b9a\u4e49\u4eba\u8138\u68c0\u6d4b\u5b9e\u4f8b\n    face_det = FaceDetectionApp(kmodel_path, model_input_size=[320, 320], anchors=anchors, confidence_threshold=confidence_threshold, nms_threshold=nms_threshold, rgb888p_size=rgb888p_size, display_size=display_size, debug_mode=0)\n    face_det.config_preprocess()  # \u914d\u7f6e\u9884\u5904\u7406\n\n    try:\n        while True:\n            os.exitpoint()                      # \u68c0\u67e5\u662f\u5426\u6709\u9000\u51fa\u4fe1\u53f7\n            with ScopedTiming("total",1):\n                img = pl.get_frame()            # \u83b7\u53d6\u5f53\u524d\u5e27\u6570\u636e\n                res = face_det.run(img)         # \u63a8\u7406\u5f53\u524d\u5e27\n                face_det.draw_result(pl, res)   # \u7ed8\u5236\u7ed3\u679c\n                pl.show_image()                 # \u663e\u793a\u7ed3\u679c\n                gc.collect()                    # \u5783\u573e\u56de\u6536\n    except Exception as e:\n        sys.print_exception(e)                  # \u6253\u5370\u5f02\u5e38\u4fe1\u606f\n    finally:\n        face_det.deinit()                       # \u53cd\u521d\u59cb\u5316\n        pl.destroy()                            # \u9500\u6bc1PipeLine\u5b9e\u4f8b\n'})}),"\n",(0,i.jsx)(n.h3,{id:"24-\u4eba\u8138\u5173\u952e\u90e8\u4f4d",children:"2.4. \u4eba\u8138\u5173\u952e\u90e8\u4f4d"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:'from libs.PipeLine import PipeLine, ScopedTiming\nfrom libs.AIBase import AIBase\nfrom libs.AI2D import Ai2d\nimport os\nimport ujson\nfrom media.media import *\nfrom time import *\nimport nncase_runtime as nn\nimport ulab.numpy as np\nimport time\nimport image\nimport aidemo\nimport random\nimport gc\nimport sys\n\n# \u81ea\u5b9a\u4e49\u4eba\u8138\u68c0\u6d4b\u4efb\u52a1\u7c7b\nclass FaceDetApp(AIBase):\n    def __init__(self,kmodel_path,model_input_size,anchors,confidence_threshold=0.25,nms_threshold=0.3,rgb888p_size=[1280,720],display_size=[1920,1080],debug_mode=0):\n        super().__init__(kmodel_path,model_input_size,rgb888p_size,debug_mode)\n        # kmodel\u8def\u5f84\n        self.kmodel_path=kmodel_path\n        # \u68c0\u6d4b\u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.model_input_size=model_input_size\n        # \u7f6e\u4fe1\u5ea6\u9608\u503c\n        self.confidence_threshold=confidence_threshold\n        # nms\u9608\u503c\n        self.nms_threshold=nms_threshold\n        # \u68c0\u6d4b\u4efb\u52a1\u951a\u6846\n        self.anchors=anchors\n        # sensor\u7ed9\u5230AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.rgb888p_size=[ALIGN_UP(rgb888p_size[0],16),rgb888p_size[1]]\n        # \u89c6\u9891\u8f93\u51faVO\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.display_size=[ALIGN_UP(display_size[0],16),display_size[1]]\n        # debug\u6a21\u5f0f\n        self.debug_mode=debug_mode\n        # \u5b9e\u4f8b\u5316Ai2d\uff0c\u7528\u4e8e\u5b9e\u73b0\u6a21\u578b\u9884\u5904\u7406\n        self.ai2d=Ai2d(debug_mode)\n        # \u8bbe\u7f6eAi2d\u7684\u8f93\u5165\u8f93\u51fa\u683c\u5f0f\u548c\u7c7b\u578b\n        self.ai2d.set_ai2d_dtype(nn.ai2d_format.NCHW_FMT,nn.ai2d_format.NCHW_FMT,np.uint8, np.uint8)\n\n    # \u914d\u7f6e\u9884\u5904\u7406\u64cd\u4f5c\uff0c\u8fd9\u91cc\u4f7f\u7528\u4e86pad\u548cresize\uff0cAi2d\u652f\u6301crop/shift/pad/resize/affine\uff0c\u5177\u4f53\u4ee3\u7801\u8bf7\u6253\u5f00/sdcard/app/libs/AI2D.py\u67e5\u770b\n    def config_preprocess(self,input_image_size=None):\n        with ScopedTiming("set preprocess config",self.debug_mode > 0):\n            # \u521d\u59cb\u5316ai2d\u9884\u5904\u7406\u914d\u7f6e\uff0c\u9ed8\u8ba4\u4e3asensor\u7ed9\u5230AI\u7684\u5c3a\u5bf8\uff0c\u53ef\u4ee5\u901a\u8fc7\u8bbe\u7f6einput_image_size\u81ea\u884c\u4fee\u6539\u8f93\u5165\u5c3a\u5bf8\n            ai2d_input_size=input_image_size if input_image_size else self.rgb888p_size\n            # \u8bbe\u7f6epadding\u9884\u5904\u7406\n            self.ai2d.pad(self.get_pad_param(), 0, [104,117,123])\n            # \u8bbe\u7f6eresize\u9884\u5904\u7406\n            self.ai2d.resize(nn.interp_method.tf_bilinear, nn.interp_mode.half_pixel)\n            # \u6784\u5efa\u9884\u5904\u7406\u6d41\u7a0b,\u53c2\u6570\u4e3a\u9884\u5904\u7406\u8f93\u5165tensor\u7684shape\u548c\u9884\u5904\u7406\u8f93\u51fa\u7684tensor\u7684shape\n            self.ai2d.build([1,3,ai2d_input_size[1],ai2d_input_size[0]],[1,3,self.model_input_size[1],self.model_input_size[0]])\n\n    # \u81ea\u5b9a\u4e49\u540e\u5904\u7406\uff0cresults\u662f\u6a21\u578b\u8f93\u51fa\u7684array\u5217\u8868\uff0c\u8fd9\u91cc\u4f7f\u7528\u4e86aidemo\u7684face_det_post_process\u5217\u8868\n    def postprocess(self,results):\n        with ScopedTiming("postprocess",self.debug_mode > 0):\n            res = aidemo.face_det_post_process(self.confidence_threshold,self.nms_threshold,self.model_input_size[0],self.anchors,self.rgb888p_size,results)\n            if len(res)==0:\n                return res\n            else:\n                return res[0]\n\n    # \u8ba1\u7b97padding\u53c2\u6570\n    def get_pad_param(self):\n        dst_w = self.model_input_size[0]\n        dst_h = self.model_input_size[1]\n        # \u8ba1\u7b97\u6700\u5c0f\u7684\u7f29\u653e\u6bd4\u4f8b\uff0c\u7b49\u6bd4\u4f8b\u7f29\u653e\n        ratio_w = dst_w / self.rgb888p_size[0]\n        ratio_h = dst_h / self.rgb888p_size[1]\n        if ratio_w < ratio_h:\n            ratio = ratio_w\n        else:\n            ratio = ratio_h\n        new_w = (int)(ratio * self.rgb888p_size[0])\n        new_h = (int)(ratio * self.rgb888p_size[1])\n        dw = (dst_w - new_w) / 2\n        dh = (dst_h - new_h) / 2\n        top = (int)(round(0))\n        bottom = (int)(round(dh * 2 + 0.1))\n        left = (int)(round(0))\n        right = (int)(round(dw * 2 - 0.1))\n        return [0,0,0,0,top, bottom, left, right]\n\n# \u81ea\u5b9a\u4e49\u4eba\u8138\u5173\u952e\u70b9\u4efb\u52a1\u7c7b\nclass FaceLandMarkApp(AIBase):\n    def __init__(self,kmodel_path,model_input_size,rgb888p_size=[1920,1080],display_size=[1920,1080],debug_mode=0):\n        super().__init__(kmodel_path,model_input_size,rgb888p_size,debug_mode)\n        # kmodel\u8def\u5f84\n        self.kmodel_path=kmodel_path\n        # \u5173\u952e\u70b9\u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.model_input_size=model_input_size\n        # sensor\u7ed9\u5230AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.rgb888p_size=[ALIGN_UP(rgb888p_size[0],16),rgb888p_size[1]]\n        # \u89c6\u9891\u8f93\u51faVO\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.display_size=[ALIGN_UP(display_size[0],16),display_size[1]]\n        # debug\u6a21\u5f0f\n        self.debug_mode=debug_mode\n        # \u76ee\u6807\u77e9\u9635\n        self.matrix_dst=None\n        self.ai2d=Ai2d(debug_mode)\n        self.ai2d.set_ai2d_dtype(nn.ai2d_format.NCHW_FMT,nn.ai2d_format.NCHW_FMT,np.uint8, np.uint8)\n\n    # \u914d\u7f6e\u9884\u5904\u7406\u64cd\u4f5c\uff0c\u8fd9\u91cc\u4f7f\u7528\u4e86affine\uff0cAi2d\u652f\u6301crop/shift/pad/resize/affine\uff0c\u5177\u4f53\u4ee3\u7801\u8bf7\u6253\u5f00/sdcard/app/libs/AI2D.py\u67e5\u770b\n    def config_preprocess(self,det,input_image_size=None):\n        with ScopedTiming("set preprocess config",self.debug_mode > 0):\n            # \u521d\u59cb\u5316ai2d\u9884\u5904\u7406\u914d\u7f6e\uff0c\u9ed8\u8ba4\u4e3asensor\u7ed9\u5230AI\u7684\u5c3a\u5bf8\uff0c\u53ef\u4ee5\u901a\u8fc7\u8bbe\u7f6einput_image_size\u81ea\u884c\u4fee\u6539\u8f93\u5165\u5c3a\u5bf8\n            ai2d_input_size=input_image_size if input_image_size else self.rgb888p_size\n            # \u8ba1\u7b97\u76ee\u6807\u77e9\u9635\uff0c\u5e76\u83b7\u53d6\u4eff\u5c04\u53d8\u6362\u77e9\u9635\n            self.matrix_dst = self.get_affine_matrix(det)\n            affine_matrix = [self.matrix_dst[0][0],self.matrix_dst[0][1],self.matrix_dst[0][2],\n                             self.matrix_dst[1][0],self.matrix_dst[1][1],self.matrix_dst[1][2]]\n            # \u8bbe\u7f6e\u4eff\u5c04\u53d8\u6362\u9884\u5904\u7406\n            self.ai2d.affine(nn.interp_method.cv2_bilinear,0, 0, 127, 1,affine_matrix)\n            # \u6784\u5efa\u9884\u5904\u7406\u6d41\u7a0b,\u53c2\u6570\u4e3a\u9884\u5904\u7406\u8f93\u5165tensor\u7684shape\u548c\u9884\u5904\u7406\u8f93\u51fa\u7684tensor\u7684shape\n            self.ai2d.build([1,3,ai2d_input_size[1],ai2d_input_size[0]],[1,3,self.model_input_size[1],self.model_input_size[0]])\n\n    # \u81ea\u5b9a\u4e49\u540e\u5904\u7406\uff0cresults\u662f\u6a21\u578b\u8f93\u51fa\u7684array\u5217\u8868\uff0c\u8fd9\u91cc\u4f7f\u7528\u4e86aidemo\u5e93\u7684invert_affine_transform\u63a5\u53e3\n    def postprocess(self,results):\n        with ScopedTiming("postprocess",self.debug_mode > 0):\n            pred=results[0]\n            # \uff081\uff09\u5c06\u4eba\u8138\u5173\u952e\u70b9\u8f93\u51fa\u53d8\u6362\u6a21\u578b\u8f93\u5165\n            half_input_len = self.model_input_size[0] // 2\n            pred = pred.flatten()\n            for i in range(len(pred)):\n                pred[i] += (pred[i] + 1) * half_input_len\n            # \uff082\uff09\u83b7\u53d6\u4eff\u5c04\u77e9\u9635\u7684\u9006\u77e9\u9635\n            matrix_dst_inv = aidemo.invert_affine_transform(self.matrix_dst)\n            matrix_dst_inv = matrix_dst_inv.flatten()\n            # \uff083\uff09\u5bf9\u6bcf\u4e2a\u5173\u952e\u70b9\u8fdb\u884c\u9006\u53d8\u6362\n            half_out_len = len(pred) // 2\n            for kp_id in range(half_out_len):\n                old_x = pred[kp_id * 2]\n                old_y = pred[kp_id * 2 + 1]\n                # \u9006\u53d8\u6362\u516c\u5f0f\n                new_x = old_x * matrix_dst_inv[0] + old_y * matrix_dst_inv[1] + matrix_dst_inv[2]\n                new_y = old_x * matrix_dst_inv[3] + old_y * matrix_dst_inv[4] + matrix_dst_inv[5]\n                pred[kp_id * 2] = new_x\n                pred[kp_id * 2 + 1] = new_y\n            return pred\n\n    def get_affine_matrix(self,bbox):\n        # \u83b7\u53d6\u4eff\u5c04\u77e9\u9635\uff0c\u7528\u4e8e\u5c06\u8fb9\u754c\u6846\u6620\u5c04\u5230\u6a21\u578b\u8f93\u5165\u7a7a\u95f4\n        with ScopedTiming("get_affine_matrix", self.debug_mode > 1):\n            # \u4ece\u8fb9\u754c\u6846\u63d0\u53d6\u5750\u6807\u548c\u5c3a\u5bf8\n            x1, y1, w, h = map(lambda x: int(round(x, 0)), bbox[:4])\n            # \u8ba1\u7b97\u7f29\u653e\u6bd4\u4f8b\uff0c\u4f7f\u5f97\u8fb9\u754c\u6846\u6620\u5c04\u5230\u6a21\u578b\u8f93\u5165\u7a7a\u95f4\u7684\u4e00\u90e8\u5206\n            scale_ratio = (self.model_input_size[0]) / (max(w, h) * 1.5)\n            # \u8ba1\u7b97\u8fb9\u754c\u6846\u4e2d\u5fc3\u70b9\u5728\u6a21\u578b\u8f93\u5165\u7a7a\u95f4\u7684\u5750\u6807\n            cx = (x1 + w / 2) * scale_ratio\n            cy = (y1 + h / 2) * scale_ratio\n            # \u8ba1\u7b97\u6a21\u578b\u8f93\u5165\u7a7a\u95f4\u7684\u4e00\u534a\u957f\u5ea6\n            half_input_len = self.model_input_size[0] / 2\n            # \u521b\u5efa\u4eff\u5c04\u77e9\u9635\u5e76\u8fdb\u884c\u8bbe\u7f6e\n            matrix_dst = np.zeros((2, 3), dtype=np.float)\n            matrix_dst[0, 0] = scale_ratio\n            matrix_dst[0, 1] = 0\n            matrix_dst[0, 2] = half_input_len - cx\n            matrix_dst[1, 0] = 0\n            matrix_dst[1, 1] = scale_ratio\n            matrix_dst[1, 2] = half_input_len - cy\n            return matrix_dst\n\n# \u4eba\u8138\u6807\u5fd7\u89e3\u6790\nclass FaceLandMark:\n    def __init__(self,face_det_kmodel,face_landmark_kmodel,det_input_size,landmark_input_size,anchors,confidence_threshold=0.25,nms_threshold=0.3,rgb888p_size=[1920,1080],display_size=[1920,1080],debug_mode=0):\n        # \u4eba\u8138\u68c0\u6d4b\u6a21\u578b\u8def\u5f84\n        self.face_det_kmodel=face_det_kmodel\n        # \u4eba\u8138\u6807\u5fd7\u89e3\u6790\u6a21\u578b\u8def\u5f84\n        self.face_landmark_kmodel=face_landmark_kmodel\n        # \u4eba\u8138\u68c0\u6d4b\u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.det_input_size=det_input_size\n        # \u4eba\u8138\u6807\u5fd7\u89e3\u6790\u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.landmark_input_size=landmark_input_size\n        # anchors\n        self.anchors=anchors\n        # \u7f6e\u4fe1\u5ea6\u9608\u503c\n        self.confidence_threshold=confidence_threshold\n        # nms\u9608\u503c\n        self.nms_threshold=nms_threshold\n        # sensor\u7ed9\u5230AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.rgb888p_size=[ALIGN_UP(rgb888p_size[0],16),rgb888p_size[1]]\n        # \u89c6\u9891\u8f93\u51faVO\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.display_size=[ALIGN_UP(display_size[0],16),display_size[1]]\n        # debug_mode\u6a21\u5f0f\n        self.debug_mode=debug_mode\n\n        # \u4eba\u8138\u5173\u952e\u70b9\u4e0d\u540c\u90e8\u4f4d\u5173\u952e\u70b9\u5217\u8868\n        self.dict_kp_seq = [\n            [43, 44, 45, 47, 46, 50, 51, 49, 48],              # left_eyebrow\n            [97, 98, 99, 100, 101, 105, 104, 103, 102],        # right_eyebrow\n            [35, 36, 33, 37, 39, 42, 40, 41],                  # left_eye\n            [89, 90, 87, 91, 93, 96, 94, 95],                  # right_eye\n            [34, 88],                                          # pupil\n            [72, 73, 74, 86],                                  # bridge_nose\n            [77, 78, 79, 80, 85, 84, 83],                      # wing_nose\n            [52, 55, 56, 53, 59, 58, 61, 68, 67, 71, 63, 64],  # out_lip\n            [65, 54, 60, 57, 69, 70, 62, 66],                  # in_lip\n            [1, 9, 10, 11, 12, 13, 14, 15, 16, 2, 3, 4, 5, 6, 7, 8, 0, 24, 23, 22, 21, 20, 19, 18, 32, 31, 30, 29, 28, 27, 26, 25, 17]  # basin\n        ]\n\n        # \u4eba\u8138\u5173\u952e\u70b9\u4e0d\u540c\u90e8\u4f4d\uff08\u987a\u5e8f\u540cdict_kp_seq\uff09\u989c\u8272\u914d\u7f6e\uff0cargb\n        self.color_list_for_osd_kp = [\n            (255, 0, 255, 0),\n            (255, 0, 255, 0),\n            (255, 255, 0, 255),\n            (255, 255, 0, 255),\n            (255, 255, 0, 0),\n            (255, 255, 170, 0),\n            (255, 255, 255, 0),\n            (255, 0, 255, 255),\n            (255, 255, 220, 50),\n            (255, 30, 30, 255)\n        ]\n        # \u4eba\u8138\u68c0\u6d4b\u5b9e\u4f8b\n        self.face_det=FaceDetApp(self.face_det_kmodel,model_input_size=self.det_input_size,anchors=self.anchors,confidence_threshold=self.confidence_threshold,nms_threshold=self.nms_threshold,rgb888p_size=self.rgb888p_size,display_size=self.display_size,debug_mode=0)\n        # \u4eba\u8138\u6807\u5fd7\u89e3\u6790\u5b9e\u4f8b\n        self.face_landmark=FaceLandMarkApp(self.face_landmark_kmodel,model_input_size=self.landmark_input_size,rgb888p_size=self.rgb888p_size,display_size=self.display_size)\n        # \u914d\u7f6e\u4eba\u8138\u68c0\u6d4b\u7684\u9884\u5904\u7406\n        self.face_det.config_preprocess()\n\n    # run\u51fd\u6570\n    def run(self,input_np):\n        # \u6267\u884c\u4eba\u8138\u68c0\u6d4b\n        det_boxes=self.face_det.run(input_np)\n        landmark_res=[]\n        for det_box in det_boxes:\n            # \u5bf9\u6bcf\u4e00\u4e2a\u68c0\u6d4b\u5230\u7684\u4eba\u8138\u89e3\u6790\u5173\u952e\u90e8\u4f4d\n            self.face_landmark.config_preprocess(det_box)\n            res=self.face_landmark.run(input_np)\n            landmark_res.append(res)\n        return det_boxes,landmark_res\n\n\n    # \u7ed8\u5236\u4eba\u8138\u89e3\u6790\u6548\u679c\n    def draw_result(self,pl,dets,landmark_res):\n        pl.osd_img.clear()\n        if dets:\n            draw_img_np = np.zeros((self.display_size[1],self.display_size[0],4),dtype=np.uint8)\n            draw_img = image.Image(self.display_size[0], self.display_size[1], image.ARGB8888, alloc=image.ALLOC_REF,data = draw_img_np)\n            for pred in landmark_res:\n                # \uff081\uff09\u83b7\u53d6\u5355\u4e2a\u4eba\u8138\u6846\u5bf9\u5e94\u7684\u4eba\u8138\u5173\u952e\u70b9\n                for sub_part_index in range(len(self.dict_kp_seq)):\n                    # \uff082\uff09\u6784\u5efa\u4eba\u8138\u67d0\u4e2a\u533a\u57df\u5173\u952e\u70b9\u96c6\n                    sub_part = self.dict_kp_seq[sub_part_index]\n                    face_sub_part_point_set = []\n                    for kp_index in range(len(sub_part)):\n                        real_kp_index = sub_part[kp_index]\n                        x, y = pred[real_kp_index * 2], pred[real_kp_index * 2 + 1]\n                        x = int(x * self.display_size[0] // self.rgb888p_size[0])\n                        y = int(y * self.display_size[1] // self.rgb888p_size[1])\n                        face_sub_part_point_set.append((x, y))\n                    # \uff083\uff09\u753b\u4eba\u8138\u4e0d\u540c\u533a\u57df\u7684\u8f6e\u5ed3\n                    if sub_part_index in (9, 6):\n                        color = np.array(self.color_list_for_osd_kp[sub_part_index],dtype = np.uint8)\n                        face_sub_part_point_set = np.array(face_sub_part_point_set)\n                        aidemo.polylines(draw_img_np, face_sub_part_point_set,False,color,5,8,0)\n                    elif sub_part_index == 4:\n                        color = self.color_list_for_osd_kp[sub_part_index]\n                        for kp in face_sub_part_point_set:\n                            x,y = kp[0],kp[1]\n                            draw_img.draw_circle(x,y ,2, color, 1)\n                    else:\n                        color = np.array(self.color_list_for_osd_kp[sub_part_index],dtype = np.uint8)\n                        face_sub_part_point_set = np.array(face_sub_part_point_set)\n                        aidemo.contours(draw_img_np, face_sub_part_point_set,-1,color,2,8)\n            pl.osd_img.copy_from(draw_img)\n\n\nif __name__=="__main__":\n    # \u663e\u793a\u6a21\u5f0f\uff0c\u9ed8\u8ba4"hdmi",\u53ef\u4ee5\u9009\u62e9"hdmi"\u548c"lcd"\n    display_mode="hdmi"\n    if display_mode=="hdmi":\n        display_size=[1920,1080]\n    else:\n        display_size=[800,480]\n    # \u4eba\u8138\u68c0\u6d4b\u6a21\u578b\u8def\u5f84\n    face_det_kmodel_path="/sdcard/app/tests/kmodel/face_detection_320.kmodel"\n    # \u4eba\u8138\u5173\u952e\u6807\u5fd7\u6a21\u578b\u8def\u5f84\n    face_landmark_kmodel_path="/sdcard/app/tests/kmodel/face_landmark.kmodel"\n    # \u5176\u5b83\u53c2\u6570\n    anchors_path="/sdcard/app/tests/utils/prior_data_320.bin"\n    rgb888p_size=[1920,1080]\n    face_det_input_size=[320,320]\n    face_landmark_input_size=[192,192]\n    confidence_threshold=0.5\n    nms_threshold=0.2\n    anchor_len=4200\n    det_dim=4\n    anchors = np.fromfile(anchors_path, dtype=np.float)\n    anchors = anchors.reshape((anchor_len,det_dim))\n\n    # \u521d\u59cb\u5316PipeLine\uff0c\u53ea\u5173\u6ce8\u4f20\u7ed9AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u663e\u793a\u7684\u5206\u8fa8\u7387\n    pl=PipeLine(rgb888p_size=rgb888p_size,display_size=display_size,display_mode=display_mode)\n    pl.create()\n    flm=FaceLandMark(face_det_kmodel_path,face_landmark_kmodel_path,det_input_size=face_det_input_size,landmark_input_size=face_landmark_input_size,anchors=anchors,confidence_threshold=confidence_threshold,nms_threshold=nms_threshold,rgb888p_size=rgb888p_size,display_size=display_size)\n    try:\n        while True:\n            os.exitpoint()\n            with ScopedTiming("total",1):\n                img=pl.get_frame()                          # \u83b7\u53d6\u5f53\u524d\u5e27\n                det_boxes,landmark_res=flm.run(img)         # \u63a8\u7406\u5f53\u524d\u5e27\n                flm.draw_result(pl,det_boxes,landmark_res)  # \u7ed8\u5236\u63a8\u7406\u7ed3\u679c\n                pl.show_image()                             # \u5c55\u793a\u63a8\u7406\u6548\u679c\n                gc.collect()\n    except Exception as e:\n        sys.print_exception(e)\n    finally:\n        flm.face_det.deinit()\n        flm.face_landmark.deinit()\n        pl.destroy()\n'})}),"\n",(0,i.jsx)(n.h3,{id:"25-\u4eba\u81383d\u7f51\u7edc",children:"2.5. \u4eba\u81383D\u7f51\u7edc"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:'from libs.PipeLine import PipeLine, ScopedTiming\nfrom libs.AIBase import AIBase\nfrom libs.AI2D import Ai2d\nimport os\nimport ujson\nfrom media.media import *\nfrom time import *\nimport nncase_runtime as nn\nimport ulab.numpy as np\nimport time\nimport image\nimport aidemo\nimport random\nimport gc\nimport sys\n\n# \u81ea\u5b9a\u4e49\u4eba\u8138\u68c0\u6d4b\u4efb\u52a1\u7c7b\nclass FaceDetApp(AIBase):\n    def __init__(self,kmodel_path,model_input_size,anchors,confidence_threshold=0.25,nms_threshold=0.3,rgb888p_size=[1280,720],display_size=[1920,1080],debug_mode=0):\n        super().__init__(kmodel_path,model_input_size,rgb888p_size,debug_mode)\n        # kmodel\u8def\u5f84\n        self.kmodel_path=kmodel_path\n        # \u68c0\u6d4b\u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.model_input_size=model_input_size\n        # \u7f6e\u4fe1\u5ea6\u9608\u503c\n        self.confidence_threshold=confidence_threshold\n        # nms\u9608\u503c\n        self.nms_threshold=nms_threshold\n        # \u68c0\u6d4b\u4efb\u52a1\u951a\u6846\n        self.anchors=anchors\n        # sensor\u7ed9\u5230AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.rgb888p_size=[ALIGN_UP(rgb888p_size[0],16),rgb888p_size[1]]\n        # \u89c6\u9891\u8f93\u51faVO\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.display_size=[ALIGN_UP(display_size[0],16),display_size[1]]\n        # debug\u6a21\u5f0f\n        self.debug_mode=debug_mode\n        # \u5b9e\u4f8b\u5316Ai2d\uff0c\u7528\u4e8e\u5b9e\u73b0\u6a21\u578b\u9884\u5904\u7406\n        self.ai2d=Ai2d(debug_mode)\n        # \u8bbe\u7f6eAi2d\u7684\u8f93\u5165\u8f93\u51fa\u683c\u5f0f\u548c\u7c7b\u578b\n        self.ai2d.set_ai2d_dtype(nn.ai2d_format.NCHW_FMT,nn.ai2d_format.NCHW_FMT,np.uint8, np.uint8)\n\n    # \u914d\u7f6e\u9884\u5904\u7406\u64cd\u4f5c\uff0c\u8fd9\u91cc\u4f7f\u7528\u4e86pad\u548cresize\uff0cAi2d\u652f\u6301crop/shift/pad/resize/affine\uff0c\u5177\u4f53\u4ee3\u7801\u8bf7\u6253\u5f00/sdcard/app/libs/AI2D.py\u67e5\u770b\n    def config_preprocess(self,input_image_size=None):\n        with ScopedTiming("set preprocess config",self.debug_mode > 0):\n            # \u521d\u59cb\u5316ai2d\u9884\u5904\u7406\u914d\u7f6e\uff0c\u9ed8\u8ba4\u4e3asensor\u7ed9\u5230AI\u7684\u5c3a\u5bf8\uff0c\u53ef\u4ee5\u901a\u8fc7\u8bbe\u7f6einput_image_size\u81ea\u884c\u4fee\u6539\u8f93\u5165\u5c3a\u5bf8\n            ai2d_input_size=input_image_size if input_image_size else self.rgb888p_size\n            # \u8bbe\u7f6epadding\u9884\u5904\u7406\n            self.ai2d.pad(self.get_pad_param(), 0, [104,117,123])\n            # \u8bbe\u7f6eresize\u9884\u5904\u7406\n            self.ai2d.resize(nn.interp_method.tf_bilinear, nn.interp_mode.half_pixel)\n            # \u6784\u5efa\u9884\u5904\u7406\u6d41\u7a0b,\u53c2\u6570\u4e3a\u9884\u5904\u7406\u8f93\u5165tensor\u7684shape\u548c\u9884\u5904\u7406\u8f93\u51fa\u7684tensor\u7684shape\n            self.ai2d.build([1,3,ai2d_input_size[1],ai2d_input_size[0]],[1,3,self.model_input_size[1],self.model_input_size[0]])\n\n    # \u81ea\u5b9a\u4e49\u540e\u5904\u7406\uff0cresults\u662f\u6a21\u578b\u63a8\u7406\u8f93\u51fa\u7684array\u5217\u8868\uff0c\u8fd9\u91cc\u4f7f\u7528\u4e86aidemo\u5e93\u7684face_det_post_process\u63a5\u53e3\n    def postprocess(self,results):\n        with ScopedTiming("postprocess",self.debug_mode > 0):\n            res = aidemo.face_det_post_process(self.confidence_threshold,self.nms_threshold,self.model_input_size[0],self.anchors,self.rgb888p_size,results)\n            if len(res)==0:\n                return res\n            else:\n                return res[0]\n\n    # padding\u53c2\u6570\u8ba1\u7b97\n    def get_pad_param(self):\n        dst_w = self.model_input_size[0]\n        dst_h = self.model_input_size[1]\n        # \u8ba1\u7b97\u6700\u5c0f\u7684\u7f29\u653e\u6bd4\u4f8b\uff0c\u7b49\u6bd4\u4f8b\u7f29\u653e\n        ratio_w = dst_w / self.rgb888p_size[0]\n        ratio_h = dst_h / self.rgb888p_size[1]\n        if ratio_w < ratio_h:\n            ratio = ratio_w\n        else:\n            ratio = ratio_h\n        new_w = (int)(ratio * self.rgb888p_size[0])\n        new_h = (int)(ratio * self.rgb888p_size[1])\n        dw = (dst_w - new_w) / 2\n        dh = (dst_h - new_h) / 2\n        top = (int)(round(0))\n        bottom = (int)(round(dh * 2 + 0.1))\n        left = (int)(round(0))\n        right = (int)(round(dw * 2 - 0.1))\n        return [0,0,0,0,top, bottom, left, right]\n\n# \u81ea\u5b9a\u4e49\u4eba\u8138\u7f51\u683c\u4efb\u52a1\u7c7b\nclass FaceMeshApp(AIBase):\n    def __init__(self,kmodel_path,model_input_size,rgb888p_size=[1920,1080],display_size=[1920,1080],debug_mode=0):\n        super().__init__(kmodel_path,model_input_size,rgb888p_size,debug_mode)\n        # kmodel\u8def\u5f84\n        self.kmodel_path=kmodel_path\n        # \u4eba\u8138\u7f51\u683c\u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.model_input_size=model_input_size\n        # sensor\u7ed9\u5230AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.rgb888p_size=[ALIGN_UP(rgb888p_size[0],16),rgb888p_size[1]]\n        # \u89c6\u9891\u8f93\u51faVO\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.display_size=[ALIGN_UP(display_size[0],16),display_size[1]]\n        # debug\u6a21\u5f0f\n        self.debug_mode=debug_mode\n        # \u4eba\u8138mesh\u53c2\u6570\u5747\u503c\n        self.param_mean = np.array([0.0003492636315058917,2.52790130161884e-07,-6.875197868794203e-07,60.1679573059082,-6.295513230725192e-07,0.0005757200415246189,-5.085391239845194e-05,74.2781982421875,5.400917189035681e-07,6.574138387804851e-05,0.0003442012530285865,-66.67157745361328,-346603.6875,-67468.234375,46822.265625,-15262.046875,4350.5888671875,-54261.453125,-18328.033203125,-1584.328857421875,-84566.34375,3835.960693359375,-20811.361328125,38094.9296875,-19967.85546875,-9241.3701171875,-19600.71484375,13168.08984375,-5259.14404296875,1848.6478271484375,-13030.662109375,-2435.55615234375,-2254.20654296875,-14396.5615234375,-6176.3291015625,-25621.919921875,226.39447021484375,-6326.12353515625,-10867.2509765625,868.465087890625,-5831.14794921875,2705.123779296875,-3629.417724609375,2043.9901123046875,-2446.6162109375,3658.697021484375,-7645.98974609375,-6674.45263671875,116.38838958740234,7185.59716796875,-1429.48681640625,2617.366455078125,-1.2070955038070679,0.6690792441368103,-0.17760828137397766,0.056725528091192245,0.03967815637588501,-0.13586315512657166,-0.09223993122577667,-0.1726071834564209,-0.015804484486579895,-0.1416848599910736],dtype=np.float)\n        # \u4eba\u8138mesh\u53c2\u6570\u65b9\u5dee\n        self.param_std = np.array([0.00017632152594160289,6.737943476764485e-05,0.00044708489440381527,26.55023193359375,0.0001231376954820007,4.493021697271615e-05,7.923670636955649e-05,6.982563018798828,0.0004350444069132209,0.00012314890045672655,0.00017400001524947584,20.80303955078125,575421.125,277649.0625,258336.84375,255163.125,150994.375,160086.109375,111277.3046875,97311.78125,117198.453125,89317.3671875,88493.5546875,72229.9296875,71080.2109375,50013.953125,55968.58203125,47525.50390625,49515.06640625,38161.48046875,44872.05859375,46273.23828125,38116.76953125,28191.162109375,32191.4375,36006.171875,32559.892578125,25551.1171875,24267.509765625,27521.3984375,23166.53125,21101.576171875,19412.32421875,19452.203125,17454.984375,22537.623046875,16174.28125,14671.640625,15115.6884765625,13870.0732421875,13746.3125,12663.1337890625,1.5870834589004517,1.5077009201049805,0.5881357789039612,0.5889744758605957,0.21327851712703705,0.2630201280117035,0.2796429395675659,0.38030216097831726,0.16162841022014618,0.2559692859649658],dtype=np.float)\n        # \u5b9e\u4f8b\u5316Ai2d\uff0c\u7528\u4e8e\u5b9e\u73b0\u6a21\u578b\u9884\u5904\u7406\n        self.ai2d=Ai2d(debug_mode)\n        # \u8bbe\u7f6eAi2d\u7684\u8f93\u5165\u8f93\u51fa\u683c\u5f0f\u548c\u7c7b\u578b\n        self.ai2d.set_ai2d_dtype(nn.ai2d_format.NCHW_FMT,nn.ai2d_format.NCHW_FMT,np.uint8, np.uint8)\n\n    # \u914d\u7f6e\u9884\u5904\u7406\u64cd\u4f5c\uff0c\u8fd9\u91cc\u4f7f\u7528\u4e86crop\u548cresize\uff0cAi2d\u652f\u6301crop/shift/pad/resize/affine\uff0c\u5177\u4f53\u4ee3\u7801\u8bf7\u6253\u5f00/sdcard/app/libs/AI2D.py\u67e5\u770b\n    def config_preprocess(self,det,input_image_size=None):\n        with ScopedTiming("set preprocess config",self.debug_mode > 0):\n            # \u521d\u59cb\u5316ai2d\u9884\u5904\u7406\u914d\u7f6e\uff0c\u9ed8\u8ba4\u4e3asensor\u7ed9\u5230AI\u7684\u5c3a\u5bf8\uff0c\u53ef\u4ee5\u901a\u8fc7\u8bbe\u7f6einput_image_size\u81ea\u884c\u4fee\u6539\u8f93\u5165\u5c3a\u5bf8\n            ai2d_input_size=input_image_size if input_image_size else self.rgb888p_size\n            # \u8ba1\u7b97crop\u53c2\u6570\uff0c\u5e76\u8bbe\u7f6ecrop\u9884\u5904\u7406\n            roi = self.parse_roi_box_from_bbox(det)\n            self.ai2d.crop(int(roi[0]),int(roi[1]),int(roi[2]),int(roi[3]))\n            # \u8bbe\u7f6eresize\u9884\u5904\u7406\n            self.ai2d.resize(nn.interp_method.tf_bilinear, nn.interp_mode.half_pixel)\n            # \u6784\u5efa\u9884\u5904\u7406\u6d41\u7a0b,\u53c2\u6570\u4e3a\u9884\u5904\u7406\u8f93\u5165tensor\u7684shape\u548c\u9884\u5904\u7406\u8f93\u51fa\u7684tensor\u7684shape\n            self.ai2d.build([1,3,ai2d_input_size[1],ai2d_input_size[0]],[1,3,self.model_input_size[1],self.model_input_size[0]])\n            return roi\n\n    # \u81ea\u5b9a\u4e49\u540e\u5904\u7406\uff0cresults\u662f\u6a21\u578b\u8f93\u51fa\u7684array\u5217\u8868\n    def postprocess(self,results):\n        with ScopedTiming("postprocess",self.debug_mode > 0):\n            param = results[0] * self.param_std + self.param_mean\n            return param\n\n    def parse_roi_box_from_bbox(self,bbox):\n        # \u83b7\u53d6\u4eba\u8138roi\n        x1, y1, w, h = map(lambda x: int(round(x, 0)), bbox[:4])\n        old_size = (w + h) / 2\n        center_x = x1 + w / 2\n        center_y = y1 + h / 2 + old_size * 0.14\n        size = int(old_size * 1.58)\n        x0 = center_x - float(size) / 2\n        y0 = center_y - float(size) / 2\n        x1 = x0 + size\n        y1 = y0 + size\n        x0 = max(0, min(x0, self.rgb888p_size[0]))\n        y0 = max(0, min(y0, self.rgb888p_size[1]))\n        x1 = max(0, min(x1, self.rgb888p_size[0]))\n        y1 = max(0, min(y1, self.rgb888p_size[1]))\n        roi = (x0, y0, x1 - x0, y1 - y0)\n        return roi\n\n# \u81ea\u5b9a\u4e49\u4eba\u8138\u7f51\u683c\u540e\u5904\u7406\u4efb\u52a1\u7c7b\nclass FaceMeshPostApp(AIBase):\n    def __init__(self,kmodel_path,model_input_size,rgb888p_size=[1920,1080],display_size=[1920,1080],debug_mode=0):\n        super().__init__(kmodel_path,model_input_size,rgb888p_size,debug_mode)\n        # kmodel\u8def\u5f84\n        self.kmodel_path=kmodel_path\n        # \u4eba\u8138\u7f51\u683c\u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.model_input_size=model_input_size\n        # sensor\u7ed9\u5230AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.rgb888p_size=[ALIGN_UP(rgb888p_size[0],16),rgb888p_size[1]]\n        # \u89c6\u9891\u8f93\u51faVO\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.display_size=[ALIGN_UP(display_size[0],16),display_size[1]]\n        # debug\u6a21\u5f0f\n        self.debug_mode=debug_mode\n        # \u5b9e\u4f8b\u5316Ai2d\uff0c\u7528\u4e8e\u5b9e\u73b0\u6a21\u578b\u9884\u5904\u7406\n        self.ai2d=Ai2d(debug_mode)\n        # \u8bbe\u7f6eAi2d\u7684\u8f93\u5165\u8f93\u51fa\u683c\u5f0f\u548c\u7c7b\u578b\n        self.ai2d.set_ai2d_dtype(nn.ai2d_format.NCHW_FMT,nn.ai2d_format.NCHW_FMT,np.uint8, np.uint8)\n\n    # \u91cd\u5199\u9884\u5904\u7406\u51fd\u6570preprocess\uff0c\u56e0\u4e3a\u8be5\u6a21\u578b\u7684\u9884\u5904\u7406\u4e0d\u662f\u5355\u7eaf\u8c03\u7528\u4e00\u4e2aai2d\u80fd\u5b9e\u73b0\u7684\uff0c\u8fd4\u56de\u6a21\u578b\u8f93\u5165\u7684tensor\u5217\u8868\n    def preprocess(self,param):\n        with ScopedTiming("set preprocess config",self.debug_mode > 0):\n            # face mesh post\u6a21\u578b\u9884\u5904\u7406\uff0cparam\u89e3\u6790\n            param = param[0]\n            trans_dim, shape_dim, exp_dim = 12, 40, 10\n            R_ = param[:trans_dim].copy().reshape((3, -1))\n            R = R_[:, :3].copy()\n            offset = R_[:, 3].copy()\n            offset = offset.reshape((3, 1))\n            alpha_shp = param[trans_dim:trans_dim + shape_dim].copy().reshape((-1, 1))\n            alpha_exp = param[trans_dim + shape_dim:].copy().reshape((-1, 1))\n            R_tensor = nn.from_numpy(R)\n            offset_tensor = nn.from_numpy(offset)\n            alpha_shp_tensor = nn.from_numpy(alpha_shp)\n            alpha_exp_tensor = nn.from_numpy(alpha_exp)\n            return [R_tensor,offset_tensor,alpha_shp_tensor,alpha_exp_tensor]\n\n    # \u81ea\u5b9a\u4e49\u6a21\u578b\u540e\u5904\u7406\uff0c\u8fd9\u91cc\u8c03\u7528\u4e86aidemo\u7684face_mesh_post_process\u63a5\u53e3\n    def postprocess(self,results,roi):\n        with ScopedTiming("postprocess",self.debug_mode > 0):\n            x, y, w, h = map(lambda x: int(round(x, 0)), roi[:4])\n            x = x * self.display_size[0] // self.rgb888p_size[0]\n            y = y * self.display_size[1] // self.rgb888p_size[1]\n            w = w * self.display_size[0] // self.rgb888p_size[0]\n            h = h * self.display_size[1] // self.rgb888p_size[1]\n            roi_array = np.array([x,y,w,h],dtype=np.float)\n            aidemo.face_mesh_post_process(roi_array,results[0])\n            return results[0]\n\n# 3D\u4eba\u8138\u7f51\u683c\nclass FaceMesh:\n    def __init__(self,face_det_kmodel,face_mesh_kmodel,mesh_post_kmodel,det_input_size,mesh_input_size,anchors,confidence_threshold=0.25,nms_threshold=0.3,rgb888p_size=[1920,1080],display_size=[1920,1080],debug_mode=0):\n        # \u4eba\u8138\u68c0\u6d4b\u6a21\u578b\u8def\u5f84\n        self.face_det_kmodel=face_det_kmodel\n        # \u4eba\u81383D\u7f51\u683c\u6a21\u578b\u8def\u5f84\n        self.face_mesh_kmodel=face_mesh_kmodel\n        # \u4eba\u81383D\u7f51\u683c\u540e\u5904\u7406\u6a21\u578b\u8def\u5f84\n        self.mesh_post_kmodel=mesh_post_kmodel\n        # \u4eba\u8138\u68c0\u6d4b\u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.det_input_size=det_input_size\n        # \u4eba\u81383D\u7f51\u683c\u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.mesh_input_size=mesh_input_size\n        # anchors\n        self.anchors=anchors\n        # \u7f6e\u4fe1\u5ea6\u9608\u503c\n        self.confidence_threshold=confidence_threshold\n        # nms\u9608\u503c\n        self.nms_threshold=nms_threshold\n        # sensor\u7ed9\u5230AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.rgb888p_size=[ALIGN_UP(rgb888p_size[0],16),rgb888p_size[1]]\n        # \u89c6\u9891\u8f93\u51faVO\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.display_size=[ALIGN_UP(display_size[0],16),display_size[1]]\n        # debug_mode\u6a21\u5f0f\n        self.debug_mode=debug_mode\n        # \u4eba\u8138\u68c0\u6d4b\u5b9e\u4f8b\n        self.face_det=FaceDetApp(self.face_det_kmodel,model_input_size=self.det_input_size,anchors=self.anchors,confidence_threshold=self.confidence_threshold,nms_threshold=self.nms_threshold,rgb888p_size=self.rgb888p_size,display_size=self.display_size,debug_mode=0)\n        # \u4eba\u8138\u7f51\u683c\u5b9e\u4f8b\n        self.face_mesh=FaceMeshApp(self.face_mesh_kmodel,model_input_size=self.mesh_input_size,rgb888p_size=self.rgb888p_size,display_size=self.display_size)\n        # \u4eba\u8138\u7f51\u683c\u540e\u5904\u7406\u5b9e\u4f8b\n        self.face_mesh_post=FaceMeshPostApp(self.mesh_post_kmodel,model_input_size=self.mesh_input_size,rgb888p_size=self.rgb888p_size,display_size=self.display_size)\n        # \u4eba\u8138\u68c0\u6d4b\u9884\u5904\u7406\u914d\u7f6e\n        self.face_det.config_preprocess()\n\n    # run\u51fd\u6570\n    def run(self,input_np):\n        # \u6267\u884c\u4eba\u8138\u68c0\u6d4b\n        det_boxes=self.face_det.run(input_np)\n        mesh_res=[]\n        for det_box in det_boxes:\n            # \u5bf9\u68c0\u6d4b\u5230\u7684\u6bcf\u4e00\u4e2a\u4eba\u8138\u914d\u7f6e\u9884\u5904\u7406\uff0c\u6267\u884c\u4eba\u8138\u7f51\u683c\u548c\u4eba\u8138\u7f51\u683c\u540e\u5904\u7406\n            roi=self.face_mesh.config_preprocess(det_box)\n            param=self.face_mesh.run(input_np)\n            tensors=self.face_mesh_post.preprocess(param)\n            results=self.face_mesh_post.inference(tensors)\n            res=self.face_mesh_post.postprocess(results,roi)\n            mesh_res.append(res)\n        return det_boxes,mesh_res\n\n\n    # \u7ed8\u5236\u4eba\u8138\u89e3\u6790\u6548\u679c\n    def draw_result(self,pl,dets,mesh_res):\n        pl.osd_img.clear()\n        if dets:\n            draw_img_np = np.zeros((self.display_size[1],self.display_size[0],4),dtype=np.uint8)\n            draw_img = image.Image(self.display_size[0], self.display_size[1], image.ARGB8888, alloc=image.ALLOC_REF,data = draw_img_np)\n            for vertices in mesh_res:\n                aidemo.face_draw_mesh(draw_img_np, vertices)\n            pl.osd_img.copy_from(draw_img)\n\n\nif __name__=="__main__":\n    # \u663e\u793a\u6a21\u5f0f\uff0c\u9ed8\u8ba4"hdmi",\u53ef\u4ee5\u9009\u62e9"hdmi"\u548c"lcd"\n    display_mode="hdmi"\n    if display_mode=="hdmi":\n        display_size=[1920,1080]\n    else:\n        display_size=[800,480]\n    # \u4eba\u8138\u68c0\u6d4b\u6a21\u578b\u8def\u5f84\n    face_det_kmodel_path="/sdcard/app/tests/kmodel/face_detection_320.kmodel"\n    # \u4eba\u8138\u7f51\u683c\u6a21\u578b\u8def\u5f84\n    face_mesh_kmodel_path="/sdcard/app/tests/kmodel/face_alignment.kmodel"\n    # \u4eba\u8138\u7f51\u683c\u540e\u5904\u7406\u6a21\u578b\u8def\u5f84\n    face_mesh_post_kmodel_path="/sdcard/app/tests/kmodel/face_alignment_post.kmodel"\n    # \u5176\u4ed6\u53c2\u6570\n    anchors_path="/sdcard/app/tests/utils/prior_data_320.bin"\n    rgb888p_size=[1920,1080]\n    face_det_input_size=[320,320]\n    face_mesh_input_size=[120,120]\n    confidence_threshold=0.5\n    nms_threshold=0.2\n    anchor_len=4200\n    det_dim=4\n    anchors = np.fromfile(anchors_path, dtype=np.float)\n    anchors = anchors.reshape((anchor_len,det_dim))\n\n    # \u521d\u59cb\u5316PipeLine\uff0c\u53ea\u5173\u6ce8\u4f20\u7ed9AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u663e\u793a\u7684\u5206\u8fa8\u7387\n    pl=PipeLine(rgb888p_size=rgb888p_size,display_size=display_size,display_mode=display_mode)\n    pl.create()\n    fm=FaceMesh(face_det_kmodel_path,face_mesh_kmodel_path,face_mesh_post_kmodel_path,det_input_size=face_det_input_size,mesh_input_size=face_mesh_input_size,anchors=anchors,confidence_threshold=confidence_threshold,nms_threshold=nms_threshold,rgb888p_size=rgb888p_size,display_size=display_size)\n    try:\n        while True:\n            os.exitpoint()\n            with ScopedTiming("total",1):\n                img=pl.get_frame()                      # \u83b7\u53d6\u5f53\u524d\u5e27\n                det_boxes,mesh_res=fm.run(img)          # \u63a8\u7406\u5f53\u524d\u5e27\n                fm.draw_result(pl,det_boxes,mesh_res)   # \u7ed8\u5236\u63a8\u7406\u7ed3\u679c\n                pl.show_image()                         # \u663e\u793a\u63a8\u7406\u6548\u679c\n                gc.collect()\n    except Exception as e:\n        sys.print_exception(e)\n    finally:\n        fm.face_det.deinit()\n        fm.face_mesh.deinit()\n        fm.face_mesh_post.deinit()\n        pl.destroy()\n'})}),"\n",(0,i.jsx)(n.h3,{id:"26-\u4eba\u8138\u89e3\u6790",children:"2.6. \u4eba\u8138\u89e3\u6790"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:'from libs.PipeLine import PipeLine, ScopedTiming\nfrom libs.AIBase import AIBase\nfrom libs.AI2D import Ai2d\nimport os\nimport ujson\nfrom media.media import *\nfrom time import *\nimport nncase_runtime as nn\nimport ulab.numpy as np\nimport time\nimport image\nimport aidemo\nimport random\nimport gc\nimport sys\n\n# \u81ea\u5b9a\u4e49\u4eba\u8138\u68c0\u6d4b\u4efb\u52a1\u7c7b\nclass FaceDetApp(AIBase):\n    def __init__(self,kmodel_path,model_input_size,anchors,confidence_threshold=0.25,nms_threshold=0.3,rgb888p_size=[1280,720],display_size=[1920,1080],debug_mode=0):\n        super().__init__(kmodel_path,model_input_size,rgb888p_size,debug_mode)\n        # kmodel\u8def\u5f84\n        self.kmodel_path=kmodel_path\n        # \u68c0\u6d4b\u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.model_input_size=model_input_size\n        # \u7f6e\u4fe1\u5ea6\u9608\u503c\n        self.confidence_threshold=confidence_threshold\n        # nms\u9608\u503c\n        self.nms_threshold=nms_threshold\n        self.anchors=anchors\n        # sensor\u7ed9\u5230AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.rgb888p_size=[ALIGN_UP(rgb888p_size[0],16),rgb888p_size[1]]\n        # \u89c6\u9891\u8f93\u51faVO\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.display_size=[ALIGN_UP(display_size[0],16),display_size[1]]\n        # debug\u6a21\u5f0f\n        self.debug_mode=debug_mode\n        # \u5b9e\u4f8b\u5316Ai2d\uff0c\u7528\u4e8e\u5b9e\u73b0\u6a21\u578b\u9884\u5904\u7406\n        self.ai2d=Ai2d(debug_mode)\n        # \u8bbe\u7f6eAi2d\u7684\u8f93\u5165\u8f93\u51fa\u683c\u5f0f\u548c\u7c7b\u578b\n        self.ai2d.set_ai2d_dtype(nn.ai2d_format.NCHW_FMT,nn.ai2d_format.NCHW_FMT,np.uint8, np.uint8)\n\n    # \u914d\u7f6e\u9884\u5904\u7406\u64cd\u4f5c\uff0c\u8fd9\u91cc\u4f7f\u7528\u4e86pad\u548cresize\uff0cAi2d\u652f\u6301crop/shift/pad/resize/affine\uff0c\u5177\u4f53\u4ee3\u7801\u8bf7\u6253\u5f00/sdcard/app/libs/AI2D.py\u67e5\u770b\n    def config_preprocess(self,input_image_size=None):\n        with ScopedTiming("set preprocess config",self.debug_mode > 0):\n            # \u521d\u59cb\u5316ai2d\u9884\u5904\u7406\u914d\u7f6e\uff0c\u9ed8\u8ba4\u4e3asensor\u7ed9\u5230AI\u7684\u5c3a\u5bf8\uff0c\u53ef\u4ee5\u901a\u8fc7\u8bbe\u7f6einput_image_size\u81ea\u884c\u4fee\u6539\u8f93\u5165\u5c3a\u5bf8\n            ai2d_input_size=input_image_size if input_image_size else self.rgb888p_size\n            # \u8ba1\u7b97padding\u53c2\u6570\uff0c\u5e76\u8bbe\u7f6epadding\u9884\u5904\u7406\n            self.ai2d.pad(self.get_pad_param(), 0, [104,117,123])\n            # \u8bbe\u7f6eresize\u9884\u5904\u7406\n            self.ai2d.resize(nn.interp_method.tf_bilinear, nn.interp_mode.half_pixel)\n            # \u6784\u5efa\u9884\u5904\u7406\u6d41\u7a0b,\u53c2\u6570\u4e3a\u9884\u5904\u7406\u8f93\u5165tensor\u7684shape\u548c\u9884\u5904\u7406\u8f93\u51fa\u7684tensor\u7684shape\n            self.ai2d.build([1,3,ai2d_input_size[1],ai2d_input_size[0]],[1,3,self.model_input_size[1],self.model_input_size[0]])\n\n    # \u81ea\u5b9a\u4e49\u540e\u5904\u7406\uff0cresults\u662f\u6a21\u578b\u8f93\u51fa\u7684array\u5217\u8868\uff0c\u8fd9\u91cc\u8c03\u7528\u4e86aidemo\u5e93\u7684face_det_post_process\u63a5\u53e3\n    def postprocess(self,results):\n        with ScopedTiming("postprocess",self.debug_mode > 0):\n            res = aidemo.face_det_post_process(self.confidence_threshold,self.nms_threshold,self.model_input_size[0],self.anchors,self.rgb888p_size,results)\n            if len(res)==0:\n                return res\n            else:\n                return res[0]\n\n    # \u8ba1\u7b97padding\u53c2\u6570\n    def get_pad_param(self):\n        dst_w = self.model_input_size[0]\n        dst_h = self.model_input_size[1]\n        # \u8ba1\u7b97\u6700\u5c0f\u7684\u7f29\u653e\u6bd4\u4f8b\uff0c\u7b49\u6bd4\u4f8b\u7f29\u653e\n        ratio_w = dst_w / self.rgb888p_size[0]\n        ratio_h = dst_h / self.rgb888p_size[1]\n        if ratio_w < ratio_h:\n            ratio = ratio_w\n        else:\n            ratio = ratio_h\n        new_w = (int)(ratio * self.rgb888p_size[0])\n        new_h = (int)(ratio * self.rgb888p_size[1])\n        dw = (dst_w - new_w) / 2\n        dh = (dst_h - new_h) / 2\n        top = (int)(round(0))\n        bottom = (int)(round(dh * 2 + 0.1))\n        left = (int)(round(0))\n        right = (int)(round(dw * 2 - 0.1))\n        return [0,0,0,0,top, bottom, left, right]\n\n# \u81ea\u5b9a\u4e49\u4eba\u8138\u89e3\u6790\u4efb\u52a1\u7c7b\nclass FaceParseApp(AIBase):\n    def __init__(self,kmodel_path,model_input_size,rgb888p_size=[1920,1080],display_size=[1920,1080],debug_mode=0):\n        super().__init__(kmodel_path,model_input_size,rgb888p_size,debug_mode)\n        # kmodel\u8def\u5f84\n        self.kmodel_path=kmodel_path\n        # \u68c0\u6d4b\u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.model_input_size=model_input_size\n        # sensor\u7ed9\u5230AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.rgb888p_size=[ALIGN_UP(rgb888p_size[0],16),rgb888p_size[1]]\n        # \u89c6\u9891\u8f93\u51faVO\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.display_size=[ALIGN_UP(display_size[0],16),display_size[1]]\n        # debug\u6a21\u5f0f\n        self.debug_mode=debug_mode\n        # \u5b9e\u4f8b\u5316Ai2d\uff0c\u7528\u4e8e\u5b9e\u73b0\u6a21\u578b\u9884\u5904\u7406\n        self.ai2d=Ai2d(debug_mode)\n        # \u8bbe\u7f6eAi2d\u7684\u8f93\u5165\u8f93\u51fa\u683c\u5f0f\u548c\u7c7b\u578b\n        self.ai2d.set_ai2d_dtype(nn.ai2d_format.NCHW_FMT,nn.ai2d_format.NCHW_FMT,np.uint8, np.uint8)\n\n    # \u914d\u7f6e\u9884\u5904\u7406\u64cd\u4f5c\uff0c\u8fd9\u91cc\u4f7f\u7528\u4e86affine\uff0cAi2d\u652f\u6301crop/shift/pad/resize/affine\uff0c\u5177\u4f53\u4ee3\u7801\u8bf7\u6253\u5f00/sdcard/app/libs/AI2D.py\u67e5\u770b\n    def config_preprocess(self,det,input_image_size=None):\n        with ScopedTiming("set preprocess config",self.debug_mode > 0):\n            # \u521d\u59cb\u5316ai2d\u9884\u5904\u7406\u914d\u7f6e\uff0c\u9ed8\u8ba4\u4e3asensor\u7ed9\u5230AI\u7684\u5c3a\u5bf8\uff0c\u53ef\u4ee5\u901a\u8fc7\u8bbe\u7f6einput_image_size\u81ea\u884c\u4fee\u6539\u8f93\u5165\u5c3a\u5bf8\n            ai2d_input_size=input_image_size if input_image_size else self.rgb888p_size\n            # \u8ba1\u7b97\u4eff\u5c04\u53d8\u6362\u77e9\u9635\u5e76\u8bbe\u7f6eaffine\u9884\u5904\u7406\n            matrix_dst = self.get_affine_matrix(det)\n            self.ai2d.affine(nn.interp_method.cv2_bilinear,0, 0, 127, 1,matrix_dst)\n            # \u6784\u5efa\u9884\u5904\u7406\u6d41\u7a0b,\u53c2\u6570\u4e3a\u9884\u5904\u7406\u8f93\u5165tensor\u7684shape\u548c\u9884\u5904\u7406\u8f93\u51fa\u7684tensor\u7684shape\n            self.ai2d.build([1,3,ai2d_input_size[1],ai2d_input_size[0]],[1,3,self.model_input_size[1],self.model_input_size[0]])\n\n    # \u81ea\u5b9a\u4e49\u540e\u5904\u7406\uff0cresults\u662f\u6a21\u578b\u8f93\u51fa\u7684array\u5217\u8868\uff0c\u8fd9\u91cc\u5c06\u7b2c\u4e00\u4e2a\u8f93\u51fa\u8fd4\u56de\n    def postprocess(self,results):\n        with ScopedTiming("postprocess",self.debug_mode > 0):\n            return results[0]\n\n    def get_affine_matrix(self,bbox):\n        # \u83b7\u53d6\u4eff\u5c04\u77e9\u9635\uff0c\u7528\u4e8e\u5c06\u8fb9\u754c\u6846\u6620\u5c04\u5230\u6a21\u578b\u8f93\u5165\u7a7a\u95f4\n        with ScopedTiming("get_affine_matrix", self.debug_mode > 1):\n            # \u8bbe\u7f6e\u7f29\u653e\u56e0\u5b50\n            factor = 2.7\n            # \u4ece\u8fb9\u754c\u6846\u63d0\u53d6\u5750\u6807\u548c\u5c3a\u5bf8\n            x1, y1, w, h = map(lambda x: int(round(x, 0)), bbox[:4])\n            # \u6a21\u578b\u8f93\u5165\u5927\u5c0f\n            edge_size = self.model_input_size[1]\n            # \u5e73\u79fb\u8ddd\u79bb\uff0c\u4f7f\u5f97\u6a21\u578b\u8f93\u5165\u7a7a\u95f4\u7684\u4e2d\u5fc3\u5bf9\u51c6\u539f\u70b9\n            trans_distance = edge_size / 2.0\n            # \u8ba1\u7b97\u8fb9\u754c\u6846\u4e2d\u5fc3\u70b9\u7684\u5750\u6807\n            center_x = x1 + w / 2.0\n            center_y = y1 + h / 2.0\n            # \u8ba1\u7b97\u6700\u5927\u8fb9\u957f\n            maximum_edge = factor * (h if h > w else w)\n            # \u8ba1\u7b97\u7f29\u653e\u6bd4\u4f8b\n            scale = edge_size * 2.0 / maximum_edge\n            # \u8ba1\u7b97\u5e73\u79fb\u53c2\u6570\n            cx = trans_distance - scale * center_x\n            cy = trans_distance - scale * center_y\n            # \u521b\u5efa\u4eff\u5c04\u77e9\u9635\n            affine_matrix = [scale, 0, cx, 0, scale, cy]\n            return affine_matrix\n\n# \u4eba\u8138\u89e3\u6790\u4efb\u52a1\nclass FaceParse:\n    def __init__(self,face_det_kmodel,face_parse_kmodel,det_input_size,parse_input_size,anchors,confidence_threshold=0.25,nms_threshold=0.3,rgb888p_size=[1920,1080],display_size=[1920,1080],debug_mode=0):\n        # \u4eba\u8138\u68c0\u6d4b\u6a21\u578b\u8def\u5f84\n        self.face_det_kmodel=face_det_kmodel\n        # \u4eba\u8138\u89e3\u6790\u6a21\u578b\u8def\u5f84\n        self.face_pose_kmodel=face_parse_kmodel\n        # \u4eba\u8138\u68c0\u6d4b\u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.det_input_size=det_input_size\n        # \u4eba\u8138\u89e3\u6790\u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.parse_input_size=parse_input_size\n        # anchors\n        self.anchors=anchors\n        # \u7f6e\u4fe1\u5ea6\u9608\u503c\n        self.confidence_threshold=confidence_threshold\n        # nms\u9608\u503c\n        self.nms_threshold=nms_threshold\n        # sensor\u7ed9\u5230AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.rgb888p_size=[ALIGN_UP(rgb888p_size[0],16),rgb888p_size[1]]\n        # \u89c6\u9891\u8f93\u51faVO\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.display_size=[ALIGN_UP(display_size[0],16),display_size[1]]\n        # debug_mode\u6a21\u5f0f\n        self.debug_mode=debug_mode\n        # \u4eba\u8138\u68c0\u6d4b\u4efb\u52a1\u7c7b\u5b9e\u4f8b\n        self.face_det=FaceDetApp(self.face_det_kmodel,model_input_size=self.det_input_size,anchors=self.anchors,confidence_threshold=self.confidence_threshold,nms_threshold=self.nms_threshold,rgb888p_size=self.rgb888p_size,display_size=self.display_size,debug_mode=0)\n        # \u4eba\u8138\u89e3\u6790\u5b9e\u4f8b\n        self.face_parse=FaceParseApp(self.face_pose_kmodel,model_input_size=self.parse_input_size,rgb888p_size=self.rgb888p_size,display_size=self.display_size)\n        # \u4eba\u8138\u68c0\u6d4b\u9884\u5904\u7406\u914d\u7f6e\n        self.face_det.config_preprocess()\n\n    # run\u51fd\u6570\n    def run(self,input_np):\n        # \u6267\u884c\u4eba\u8138\u68c0\u6d4b\n        det_boxes=self.face_det.run(input_np)\n        parse_res=[]\n        for det_box in det_boxes:\n            # \u5bf9\u68c0\u6d4b\u5230\u6bcf\u4e00\u4e2a\u4eba\u8138\u8fdb\u884c\u4eba\u8138\u89e3\u6790\n            self.face_parse.config_preprocess(det_box)\n            res=self.face_parse.run(input_np)\n            parse_res.append(res)\n        return det_boxes,parse_res\n\n\n    # \u7ed8\u5236\u4eba\u8138\u89e3\u6790\u6548\u679c\n    def draw_result(self,pl,dets,parse_res):\n        pl.osd_img.clear()\n        if dets:\n            draw_img_np = np.zeros((self.display_size[1],self.display_size[0],4),dtype=np.uint8)\n            draw_img=image.Image(self.display_size[0], self.display_size[1], image.ARGB8888,alloc=image.ALLOC_REF,data=draw_img_np)\n            for i,det in enumerate(dets):\n                # \uff081\uff09\u5c06\u4eba\u8138\u68c0\u6d4b\u6846\u753b\u5230draw_img\n                x, y, w, h = map(lambda x: int(round(x, 0)), det[:4])\n                x = x * self.display_size[0] // self.rgb888p_size[0]\n                y = y * self.display_size[1] // self.rgb888p_size[1]\n                w = w * self.display_size[0] // self.rgb888p_size[0]\n                h = h * self.display_size[1] // self.rgb888p_size[1]\n                aidemo.face_parse_post_process(draw_img_np,self.rgb888p_size,self.display_size,self.parse_input_size[0],det.tolist(),parse_res[i])\n            pl.osd_img.copy_from(draw_img)\n\n\nif __name__=="__main__":\n    # \u663e\u793a\u6a21\u5f0f\uff0c\u9ed8\u8ba4"hdmi",\u53ef\u4ee5\u9009\u62e9"hdmi"\u548c"lcd"\n    display_mode="hdmi"\n    if display_mode=="hdmi":\n        display_size=[1920,1080]\n    else:\n        display_size=[800,480]\n    # \u4eba\u8138\u68c0\u6d4b\u6a21\u578b\u8def\u5f84\n    face_det_kmodel_path="/sdcard/app/tests/kmodel/face_detection_320.kmodel"\n    # \u4eba\u8138\u89e3\u6790\u6a21\u578b\u8def\u5f84\n    face_parse_kmodel_path="/sdcard/app/tests/kmodel/face_parse.kmodel"\n    # \u5176\u4ed6\u53c2\u6570\n    anchors_path="/sdcard/app/tests/utils/prior_data_320.bin"\n    rgb888p_size=[1920,1080]\n    face_det_input_size=[320,320]\n    face_parse_input_size=[320,320]\n    confidence_threshold=0.5\n    nms_threshold=0.2\n    anchor_len=4200\n    det_dim=4\n    anchors = np.fromfile(anchors_path, dtype=np.float)\n    anchors = anchors.reshape((anchor_len,det_dim))\n\n    # \u521d\u59cb\u5316PipeLine\uff0c\u53ea\u5173\u6ce8\u4f20\u7ed9AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u663e\u793a\u7684\u5206\u8fa8\u7387\n    pl=PipeLine(rgb888p_size=rgb888p_size,display_size=display_size,display_mode=display_mode)\n    pl.create()\n    fp=FaceParse(face_det_kmodel_path,face_parse_kmodel_path,det_input_size=face_det_input_size,parse_input_size=face_parse_input_size,anchors=anchors,confidence_threshold=confidence_threshold,nms_threshold=nms_threshold,rgb888p_size=rgb888p_size,display_size=display_size)\n    try:\n        while True:\n            os.exitpoint()\n            with ScopedTiming("total",1):\n                img=pl.get_frame()                      # \u83b7\u53d6\u5f53\u524d\u5e27\n                det_boxes,parse_res=fp.run(img)         # \u63a8\u7406\u5f53\u524d\u5e27\n                fp.draw_result(pl,det_boxes,parse_res)  # \u7ed8\u5236\u5f53\u524d\u5e27\u63a8\u7406\u7ed3\u679c\n                pl.show_image()                         # \u5c55\u793a\u63a8\u7406\u6548\u679c\n                gc.collect()\n    except Exception as e:\n        sys.print_exception(e)\n    finally:\n        fp.face_det.deinit()\n        fp.face_parse.deinit()\n        pl.destroy()\n'})}),"\n",(0,i.jsx)(n.h3,{id:"27-\u4eba\u8138\u59ff\u6001",children:"2.7. \u4eba\u8138\u59ff\u6001"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:'from libs.PipeLine import PipeLine, ScopedTiming\nfrom libs.AIBase import AIBase\nfrom libs.AI2D import Ai2d\nimport os\nimport ujson\nfrom media.media import *\nfrom time import *\nimport nncase_runtime as nn\nimport ulab.numpy as np\nimport time\nimport image\nimport aidemo\nimport random\nimport gc\nimport sys\n\n# \u81ea\u5b9a\u4e49\u4eba\u8138\u68c0\u6d4b\u4efb\u52a1\u7c7b\nclass FaceDetApp(AIBase):\n    def __init__(self,kmodel_path,model_input_size,anchors,confidence_threshold=0.25,nms_threshold=0.3,rgb888p_size=[1280,720],display_size=[1920,1080],debug_mode=0):\n        super().__init__(kmodel_path,model_input_size,rgb888p_size,debug_mode)\n        # kmodel\u8def\u5f84\n        self.kmodel_path=kmodel_path\n        # \u68c0\u6d4b\u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.model_input_size=model_input_size\n        # \u7f6e\u4fe1\u5ea6\u9608\u503c\n        self.confidence_threshold=confidence_threshold\n        # nms\u9608\u503c\n        self.nms_threshold=nms_threshold\n        self.anchors=anchors\n        # sensor\u7ed9\u5230AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.rgb888p_size=[ALIGN_UP(rgb888p_size[0],16),rgb888p_size[1]]\n        # \u89c6\u9891\u8f93\u51faVO\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.display_size=[ALIGN_UP(display_size[0],16),display_size[1]]\n        # debug\u6a21\u5f0f\n        self.debug_mode=debug_mode\n        # \u5b9e\u4f8b\u5316Ai2d\uff0c\u7528\u4e8e\u5b9e\u73b0\u6a21\u578b\u9884\u5904\u7406\n        self.ai2d=Ai2d(debug_mode)\n        # \u8bbe\u7f6eAi2d\u7684\u8f93\u5165\u8f93\u51fa\u683c\u5f0f\u548c\u7c7b\u578b\n        self.ai2d.set_ai2d_dtype(nn.ai2d_format.NCHW_FMT,nn.ai2d_format.NCHW_FMT,np.uint8, np.uint8)\n\n    # \u914d\u7f6e\u9884\u5904\u7406\u64cd\u4f5c\uff0c\u8fd9\u91cc\u4f7f\u7528\u4e86pad\u548cresize\uff0cAi2d\u652f\u6301crop/shift/pad/resize/affine\uff0c\u5177\u4f53\u4ee3\u7801\u8bf7\u6253\u5f00/sdcard/app/libs/AI2D.py\u67e5\u770b\n    def config_preprocess(self,input_image_size=None):\n        with ScopedTiming("set preprocess config",self.debug_mode > 0):\n            # \u521d\u59cb\u5316ai2d\u9884\u5904\u7406\u914d\u7f6e\uff0c\u9ed8\u8ba4\u4e3asensor\u7ed9\u5230AI\u7684\u5c3a\u5bf8\uff0c\u53ef\u4ee5\u901a\u8fc7\u8bbe\u7f6einput_image_size\u81ea\u884c\u4fee\u6539\u8f93\u5165\u5c3a\u5bf8\n            ai2d_input_size=input_image_size if input_image_size else self.rgb888p_size\n            # \u8ba1\u7b97padding\u53c2\u6570\uff0c\u5e76\u8bbe\u7f6epadding\u9884\u5904\u7406\n            self.ai2d.pad(self.get_pad_param(), 0, [104,117,123])\n            # \u8bbe\u7f6eresize\u9884\u5904\u7406\n            self.ai2d.resize(nn.interp_method.tf_bilinear, nn.interp_mode.half_pixel)\n            # \u6784\u5efa\u9884\u5904\u7406\u6d41\u7a0b,\u53c2\u6570\u4e3a\u9884\u5904\u7406\u8f93\u5165tensor\u7684shape\u548c\u9884\u5904\u7406\u8f93\u51fa\u7684tensor\u7684shape\n            self.ai2d.build([1,3,ai2d_input_size[1],ai2d_input_size[0]],[1,3,self.model_input_size[1],self.model_input_size[0]])\n\n    # \u81ea\u5b9a\u4e49\u540e\u5904\u7406\uff0cresults\u662f\u6a21\u578b\u8f93\u51fa\u7684array\u5217\u8868\uff0c\u8fd9\u91cc\u4f7f\u7528\u4e86aidemo\u5e93\u7684face_det_post_process\u63a5\u53e3\n    def postprocess(self,results):\n        with ScopedTiming("postprocess",self.debug_mode > 0):\n            res = aidemo.face_det_post_process(self.confidence_threshold,self.nms_threshold,self.model_input_size[0],self.anchors,self.rgb888p_size,results)\n            if len(res)==0:\n                return res\n            else:\n                return res[0]\n\n    # \u8ba1\u7b97padding\u53c2\u6570\n    def get_pad_param(self):\n        dst_w = self.model_input_size[0]\n        dst_h = self.model_input_size[1]\n        # \u8ba1\u7b97\u6700\u5c0f\u7684\u7f29\u653e\u6bd4\u4f8b\uff0c\u7b49\u6bd4\u4f8b\u7f29\u653e\n        ratio_w = dst_w / self.rgb888p_size[0]\n        ratio_h = dst_h / self.rgb888p_size[1]\n        if ratio_w < ratio_h:\n            ratio = ratio_w\n        else:\n            ratio = ratio_h\n        new_w = (int)(ratio * self.rgb888p_size[0])\n        new_h = (int)(ratio * self.rgb888p_size[1])\n        dw = (dst_w - new_w) / 2\n        dh = (dst_h - new_h) / 2\n        top = (int)(round(0))\n        bottom = (int)(round(dh * 2 + 0.1))\n        left = (int)(round(0))\n        right = (int)(round(dw * 2 - 0.1))\n        return [0,0,0,0,top, bottom, left, right]\n\n# \u81ea\u5b9a\u4e49\u4eba\u8138\u59ff\u6001\u4efb\u52a1\u7c7b\nclass FacePoseApp(AIBase):\n    def __init__(self,kmodel_path,model_input_size,rgb888p_size=[1920,1080],display_size=[1920,1080],debug_mode=0):\n        super().__init__(kmodel_path,model_input_size,rgb888p_size,debug_mode)\n        # kmodel\u8def\u5f84\n        self.kmodel_path=kmodel_path\n        # \u4eba\u8138\u59ff\u6001\u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.model_input_size=model_input_size\n        # sensor\u7ed9\u5230AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.rgb888p_size=[ALIGN_UP(rgb888p_size[0],16),rgb888p_size[1]]\n        # \u89c6\u9891\u8f93\u51faVO\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.display_size=[ALIGN_UP(display_size[0],16),display_size[1]]\n        # debug\u6a21\u5f0f\n        self.debug_mode=debug_mode\n        # \u5b9e\u4f8b\u5316Ai2d\uff0c\u7528\u4e8e\u5b9e\u73b0\u6a21\u578b\u9884\u5904\u7406\n        self.ai2d=Ai2d(debug_mode)\n        # \u8bbe\u7f6eAi2d\u7684\u8f93\u5165\u8f93\u51fa\u683c\u5f0f\u548c\u7c7b\u578b\n        self.ai2d.set_ai2d_dtype(nn.ai2d_format.NCHW_FMT,nn.ai2d_format.NCHW_FMT,np.uint8, np.uint8)\n\n    # \u914d\u7f6e\u9884\u5904\u7406\u64cd\u4f5c\uff0c\u8fd9\u91cc\u4f7f\u7528\u4e86affine\uff0cAi2d\u652f\u6301crop/shift/pad/resize/affine\uff0c\u5177\u4f53\u4ee3\u7801\u8bf7\u6253\u5f00/sdcard/app/libs/AI2D.py\u67e5\u770b\n    def config_preprocess(self,det,input_image_size=None):\n        with ScopedTiming("set preprocess config",self.debug_mode > 0):\n            # \u521d\u59cb\u5316ai2d\u9884\u5904\u7406\u914d\u7f6e\uff0c\u9ed8\u8ba4\u4e3asensor\u7ed9\u5230AI\u7684\u5c3a\u5bf8\uff0c\u53ef\u4ee5\u901a\u8fc7\u8bbe\u7f6einput_image_size\u81ea\u884c\u4fee\u6539\u8f93\u5165\u5c3a\u5bf8\n            ai2d_input_size=input_image_size if input_image_size else self.rgb888p_size\n            # \u8ba1\u7b97affine\u77e9\u9635\u5e76\u8bbe\u7f6eaffine\u9884\u5904\u7406\n            matrix_dst = self.get_affine_matrix(det)\n            self.ai2d.affine(nn.interp_method.cv2_bilinear,0, 0, 127, 1,matrix_dst)\n            # \u6784\u5efa\u9884\u5904\u7406\u6d41\u7a0b,\u53c2\u6570\u4e3a\u9884\u5904\u7406\u8f93\u5165tensor\u7684shape\u548c\u9884\u5904\u7406\u8f93\u51fa\u7684tensor\u7684shape\n            self.ai2d.build([1,3,ai2d_input_size[1],ai2d_input_size[0]],[1,3,self.model_input_size[1],self.model_input_size[0]])\n\n    # \u81ea\u5b9a\u4e49\u540e\u5904\u7406\uff0cresults\u662f\u6a21\u578b\u8f93\u51fa\u7684array\u5217\u8868\uff0c\u8ba1\u7b97\u6b27\u62c9\u89d2\n    def postprocess(self,results):\n        with ScopedTiming("postprocess",self.debug_mode > 0):\n            R,eular = self.get_euler(results[0][0])\n            return R,eular\n\n    def get_affine_matrix(self,bbox):\n        # \u83b7\u53d6\u4eff\u5c04\u77e9\u9635\uff0c\u7528\u4e8e\u5c06\u8fb9\u754c\u6846\u6620\u5c04\u5230\u6a21\u578b\u8f93\u5165\u7a7a\u95f4\n        with ScopedTiming("get_affine_matrix", self.debug_mode > 1):\n            # \u8bbe\u7f6e\u7f29\u653e\u56e0\u5b50\n            factor = 2.7\n            # \u4ece\u8fb9\u754c\u6846\u63d0\u53d6\u5750\u6807\u548c\u5c3a\u5bf8\n            x1, y1, w, h = map(lambda x: int(round(x, 0)), bbox[:4])\n            # \u6a21\u578b\u8f93\u5165\u5927\u5c0f\n            edge_size = self.model_input_size[1]\n            # \u5e73\u79fb\u8ddd\u79bb\uff0c\u4f7f\u5f97\u6a21\u578b\u8f93\u5165\u7a7a\u95f4\u7684\u4e2d\u5fc3\u5bf9\u51c6\u539f\u70b9\n            trans_distance = edge_size / 2.0\n            # \u8ba1\u7b97\u8fb9\u754c\u6846\u4e2d\u5fc3\u70b9\u7684\u5750\u6807\n            center_x = x1 + w / 2.0\n            center_y = y1 + h / 2.0\n            # \u8ba1\u7b97\u6700\u5927\u8fb9\u957f\n            maximum_edge = factor * (h if h > w else w)\n            # \u8ba1\u7b97\u7f29\u653e\u6bd4\u4f8b\n            scale = edge_size * 2.0 / maximum_edge\n            # \u8ba1\u7b97\u5e73\u79fb\u53c2\u6570\n            cx = trans_distance - scale * center_x\n            cy = trans_distance - scale * center_y\n            # \u521b\u5efa\u4eff\u5c04\u77e9\u9635\n            affine_matrix = [scale, 0, cx, 0, scale, cy]\n            return affine_matrix\n\n    def rotation_matrix_to_euler_angles(self,R):\n        # \u5c06\u65cb\u8f6c\u77e9\u9635\uff083x3 \u77e9\u9635\uff09\u8f6c\u6362\u4e3a\u6b27\u62c9\u89d2\uff08pitch\u3001yaw\u3001roll\uff09\n        # \u8ba1\u7b97 sin(yaw)\n        sy = np.sqrt(R[0, 0] ** 2 + R[1, 0] ** 2)\n        if sy < 1e-6:\n            # \u82e5 sin(yaw) \u8fc7\u5c0f\uff0c\u8bf4\u660e pitch \u63a5\u8fd1 \xb190 \u5ea6\n            pitch = np.arctan2(-R[1, 2], R[1, 1]) * 180 / np.pi\n            yaw = np.arctan2(-R[2, 0], sy) * 180 / np.pi\n            roll = 0\n        else:\n            # \u8ba1\u7b97 pitch\u3001yaw\u3001roll \u7684\u89d2\u5ea6\n            pitch = np.arctan2(R[2, 1], R[2, 2]) * 180 / np.pi\n            yaw = np.arctan2(-R[2, 0], sy) * 180 / np.pi\n            roll = np.arctan2(R[1, 0], R[0, 0]) * 180 / np.pi\n        return [pitch,yaw,roll]\n\n    def get_euler(self,data):\n        # \u83b7\u53d6\u65cb\u8f6c\u77e9\u9635\u548c\u6b27\u62c9\u89d2\n        R = data[:3, :3].copy()\n        eular = self.rotation_matrix_to_euler_angles(R)\n        return R,eular\n\n# \u4eba\u8138\u59ff\u6001\u4efb\u52a1\u7c7b\nclass FacePose:\n    def __init__(self,face_det_kmodel,face_pose_kmodel,det_input_size,pose_input_size,anchors,confidence_threshold=0.25,nms_threshold=0.3,rgb888p_size=[1280,720],display_size=[1920,1080],debug_mode=0):\n        # \u4eba\u8138\u68c0\u6d4b\u6a21\u578b\u8def\u5f84\n        self.face_det_kmodel=face_det_kmodel\n        # \u4eba\u8138\u59ff\u6001\u6a21\u578b\u8def\u5f84\n        self.face_pose_kmodel=face_pose_kmodel\n        # \u4eba\u8138\u68c0\u6d4b\u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.det_input_size=det_input_size\n        # \u4eba\u8138\u59ff\u6001\u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.pose_input_size=pose_input_size\n        # anchors\n        self.anchors=anchors\n        # \u7f6e\u4fe1\u5ea6\u9608\u503c\n        self.confidence_threshold=confidence_threshold\n        # nms\u9608\u503c\n        self.nms_threshold=nms_threshold\n        # sensor\u7ed9\u5230AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.rgb888p_size=[ALIGN_UP(rgb888p_size[0],16),rgb888p_size[1]]\n        # \u89c6\u9891\u8f93\u51faVO\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.display_size=[ALIGN_UP(display_size[0],16),display_size[1]]\n        # debug_mode\u6a21\u5f0f\n        self.debug_mode=debug_mode\n        self.face_det=FaceDetApp(self.face_det_kmodel,model_input_size=self.det_input_size,anchors=self.anchors,confidence_threshold=self.confidence_threshold,nms_threshold=self.nms_threshold,rgb888p_size=self.rgb888p_size,display_size=self.display_size,debug_mode=0)\n        self.face_pose=FacePoseApp(self.face_pose_kmodel,model_input_size=self.pose_input_size,rgb888p_size=self.rgb888p_size,display_size=self.display_size)\n        self.face_det.config_preprocess()\n\n    # run\u51fd\u6570\n    def run(self,input_np):\n        # \u4eba\u8138\u68c0\u6d4b\n        det_boxes=self.face_det.run(input_np)\n        pose_res=[]\n        for det_box in det_boxes:\n            # \u5bf9\u68c0\u6d4b\u5230\u7684\u6bcf\u4e00\u4e2a\u4eba\u8138\u505a\u4eba\u8138\u59ff\u6001\u4f30\u8ba1\n            self.face_pose.config_preprocess(det_box)\n            R,eular=self.face_pose.run(input_np)\n            pose_res.append((R,eular))\n        return det_boxes,pose_res\n\n\n    # \u7ed8\u5236\u4eba\u8138\u59ff\u6001\u89d2\u6548\u679c\n    def draw_result(self,pl,dets,pose_res):\n        pl.osd_img.clear()\n        if dets:\n            draw_img_np = np.zeros((self.display_size[1],self.display_size[0],4),dtype=np.uint8)\n            draw_img=image.Image(self.display_size[0], self.display_size[1], image.ARGB8888,alloc=image.ALLOC_REF,data=draw_img_np)\n            line_color = np.array([255, 0, 0 ,255],dtype=np.uint8)    #bgra\n            for i,det in enumerate(dets):\n                # \uff081\uff09\u83b7\u53d6\u4eba\u8138\u59ff\u6001\u77e9\u9635\u548c\u6b27\u62c9\u89d2\n                projections,center_point = self.build_projection_matrix(det)\n                R,euler = pose_res[i]\n                # \uff082\uff09\u904d\u5386\u4eba\u8138\u6295\u5f71\u77e9\u9635\u7684\u5173\u952e\u70b9\uff0c\u8fdb\u884c\u6295\u5f71\uff0c\u5e76\u5c06\u7ed3\u679c\u753b\u5728\u56fe\u50cf\u4e0a\n                first_points = []\n                second_points = []\n                for pp in range(8):\n                    sum_x, sum_y = 0.0, 0.0\n                    for cc in range(3):\n                        sum_x += projections[pp][cc] * R[cc][0]\n                        sum_y += projections[pp][cc] * (-R[cc][1])\n                    center_x,center_y = center_point[0],center_point[1]\n                    x = (sum_x + center_x) / self.rgb888p_size[0] * self.display_size[0]\n                    y = (sum_y + center_y) / self.rgb888p_size[1] * self.display_size[1]\n                    x = max(0, min(x, self.display_size[0]))\n                    y = max(0, min(y, self.display_size[1]))\n                    if pp < 4:\n                        first_points.append((x, y))\n                    else:\n                        second_points.append((x, y))\n                first_points = np.array(first_points,dtype=np.float)\n                aidemo.polylines(draw_img_np,first_points,True,line_color,2,8,0)\n                second_points = np.array(second_points,dtype=np.float)\n                aidemo.polylines(draw_img_np,second_points,True,line_color,2,8,0)\n                for ll in range(4):\n                    x0, y0 = int(first_points[ll][0]),int(first_points[ll][1])\n                    x1, y1 = int(second_points[ll][0]),int(second_points[ll][1])\n                    draw_img.draw_line(x0, y0, x1, y1, color = (255, 0, 0 ,255), thickness = 2)\n            pl.osd_img.copy_from(draw_img)\n\n    def build_projection_matrix(self,det):\n        x1, y1, w, h = map(lambda x: int(round(x, 0)), det[:4])\n        # \u8ba1\u7b97\u8fb9\u754c\u6846\u4e2d\u5fc3\u5750\u6807\n        center_x = x1 + w / 2.0\n        center_y = y1 + h / 2.0\n        # \u5b9a\u4e49\u540e\u90e8\uff08rear\uff09\u548c\u524d\u90e8\uff08front\uff09\u7684\u5c3a\u5bf8\u548c\u6df1\u5ea6\n        rear_width = 0.5 * w\n        rear_height = 0.5 * h\n        rear_depth = 0\n        factor = np.sqrt(2.0)\n        front_width = factor * rear_width\n        front_height = factor * rear_height\n        front_depth = factor * rear_width  # \u4f7f\u7528\u5bbd\u5ea6\u6765\u8ba1\u7b97\u6df1\u5ea6\uff0c\u4e5f\u53ef\u4ee5\u4f7f\u7528\u9ad8\u5ea6\uff0c\u53d6\u51b3\u4e8e\u9700\u6c42\n        # \u5b9a\u4e49\u7acb\u65b9\u4f53\u7684\u9876\u70b9\u5750\u6807\n        temp = [\n            [-rear_width, -rear_height, rear_depth],\n            [-rear_width, rear_height, rear_depth],\n            [rear_width, rear_height, rear_depth],\n            [rear_width, -rear_height, rear_depth],\n            [-front_width, -front_height, front_depth],\n            [-front_width, front_height, front_depth],\n            [front_width, front_height, front_depth],\n            [front_width, -front_height, front_depth]\n        ]\n        projections = np.array(temp)\n        # \u8fd4\u56de\u6295\u5f71\u77e9\u9635\u548c\u4e2d\u5fc3\u5750\u6807\n        return projections, (center_x, center_y)\n\n\nif __name__=="__main__":\n    # \u663e\u793a\u6a21\u5f0f\uff0c\u9ed8\u8ba4"hdmi",\u53ef\u4ee5\u9009\u62e9"hdmi"\u548c"lcd"\n    display_mode="hdmi"\n    if display_mode=="hdmi":\n        display_size=[1920,1080]\n    else:\n        display_size=[800,480]\n    # \u4eba\u8138\u68c0\u6d4b\u6a21\u578b\u8def\u5f84\n    face_det_kmodel_path="/sdcard/app/tests/kmodel/face_detection_320.kmodel"\n    # \u4eba\u8138\u59ff\u6001\u6a21\u578b\u8def\u5f84\n    face_pose_kmodel_path="/sdcard/app/tests/kmodel/face_pose.kmodel"\n    # \u5176\u5b83\u53c2\u6570\n    anchors_path="/sdcard/app/tests/utils/prior_data_320.bin"\n    rgb888p_size=[1920,1080]\n    face_det_input_size=[320,320]\n    face_pose_input_size=[120,120]\n    confidence_threshold=0.5\n    nms_threshold=0.2\n    anchor_len=4200\n    det_dim=4\n    anchors = np.fromfile(anchors_path, dtype=np.float)\n    anchors = anchors.reshape((anchor_len,det_dim))\n\n    # \u521d\u59cb\u5316PipeLine\uff0c\u53ea\u5173\u6ce8\u4f20\u7ed9AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u663e\u793a\u7684\u5206\u8fa8\u7387\n    pl=PipeLine(rgb888p_size=rgb888p_size,display_size=display_size,display_mode=display_mode)\n    pl.create()\n    fp=FacePose(face_det_kmodel_path,face_pose_kmodel_path,det_input_size=face_det_input_size,pose_input_size=face_pose_input_size,anchors=anchors,confidence_threshold=confidence_threshold,nms_threshold=nms_threshold,rgb888p_size=rgb888p_size,display_size=display_size)\n    try:\n        while True:\n            os.exitpoint()\n            with ScopedTiming("total",1):\n                img=pl.get_frame()                      # \u83b7\u53d6\u5f53\u524d\u5e27\n                det_boxes,pose_res=fp.run(img)          # \u63a8\u7406\u5f53\u524d\u5e27\n                fp.draw_result(pl,det_boxes,pose_res)   # \u7ed8\u5236\u63a8\u7406\u6548\u679c\n                pl.show_image()                         # \u5c55\u793a\u63a8\u7406\u6548\u679c\n                gc.collect()\n    except Exception as e:\n        sys.print_exception(e)\n    finally:\n        fp.face_det.deinit()\n        fp.face_pose.deinit()\n        pl.destroy()\n'})}),"\n",(0,i.jsx)(n.h3,{id:"28-\u4eba\u8138\u8bc6\u522b",children:"2.8. \u4eba\u8138\u8bc6\u522b"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:'from libs.PipeLine import PipeLine, ScopedTiming\nfrom libs.AIBase import AIBase\nfrom libs.AI2D import Ai2d\nimport os\nimport ujson\nfrom media.media import *\nfrom time import *\nimport nncase_runtime as nn\nimport ulab.numpy as np\nimport time\nimport image\nimport aidemo\nimport random\nimport gc\nimport sys\nimport math\n\n# \u81ea\u5b9a\u4e49\u4eba\u8138\u68c0\u6d4b\u4efb\u52a1\u7c7b\nclass FaceDetApp(AIBase):\n    def __init__(self,kmodel_path,model_input_size,anchors,confidence_threshold=0.25,nms_threshold=0.3,rgb888p_size=[1920,1080],display_size=[1920,1080],debug_mode=0):\n        super().__init__(kmodel_path,model_input_size,rgb888p_size,debug_mode)\n        # kmodel\u8def\u5f84\n        self.kmodel_path=kmodel_path\n        # \u68c0\u6d4b\u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.model_input_size=model_input_size\n        # \u7f6e\u4fe1\u5ea6\u9608\u503c\n        self.confidence_threshold=confidence_threshold\n        # nms\u9608\u503c\n        self.nms_threshold=nms_threshold\n        self.anchors=anchors\n        # sensor\u7ed9\u5230AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.rgb888p_size=[ALIGN_UP(rgb888p_size[0],16),rgb888p_size[1]]\n        # \u89c6\u9891\u8f93\u51faVO\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.display_size=[ALIGN_UP(display_size[0],16),display_size[1]]\n        # debug\u6a21\u5f0f\n        self.debug_mode=debug_mode\n        # \u5b9e\u4f8b\u5316Ai2d\uff0c\u7528\u4e8e\u5b9e\u73b0\u6a21\u578b\u9884\u5904\u7406\n        self.ai2d=Ai2d(debug_mode)\n        # \u8bbe\u7f6eAi2d\u7684\u8f93\u5165\u8f93\u51fa\u683c\u5f0f\u548c\u7c7b\u578b\n        self.ai2d.set_ai2d_dtype(nn.ai2d_format.NCHW_FMT,nn.ai2d_format.NCHW_FMT,np.uint8, np.uint8)\n\n    # \u914d\u7f6e\u9884\u5904\u7406\u64cd\u4f5c\uff0c\u8fd9\u91cc\u4f7f\u7528\u4e86pad\u548cresize\uff0cAi2d\u652f\u6301crop/shift/pad/resize/affine\uff0c\u5177\u4f53\u4ee3\u7801\u8bf7\u6253\u5f00/sdcard/app/libs/AI2D.py\u67e5\u770b\n    def config_preprocess(self,input_image_size=None):\n        with ScopedTiming("set preprocess config",self.debug_mode > 0):\n            # \u521d\u59cb\u5316ai2d\u9884\u5904\u7406\u914d\u7f6e\uff0c\u9ed8\u8ba4\u4e3asensor\u7ed9\u5230AI\u7684\u5c3a\u5bf8\uff0c\u53ef\u4ee5\u901a\u8fc7\u8bbe\u7f6einput_image_size\u81ea\u884c\u4fee\u6539\u8f93\u5165\u5c3a\u5bf8\n            ai2d_input_size=input_image_size if input_image_size else self.rgb888p_size\n            # \u8ba1\u7b97padding\u53c2\u6570\uff0c\u5e76\u8bbe\u7f6epadding\u9884\u5904\u7406\n            self.ai2d.pad(self.get_pad_param(), 0, [104,117,123])\n            # \u8bbe\u7f6eresize\u9884\u5904\u7406\n            self.ai2d.resize(nn.interp_method.tf_bilinear, nn.interp_mode.half_pixel)\n            # \u6784\u5efa\u9884\u5904\u7406\u6d41\u7a0b,\u53c2\u6570\u4e3a\u9884\u5904\u7406\u8f93\u5165tensor\u7684shape\u548c\u9884\u5904\u7406\u8f93\u51fa\u7684tensor\u7684shape\n            self.ai2d.build([1,3,ai2d_input_size[1],ai2d_input_size[0]],[1,3,self.model_input_size[1],self.model_input_size[0]])\n\n    # \u81ea\u5b9a\u4e49\u540e\u5904\u7406\uff0cresults\u662f\u6a21\u578b\u8f93\u51fa\u7684array\u5217\u8868\uff0c\u8fd9\u91cc\u4f7f\u7528\u4e86aidemo\u5e93\u7684face_det_post_process\u63a5\u53e3\n    def postprocess(self,results):\n        with ScopedTiming("postprocess",self.debug_mode > 0):\n            res = aidemo.face_det_post_process(self.confidence_threshold,self.nms_threshold,self.model_input_size[0],self.anchors,self.rgb888p_size,results)\n            if len(res)==0:\n                return res,res\n            else:\n                return res[0],res[1]\n\n    def get_pad_param(self):\n        dst_w = self.model_input_size[0]\n        dst_h = self.model_input_size[1]\n        # \u8ba1\u7b97\u6700\u5c0f\u7684\u7f29\u653e\u6bd4\u4f8b\uff0c\u7b49\u6bd4\u4f8b\u7f29\u653e\n        ratio_w = dst_w / self.rgb888p_size[0]\n        ratio_h = dst_h / self.rgb888p_size[1]\n        if ratio_w < ratio_h:\n            ratio = ratio_w\n        else:\n            ratio = ratio_h\n        new_w = (int)(ratio * self.rgb888p_size[0])\n        new_h = (int)(ratio * self.rgb888p_size[1])\n        dw = (dst_w - new_w) / 2\n        dh = (dst_h - new_h) / 2\n        top = (int)(round(0))\n        bottom = (int)(round(dh * 2 + 0.1))\n        left = (int)(round(0))\n        right = (int)(round(dw * 2 - 0.1))\n        return [0,0,0,0,top, bottom, left, right]\n\n# \u81ea\u5b9a\u4e49\u4eba\u8138\u6ce8\u518c\u4efb\u52a1\u7c7b\nclass FaceRegistrationApp(AIBase):\n    def __init__(self,kmodel_path,model_input_size,rgb888p_size=[1920,1080],display_size=[1920,1080],debug_mode=0):\n        super().__init__(kmodel_path,model_input_size,rgb888p_size,debug_mode)\n        # kmodel\u8def\u5f84\n        self.kmodel_path=kmodel_path\n        # \u68c0\u6d4b\u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.model_input_size=model_input_size\n        # sensor\u7ed9\u5230AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.rgb888p_size=[ALIGN_UP(rgb888p_size[0],16),rgb888p_size[1]]\n        # \u89c6\u9891\u8f93\u51faVO\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.display_size=[ALIGN_UP(display_size[0],16),display_size[1]]\n        # debug\u6a21\u5f0f\n        self.debug_mode=debug_mode\n        # \u6807\u51c65\u5b98\n        self.umeyama_args_112 = [\n            38.2946 , 51.6963 ,\n            73.5318 , 51.5014 ,\n            56.0252 , 71.7366 ,\n            41.5493 , 92.3655 ,\n            70.7299 , 92.2041\n        ]\n        self.ai2d=Ai2d(debug_mode)\n        self.ai2d.set_ai2d_dtype(nn.ai2d_format.NCHW_FMT,nn.ai2d_format.NCHW_FMT,np.uint8, np.uint8)\n\n    # \u914d\u7f6e\u9884\u5904\u7406\u64cd\u4f5c\uff0c\u8fd9\u91cc\u4f7f\u7528\u4e86affine\uff0cAi2d\u652f\u6301crop/shift/pad/resize/affine\uff0c\u5177\u4f53\u4ee3\u7801\u8bf7\u6253\u5f00/sdcard/app/libs/AI2D.py\u67e5\u770b\n    def config_preprocess(self,landm,input_image_size=None):\n        with ScopedTiming("set preprocess config",self.debug_mode > 0):\n            ai2d_input_size=input_image_size if input_image_size else self.rgb888p_size\n            # \u8ba1\u7b97affine\u77e9\u9635\uff0c\u5e76\u8bbe\u7f6e\u4eff\u5c04\u53d8\u6362\u9884\u5904\u7406\n            affine_matrix = self.get_affine_matrix(landm)\n            self.ai2d.affine(nn.interp_method.cv2_bilinear,0, 0, 127, 1,affine_matrix)\n            # \u6784\u5efa\u9884\u5904\u7406\u6d41\u7a0b,\u53c2\u6570\u4e3a\u9884\u5904\u7406\u8f93\u5165tensor\u7684shape\u548c\u9884\u5904\u7406\u8f93\u51fa\u7684tensor\u7684shape\n            self.ai2d.build([1,3,ai2d_input_size[1],ai2d_input_size[0]],[1,3,self.model_input_size[1],self.model_input_size[0]])\n\n    # \u81ea\u5b9a\u4e49\u540e\u5904\u7406\n    def postprocess(self,results):\n        with ScopedTiming("postprocess",self.debug_mode > 0):\n            return results[0][0]\n\n    def svd22(self,a):\n        # svd\n        s = [0.0, 0.0]\n        u = [0.0, 0.0, 0.0, 0.0]\n        v = [0.0, 0.0, 0.0, 0.0]\n        s[0] = (math.sqrt((a[0] - a[3]) ** 2 + (a[1] + a[2]) ** 2) + math.sqrt((a[0] + a[3]) ** 2 + (a[1] - a[2]) ** 2)) / 2\n        s[1] = abs(s[0] - math.sqrt((a[0] - a[3]) ** 2 + (a[1] + a[2]) ** 2))\n        v[2] = math.sin((math.atan2(2 * (a[0] * a[1] + a[2] * a[3]), a[0] ** 2 - a[1] ** 2 + a[2] ** 2 - a[3] ** 2)) / 2) if \\\n        s[0] > s[1] else 0\n        v[0] = math.sqrt(1 - v[2] ** 2)\n        v[1] = -v[2]\n        v[3] = v[0]\n        u[0] = -(a[0] * v[0] + a[1] * v[2]) / s[0] if s[0] != 0 else 1\n        u[2] = -(a[2] * v[0] + a[3] * v[2]) / s[0] if s[0] != 0 else 0\n        u[1] = (a[0] * v[1] + a[1] * v[3]) / s[1] if s[1] != 0 else -u[2]\n        u[3] = (a[2] * v[1] + a[3] * v[3]) / s[1] if s[1] != 0 else u[0]\n        v[0] = -v[0]\n        v[2] = -v[2]\n        return u, s, v\n\n    def image_umeyama_112(self,src):\n        # \u4f7f\u7528Umeyama\u7b97\u6cd5\u8ba1\u7b97\u4eff\u5c04\u53d8\u6362\u77e9\u9635\n        SRC_NUM = 5\n        SRC_DIM = 2\n        src_mean = [0.0, 0.0]\n        dst_mean = [0.0, 0.0]\n        for i in range(0,SRC_NUM * 2,2):\n            src_mean[0] += src[i]\n            src_mean[1] += src[i + 1]\n            dst_mean[0] += self.umeyama_args_112[i]\n            dst_mean[1] += self.umeyama_args_112[i + 1]\n        src_mean[0] /= SRC_NUM\n        src_mean[1] /= SRC_NUM\n        dst_mean[0] /= SRC_NUM\n        dst_mean[1] /= SRC_NUM\n        src_demean = [[0.0, 0.0] for _ in range(SRC_NUM)]\n        dst_demean = [[0.0, 0.0] for _ in range(SRC_NUM)]\n        for i in range(SRC_NUM):\n            src_demean[i][0] = src[2 * i] - src_mean[0]\n            src_demean[i][1] = src[2 * i + 1] - src_mean[1]\n            dst_demean[i][0] = self.umeyama_args_112[2 * i] - dst_mean[0]\n            dst_demean[i][1] = self.umeyama_args_112[2 * i + 1] - dst_mean[1]\n        A = [[0.0, 0.0], [0.0, 0.0]]\n        for i in range(SRC_DIM):\n            for k in range(SRC_DIM):\n                for j in range(SRC_NUM):\n                    A[i][k] += dst_demean[j][i] * src_demean[j][k]\n                A[i][k] /= SRC_NUM\n        T = [[1, 0, 0], [0, 1, 0], [0, 0, 1]]\n        U, S, V = self.svd22([A[0][0], A[0][1], A[1][0], A[1][1]])\n        T[0][0] = U[0] * V[0] + U[1] * V[2]\n        T[0][1] = U[0] * V[1] + U[1] * V[3]\n        T[1][0] = U[2] * V[0] + U[3] * V[2]\n        T[1][1] = U[2] * V[1] + U[3] * V[3]\n        scale = 1.0\n        src_demean_mean = [0.0, 0.0]\n        src_demean_var = [0.0, 0.0]\n        for i in range(SRC_NUM):\n            src_demean_mean[0] += src_demean[i][0]\n            src_demean_mean[1] += src_demean[i][1]\n        src_demean_mean[0] /= SRC_NUM\n        src_demean_mean[1] /= SRC_NUM\n        for i in range(SRC_NUM):\n            src_demean_var[0] += (src_demean_mean[0] - src_demean[i][0]) * (src_demean_mean[0] - src_demean[i][0])\n            src_demean_var[1] += (src_demean_mean[1] - src_demean[i][1]) * (src_demean_mean[1] - src_demean[i][1])\n        src_demean_var[0] /= SRC_NUM\n        src_demean_var[1] /= SRC_NUM\n        scale = 1.0 / (src_demean_var[0] + src_demean_var[1]) * (S[0] + S[1])\n        T[0][2] = dst_mean[0] - scale * (T[0][0] * src_mean[0] + T[0][1] * src_mean[1])\n        T[1][2] = dst_mean[1] - scale * (T[1][0] * src_mean[0] + T[1][1] * src_mean[1])\n        T[0][0] *= scale\n        T[0][1] *= scale\n        T[1][0] *= scale\n        T[1][1] *= scale\n        return T\n\n    def get_affine_matrix(self,sparse_points):\n        # \u83b7\u53d6affine\u53d8\u6362\u77e9\u9635\n        with ScopedTiming("get_affine_matrix", self.debug_mode > 1):\n            # \u4f7f\u7528Umeyama\u7b97\u6cd5\u8ba1\u7b97\u4eff\u5c04\u53d8\u6362\u77e9\u9635\n            matrix_dst = self.image_umeyama_112(sparse_points)\n            matrix_dst = [matrix_dst[0][0],matrix_dst[0][1],matrix_dst[0][2],\n                          matrix_dst[1][0],matrix_dst[1][1],matrix_dst[1][2]]\n            return matrix_dst\n\n# \u4eba\u8138\u8bc6\u522b\u4efb\u52a1\u7c7b\nclass FaceRecognition:\n    def __init__(self,face_det_kmodel,face_reg_kmodel,det_input_size,reg_input_size,database_dir,anchors,confidence_threshold=0.25,nms_threshold=0.3,face_recognition_threshold=0.75,rgb888p_size=[1280,720],display_size=[1920,1080],debug_mode=0):\n        # \u4eba\u8138\u68c0\u6d4b\u6a21\u578b\u8def\u5f84\n        self.face_det_kmodel=face_det_kmodel\n        # \u4eba\u8138\u8bc6\u522b\u6a21\u578b\u8def\u5f84\n        self.face_reg_kmodel=face_reg_kmodel\n        # \u4eba\u8138\u68c0\u6d4b\u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.det_input_size=det_input_size\n        # \u4eba\u8138\u8bc6\u522b\u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.reg_input_size=reg_input_size\n        self.database_dir=database_dir\n        # anchors\n        self.anchors=anchors\n        # \u7f6e\u4fe1\u5ea6\u9608\u503c\n        self.confidence_threshold=confidence_threshold\n        # nms\u9608\u503c\n        self.nms_threshold=nms_threshold\n        self.face_recognition_threshold=face_recognition_threshold\n        # sensor\u7ed9\u5230AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.rgb888p_size=[ALIGN_UP(rgb888p_size[0],16),rgb888p_size[1]]\n        # \u89c6\u9891\u8f93\u51faVO\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.display_size=[ALIGN_UP(display_size[0],16),display_size[1]]\n        # debug_mode\u6a21\u5f0f\n        self.debug_mode=debug_mode\n        self.max_register_face = 100                  # \u6570\u636e\u5e93\u6700\u591a\u4eba\u8138\u4e2a\u6570\n        self.feature_num = 128                        # \u4eba\u8138\u8bc6\u522b\u7279\u5f81\u7ef4\u5ea6\n        self.valid_register_face = 0                  # \u5df2\u6ce8\u518c\u4eba\u8138\u6570\n        self.db_name= []\n        self.db_data= []\n        self.face_det=FaceDetApp(self.face_det_kmodel,model_input_size=self.det_input_size,anchors=self.anchors,confidence_threshold=self.confidence_threshold,nms_threshold=self.nms_threshold,rgb888p_size=self.rgb888p_size,display_size=self.display_size,debug_mode=0)\n        self.face_reg=FaceRegistrationApp(self.face_reg_kmodel,model_input_size=self.reg_input_size,rgb888p_size=self.rgb888p_size,display_size=self.display_size)\n        self.face_det.config_preprocess()\n        # \u4eba\u8138\u6570\u636e\u5e93\u521d\u59cb\u5316\n        self.database_init()\n\n    # run\u51fd\u6570\n    def run(self,input_np):\n        # \u6267\u884c\u4eba\u8138\u68c0\u6d4b\n        det_boxes,landms=self.face_det.run(input_np)\n        recg_res = []\n        for landm in landms:\n            # \u9488\u5bf9\u6bcf\u4e2a\u4eba\u8138\u4e94\u5b98\u70b9\uff0c\u63a8\u7406\u5f97\u5230\u4eba\u8138\u7279\u5f81\uff0c\u5e76\u8ba1\u7b97\u7279\u5f81\u5728\u6570\u636e\u5e93\u4e2d\u76f8\u4f3c\u5ea6\n            self.face_reg.config_preprocess(landm)\n            feature=self.face_reg.run(input_np)\n            res = self.database_search(feature)\n            recg_res.append(res)\n        return det_boxes,recg_res\n\n    def database_init(self):\n        # \u6570\u636e\u521d\u59cb\u5316\uff0c\u6784\u5efa\u6570\u636e\u5e93\u4eba\u540d\u5217\u8868\u548c\u6570\u636e\u5e93\u7279\u5f81\u5217\u8868\n        with ScopedTiming("database_init", self.debug_mode > 1):\n            db_file_list = os.listdir(self.database_dir)\n            for db_file in db_file_list:\n                if not db_file.endswith(\'.bin\'):\n                    continue\n                if self.valid_register_face >= self.max_register_face:\n                    break\n                valid_index = self.valid_register_face\n                full_db_file = self.database_dir + db_file\n                with open(full_db_file, \'rb\') as f:\n                    data = f.read()\n                feature = np.frombuffer(data, dtype=np.float)\n                self.db_data.append(feature)\n                name = db_file.split(\'.\')[0]\n                self.db_name.append(name)\n                self.valid_register_face += 1\n\n    def database_reset(self):\n        # \u6570\u636e\u5e93\u6e05\u7a7a\n        with ScopedTiming("database_reset", self.debug_mode > 1):\n            print("database clearing...")\n            self.db_name = []\n            self.db_data = []\n            self.valid_register_face = 0\n            print("database clear Done!")\n\n    def database_search(self,feature):\n        # \u6570\u636e\u5e93\u67e5\u8be2\n        with ScopedTiming("database_search", self.debug_mode > 1):\n            v_id = -1\n            v_score_max = 0.0\n            # \u5c06\u5f53\u524d\u4eba\u8138\u7279\u5f81\u5f52\u4e00\u5316\n            feature /= np.linalg.norm(feature)\n            # \u904d\u5386\u5f53\u524d\u4eba\u8138\u6570\u636e\u5e93\uff0c\u7edf\u8ba1\u6700\u9ad8\u5f97\u5206\n            for i in range(self.valid_register_face):\n                db_feature = self.db_data[i]\n                db_feature /= np.linalg.norm(db_feature)\n                # \u8ba1\u7b97\u6570\u636e\u5e93\u7279\u5f81\u4e0e\u5f53\u524d\u4eba\u8138\u7279\u5f81\u76f8\u4f3c\u5ea6\n                v_score = np.dot(feature, db_feature)/2 + 0.5\n                if v_score > v_score_max:\n                    v_score_max = v_score\n                    v_id = i\n            if v_id == -1:\n                # \u6570\u636e\u5e93\u4e2d\u65e0\u4eba\u8138\n                return \'unknown\'\n            elif v_score_max < self.face_recognition_threshold:\n                # \u5c0f\u4e8e\u4eba\u8138\u8bc6\u522b\u9608\u503c\uff0c\u672a\u8bc6\u522b\n                return \'unknown\'\n            else:\n                # \u8bc6\u522b\u6210\u529f\n                result = \'name: {}, score:{}\'.format(self.db_name[v_id],v_score_max)\n                return result\n\n    # \u7ed8\u5236\u8bc6\u522b\u7ed3\u679c\n    def draw_result(self,pl,dets,recg_results):\n        pl.osd_img.clear()\n        if dets:\n            for i,det in enumerate(dets):\n                # \uff081\uff09\u753b\u4eba\u8138\u6846\n                x1, y1, w, h = map(lambda x: int(round(x, 0)), det[:4])\n                x1 = x1 * self.display_size[0]//self.rgb888p_size[0]\n                y1 = y1 * self.display_size[1]//self.rgb888p_size[1]\n                w =  w * self.display_size[0]//self.rgb888p_size[0]\n                h = h * self.display_size[1]//self.rgb888p_size[1]\n                pl.osd_img.draw_rectangle(x1,y1, w, h, color=(255,0, 0, 255), thickness = 4)\n                # \uff082\uff09\u5199\u4eba\u8138\u8bc6\u522b\u7ed3\u679c\n                recg_text = recg_results[i]\n                pl.osd_img.draw_string_advanced(x1,y1,32,recg_text,color=(255, 255, 0, 0))\n\n\nif __name__=="__main__":\n    # \u6ce8\u610f\uff1a\u6267\u884c\u4eba\u8138\u8bc6\u522b\u4efb\u52a1\u4e4b\u524d\uff0c\u9700\u8981\u5148\u6267\u884c\u4eba\u8138\u6ce8\u518c\u4efb\u52a1\u8fdb\u884c\u4eba\u8138\u8eab\u4efd\u6ce8\u518c\u751f\u6210feature\u6570\u636e\u5e93\n    # \u663e\u793a\u6a21\u5f0f\uff0c\u9ed8\u8ba4"hdmi",\u53ef\u4ee5\u9009\u62e9"hdmi"\u548c"lcd"\n    display_mode="hdmi"\n    if display_mode=="hdmi":\n        display_size=[1920,1080]\n    else:\n        display_size=[800,480]\n    # \u4eba\u8138\u68c0\u6d4b\u6a21\u578b\u8def\u5f84\n    face_det_kmodel_path="/sdcard/app/tests/kmodel/face_detection_320.kmodel"\n    # \u4eba\u8138\u8bc6\u522b\u6a21\u578b\u8def\u5f84\n    face_reg_kmodel_path="/sdcard/app/tests/kmodel/face_recognition.kmodel"\n    # \u5176\u5b83\u53c2\u6570\n    anchors_path="/sdcard/app/tests/utils/prior_data_320.bin"\n    database_dir ="/sdcard/app/tests/utils/db/"\n    rgb888p_size=[1920,1080]\n    face_det_input_size=[320,320]\n    face_reg_input_size=[112,112]\n    confidence_threshold=0.5\n    nms_threshold=0.2\n    anchor_len=4200\n    det_dim=4\n    anchors = np.fromfile(anchors_path, dtype=np.float)\n    anchors = anchors.reshape((anchor_len,det_dim))\n    face_recognition_threshold = 0.75        # \u4eba\u8138\u8bc6\u522b\u9608\u503c\n\n    # \u521d\u59cb\u5316PipeLine\uff0c\u53ea\u5173\u6ce8\u4f20\u7ed9AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u663e\u793a\u7684\u5206\u8fa8\u7387\n    pl=PipeLine(rgb888p_size=rgb888p_size,display_size=display_size,display_mode=display_mode)\n    pl.create()\n    fr=FaceRecognition(face_det_kmodel_path,face_reg_kmodel_path,det_input_size=face_det_input_size,reg_input_size=face_reg_input_size,database_dir=database_dir,anchors=anchors,confidence_threshold=confidence_threshold,nms_threshold=nms_threshold,face_recognition_threshold=face_recognition_threshold,rgb888p_size=rgb888p_size,display_size=display_size)\n    try:\n        while True:\n            os.exitpoint()\n            with ScopedTiming("total", 1):\n                img=pl.get_frame()                      # \u83b7\u53d6\u5f53\u524d\u5e27\n                det_boxes,recg_res=fr.run(img)          # \u63a8\u7406\u5f53\u524d\u5e27\n                fr.draw_result(pl,det_boxes,recg_res)   # \u7ed8\u5236\u63a8\u7406\u7ed3\u679c\n                pl.show_image()                         # \u5c55\u793a\u63a8\u7406\u6548\u679c\n                gc.collect()\n    except Exception as e:\n        sys.print_exception(e)\n    finally:\n        fr.face_det.deinit()\n        fr.face_reg.deinit()\n        pl.destroy()\n'})}),"\n",(0,i.jsx)(n.h3,{id:"29-\u4eba\u8138\u6ce8\u518c",children:"2.9. \u4eba\u8138\u6ce8\u518c"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:'from libs.PipeLine import PipeLine, ScopedTiming\nfrom libs.AIBase import AIBase\nfrom libs.AI2D import Ai2d\nimport os\nimport ujson\nfrom media.media import *\nfrom time import *\nimport nncase_runtime as nn\nimport ulab.numpy as np\nimport time\nimport image\nimport aidemo\nimport random\nimport gc\nimport sys\nimport math\n\n# \u81ea\u5b9a\u4e49\u4eba\u8138\u68c0\u6d4b\u4efb\u52a1\u7c7b\nclass FaceDetApp(AIBase):\n    def __init__(self,kmodel_path,model_input_size,anchors,confidence_threshold=0.25,nms_threshold=0.3,rgb888p_size=[1280,720],display_size=[1920,1080],debug_mode=0):\n        super().__init__(kmodel_path,model_input_size,rgb888p_size,debug_mode)\n        # kmodel\u8def\u5f84\n        self.kmodel_path=kmodel_path\n        # \u68c0\u6d4b\u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.model_input_size=model_input_size\n        # \u7f6e\u4fe1\u5ea6\u9608\u503c\n        self.confidence_threshold=confidence_threshold\n        # nms\u9608\u503c\n        self.nms_threshold=nms_threshold\n        self.anchors=anchors\n        # sensor\u7ed9\u5230AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.rgb888p_size=[ALIGN_UP(rgb888p_size[0],16),rgb888p_size[1]]\n        # \u89c6\u9891\u8f93\u51faVO\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.display_size=[ALIGN_UP(display_size[0],16),display_size[1]]\n        # debug\u6a21\u5f0f\n        self.debug_mode=debug_mode\n        # \u5b9e\u4f8b\u5316Ai2d\uff0c\u7528\u4e8e\u5b9e\u73b0\u6a21\u578b\u9884\u5904\u7406\n        self.ai2d=Ai2d(debug_mode)\n        # \u8bbe\u7f6eAi2d\u7684\u8f93\u5165\u8f93\u51fa\u683c\u5f0f\u548c\u7c7b\u578b\n        self.ai2d.set_ai2d_dtype(nn.ai2d_format.NCHW_FMT,nn.ai2d_format.NCHW_FMT,np.uint8, np.uint8)\n        self.image_size=[]\n\n    # \u914d\u7f6e\u9884\u5904\u7406\u64cd\u4f5c\uff0c\u8fd9\u91cc\u4f7f\u7528\u4e86pad\u548cresize\uff0cAi2d\u652f\u6301crop/shift/pad/resize/affine\uff0c\u5177\u4f53\u4ee3\u7801\u8bf7\u6253\u5f00/sdcard/app/libs/AI2D.py\u67e5\u770b\n    def config_preprocess(self,input_image_size=None):\n        with ScopedTiming("set preprocess config",self.debug_mode > 0):\n            # \u521d\u59cb\u5316ai2d\u9884\u5904\u7406\u914d\u7f6e\uff0c\u9ed8\u8ba4\u4e3asensor\u7ed9\u5230AI\u7684\u5c3a\u5bf8\uff0c\u53ef\u4ee5\u901a\u8fc7\u8bbe\u7f6einput_image_size\u81ea\u884c\u4fee\u6539\u8f93\u5165\u5c3a\u5bf8\n            ai2d_input_size=input_image_size if input_image_size else self.rgb888p_size\n            self.image_size=[input_image_size[1],input_image_size[0]]\n            # \u8ba1\u7b97padding\u53c2\u6570\uff0c\u5e76\u8bbe\u7f6epadding\u9884\u5904\u7406\n            self.ai2d.pad(self.get_pad_param(ai2d_input_size), 0, [104,117,123])\n            # \u8bbe\u7f6eresize\u9884\u5904\u7406\n            self.ai2d.resize(nn.interp_method.tf_bilinear, nn.interp_mode.half_pixel)\n            # \u6784\u5efa\u9884\u5904\u7406\u6d41\u7a0b,\u53c2\u6570\u4e3a\u9884\u5904\u7406\u8f93\u5165tensor\u7684shape\u548c\u9884\u5904\u7406\u8f93\u51fa\u7684tensor\u7684shape\n            self.ai2d.build([1,3,ai2d_input_size[1],ai2d_input_size[0]],[1,3,self.model_input_size[1],self.model_input_size[0]])\n\n    # \u81ea\u5b9a\u4e49\u540e\u5904\u7406\uff0cresults\u662f\u6a21\u578b\u8f93\u51fa\u7684array\u5217\u8868\uff0c\u8fd9\u91cc\u4f7f\u7528\u4e86aidemo\u5e93\u7684face_det_post_process\u63a5\u53e3\n    def postprocess(self,results):\n        with ScopedTiming("postprocess",self.debug_mode > 0):\n            res = aidemo.face_det_post_process(self.confidence_threshold,self.nms_threshold,self.model_input_size[0],self.anchors,self.image_size,results)\n            if len(res)==0:\n                return res\n            else:\n                return res[0],res[1]\n\n    def get_pad_param(self,image_input_size):\n        dst_w = self.model_input_size[0]\n        dst_h = self.model_input_size[1]\n        # \u8ba1\u7b97\u6700\u5c0f\u7684\u7f29\u653e\u6bd4\u4f8b\uff0c\u7b49\u6bd4\u4f8b\u7f29\u653e\n        ratio_w = dst_w / image_input_size[0]\n        ratio_h = dst_h / image_input_size[1]\n        if ratio_w < ratio_h:\n            ratio = ratio_w\n        else:\n            ratio = ratio_h\n        new_w = (int)(ratio * image_input_size[0])\n        new_h = (int)(ratio * image_input_size[1])\n        dw = (dst_w - new_w) / 2\n        dh = (dst_h - new_h) / 2\n        top = (int)(round(0))\n        bottom = (int)(round(dh * 2 + 0.1))\n        left = (int)(round(0))\n        right = (int)(round(dw * 2 - 0.1))\n        return [0,0,0,0,top, bottom, left, right]\n\n# \u81ea\u5b9a\u4e49\u4eba\u8138\u6ce8\u518c\u4efb\u52a1\u7c7b\nclass FaceRegistrationApp(AIBase):\n    def __init__(self,kmodel_path,model_input_size,rgb888p_size=[1920,1080],display_size=[1920,1080],debug_mode=0):\n        super().__init__(kmodel_path,model_input_size,rgb888p_size,debug_mode)\n        # kmodel\u8def\u5f84\n        self.kmodel_path=kmodel_path\n        # \u4eba\u8138\u6ce8\u518c\u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.model_input_size=model_input_size\n        # sensor\u7ed9\u5230AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.rgb888p_size=[ALIGN_UP(rgb888p_size[0],16),rgb888p_size[1]]\n        # \u89c6\u9891\u8f93\u51faVO\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.display_size=[ALIGN_UP(display_size[0],16),display_size[1]]\n        # debug\u6a21\u5f0f\n        self.debug_mode=debug_mode\n        # \u6807\u51c65\u5b98\n        self.umeyama_args_112 = [\n            38.2946 , 51.6963 ,\n            73.5318 , 51.5014 ,\n            56.0252 , 71.7366 ,\n            41.5493 , 92.3655 ,\n            70.7299 , 92.2041\n        ]\n        self.ai2d=Ai2d(debug_mode)\n        self.ai2d.set_ai2d_dtype(nn.ai2d_format.NCHW_FMT,nn.ai2d_format.NCHW_FMT,np.uint8, np.uint8)\n\n    # \u914d\u7f6e\u9884\u5904\u7406\u64cd\u4f5c\uff0c\u8fd9\u91cc\u4f7f\u7528\u4e86affine\uff0cAi2d\u652f\u6301crop/shift/pad/resize/affine\uff0c\u5177\u4f53\u4ee3\u7801\u8bf7\u6253\u5f00/sdcard/app/libs/AI2D.py\u67e5\u770b\n    def config_preprocess(self,landm,input_image_size=None):\n        with ScopedTiming("set preprocess config",self.debug_mode > 0):\n            ai2d_input_size=input_image_size if input_image_size else self.rgb888p_size\n            # \u8ba1\u7b97affine\u77e9\u9635\uff0c\u5e76\u8bbe\u7f6e\u4eff\u5c04\u53d8\u6362\u9884\u5904\u7406\n            affine_matrix = self.get_affine_matrix(landm)\n            self.ai2d.affine(nn.interp_method.cv2_bilinear,0, 0, 127, 1,affine_matrix)\n            # \u6784\u5efa\u9884\u5904\u7406\u6d41\u7a0b,\u53c2\u6570\u4e3a\u9884\u5904\u7406\u8f93\u5165tensor\u7684shape\u548c\u9884\u5904\u7406\u8f93\u51fa\u7684tensor\u7684shape\n            self.ai2d.build([1,3,ai2d_input_size[1],ai2d_input_size[0]],[1,3,self.model_input_size[1],self.model_input_size[0]])\n\n    # \u81ea\u5b9a\u4e49\u540e\u5904\u7406\n    def postprocess(self,results):\n        with ScopedTiming("postprocess",self.debug_mode > 0):\n            return results[0][0]\n\n    def svd22(self,a):\n        # svd\n        s = [0.0, 0.0]\n        u = [0.0, 0.0, 0.0, 0.0]\n        v = [0.0, 0.0, 0.0, 0.0]\n        s[0] = (math.sqrt((a[0] - a[3]) ** 2 + (a[1] + a[2]) ** 2) + math.sqrt((a[0] + a[3]) ** 2 + (a[1] - a[2]) ** 2)) / 2\n        s[1] = abs(s[0] - math.sqrt((a[0] - a[3]) ** 2 + (a[1] + a[2]) ** 2))\n        v[2] = math.sin((math.atan2(2 * (a[0] * a[1] + a[2] * a[3]), a[0] ** 2 - a[1] ** 2 + a[2] ** 2 - a[3] ** 2)) / 2) if \\\n        s[0] > s[1] else 0\n        v[0] = math.sqrt(1 - v[2] ** 2)\n        v[1] = -v[2]\n        v[3] = v[0]\n        u[0] = -(a[0] * v[0] + a[1] * v[2]) / s[0] if s[0] != 0 else 1\n        u[2] = -(a[2] * v[0] + a[3] * v[2]) / s[0] if s[0] != 0 else 0\n        u[1] = (a[0] * v[1] + a[1] * v[3]) / s[1] if s[1] != 0 else -u[2]\n        u[3] = (a[2] * v[1] + a[3] * v[3]) / s[1] if s[1] != 0 else u[0]\n        v[0] = -v[0]\n        v[2] = -v[2]\n        return u, s, v\n\n    def image_umeyama_112(self,src):\n        # \u4f7f\u7528Umeyama\u7b97\u6cd5\u8ba1\u7b97\u4eff\u5c04\u53d8\u6362\u77e9\u9635\n        SRC_NUM = 5\n        SRC_DIM = 2\n        src_mean = [0.0, 0.0]\n        dst_mean = [0.0, 0.0]\n        for i in range(0,SRC_NUM * 2,2):\n            src_mean[0] += src[i]\n            src_mean[1] += src[i + 1]\n            dst_mean[0] += self.umeyama_args_112[i]\n            dst_mean[1] += self.umeyama_args_112[i + 1]\n        src_mean[0] /= SRC_NUM\n        src_mean[1] /= SRC_NUM\n        dst_mean[0] /= SRC_NUM\n        dst_mean[1] /= SRC_NUM\n        src_demean = [[0.0, 0.0] for _ in range(SRC_NUM)]\n        dst_demean = [[0.0, 0.0] for _ in range(SRC_NUM)]\n        for i in range(SRC_NUM):\n            src_demean[i][0] = src[2 * i] - src_mean[0]\n            src_demean[i][1] = src[2 * i + 1] - src_mean[1]\n            dst_demean[i][0] = self.umeyama_args_112[2 * i] - dst_mean[0]\n            dst_demean[i][1] = self.umeyama_args_112[2 * i + 1] - dst_mean[1]\n        A = [[0.0, 0.0], [0.0, 0.0]]\n        for i in range(SRC_DIM):\n            for k in range(SRC_DIM):\n                for j in range(SRC_NUM):\n                    A[i][k] += dst_demean[j][i] * src_demean[j][k]\n                A[i][k] /= SRC_NUM\n        T = [[1, 0, 0], [0, 1, 0], [0, 0, 1]]\n        U, S, V = self.svd22([A[0][0], A[0][1], A[1][0], A[1][1]])\n        T[0][0] = U[0] * V[0] + U[1] * V[2]\n        T[0][1] = U[0] * V[1] + U[1] * V[3]\n        T[1][0] = U[2] * V[0] + U[3] * V[2]\n        T[1][1] = U[2] * V[1] + U[3] * V[3]\n        scale = 1.0\n        src_demean_mean = [0.0, 0.0]\n        src_demean_var = [0.0, 0.0]\n        for i in range(SRC_NUM):\n            src_demean_mean[0] += src_demean[i][0]\n            src_demean_mean[1] += src_demean[i][1]\n        src_demean_mean[0] /= SRC_NUM\n        src_demean_mean[1] /= SRC_NUM\n        for i in range(SRC_NUM):\n            src_demean_var[0] += (src_demean_mean[0] - src_demean[i][0]) * (src_demean_mean[0] - src_demean[i][0])\n            src_demean_var[1] += (src_demean_mean[1] - src_demean[i][1]) * (src_demean_mean[1] - src_demean[i][1])\n        src_demean_var[0] /= SRC_NUM\n        src_demean_var[1] /= SRC_NUM\n        scale = 1.0 / (src_demean_var[0] + src_demean_var[1]) * (S[0] + S[1])\n        T[0][2] = dst_mean[0] - scale * (T[0][0] * src_mean[0] + T[0][1] * src_mean[1])\n        T[1][2] = dst_mean[1] - scale * (T[1][0] * src_mean[0] + T[1][1] * src_mean[1])\n        T[0][0] *= scale\n        T[0][1] *= scale\n        T[1][0] *= scale\n        T[1][1] *= scale\n        return T\n\n    def get_affine_matrix(self,sparse_points):\n        # \u83b7\u53d6affine\u53d8\u6362\u77e9\u9635\n        with ScopedTiming("get_affine_matrix", self.debug_mode > 1):\n            # \u4f7f\u7528Umeyama\u7b97\u6cd5\u8ba1\u7b97\u4eff\u5c04\u53d8\u6362\u77e9\u9635\n            matrix_dst = self.image_umeyama_112(sparse_points)\n            matrix_dst = [matrix_dst[0][0],matrix_dst[0][1],matrix_dst[0][2],\n                          matrix_dst[1][0],matrix_dst[1][1],matrix_dst[1][2]]\n            return matrix_dst\n\n# \u4eba\u8138\u6ce8\u518c\u4efb\u52a1\u7c7b\nclass FaceRegistration:\n    def __init__(self,face_det_kmodel,face_reg_kmodel,det_input_size,reg_input_size,database_dir,anchors,confidence_threshold=0.25,nms_threshold=0.3,rgb888p_size=[1280,720],display_size=[1920,1080],debug_mode=0):\n        # \u4eba\u8138\u68c0\u6d4b\u6a21\u578b\u8def\u5f84\n        self.face_det_kmodel=face_det_kmodel\n        # \u4eba\u8138\u6ce8\u518c\u6a21\u578b\u8def\u5f84\n        self.face_reg_kmodel=face_reg_kmodel\n        # \u4eba\u8138\u68c0\u6d4b\u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.det_input_size=det_input_size\n        # \u4eba\u8138\u6ce8\u518c\u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.reg_input_size=reg_input_size\n        self.database_dir=database_dir\n        # anchors\n        self.anchors=anchors\n        # \u7f6e\u4fe1\u5ea6\u9608\u503c\n        self.confidence_threshold=confidence_threshold\n        # nms\u9608\u503c\n        self.nms_threshold=nms_threshold\n        # sensor\u7ed9\u5230AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.rgb888p_size=[ALIGN_UP(rgb888p_size[0],16),rgb888p_size[1]]\n        # \u89c6\u9891\u8f93\u51faVO\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.display_size=[ALIGN_UP(display_size[0],16),display_size[1]]\n        # debug_mode\u6a21\u5f0f\n        self.debug_mode=debug_mode\n        self.face_det=FaceDetApp(self.face_det_kmodel,model_input_size=self.det_input_size,anchors=self.anchors,confidence_threshold=self.confidence_threshold,nms_threshold=self.nms_threshold,debug_mode=0)\n        self.face_reg=FaceRegistrationApp(self.face_reg_kmodel,model_input_size=self.reg_input_size,rgb888p_size=self.rgb888p_size)\n\n    # run\u51fd\u6570\n    def run(self,input_np,img_file):\n        self.face_det.config_preprocess(input_image_size=[input_np.shape[3],input_np.shape[2]])\n        det_boxes,landms=self.face_det.run(input_np)\n        if det_boxes:\n            if det_boxes.shape[0] == 1:\n                # \u82e5\u662f\u53ea\u68c0\u6d4b\u5230\u4e00\u5f20\u4eba\u8138\uff0c\u5219\u5c06\u8be5\u4eba\u8138\u6ce8\u518c\u5230\u6570\u636e\u5e93\n                db_i_name = img_file.split(\'.\')[0]\n                for landm in landms:\n                    self.face_reg.config_preprocess(landm,input_image_size=[input_np.shape[3],input_np.shape[2]])\n                    reg_result = self.face_reg.run(input_np)\n                    with open(self.database_dir+\'{}.bin\'.format(db_i_name), "wb") as file:\n                        file.write(reg_result.tobytes())\n                        print(\'Success!\')\n            else:\n                print(\'Only one person in a picture when you sign up\')\n        else:\n            print(\'No person detected\')\n\n    def image2rgb888array(self,img):   #4\u7ef4\n        # \u5c06Image\u8f6c\u6362\u4e3argb888\u683c\u5f0f\n        with ScopedTiming("fr_kpu_deinit",self.debug_mode > 0):\n            img_data_rgb888=img.to_rgb888()\n            # hwc,rgb888\n            img_hwc=img_data_rgb888.to_numpy_ref()\n            shape=img_hwc.shape\n            img_tmp = img_hwc.reshape((shape[0] * shape[1], shape[2]))\n            img_tmp_trans = img_tmp.transpose()\n            img_res=img_tmp_trans.copy()\n            # chw,rgb888\n            img_return=img_res.reshape((1,shape[2],shape[0],shape[1]))\n        return  img_return\n\n\nif __name__=="__main__":\n    # \u4eba\u8138\u68c0\u6d4b\u6a21\u578b\u8def\u5f84\n    face_det_kmodel_path="/sdcard/app/tests/kmodel/face_detection_320.kmodel"\n    # \u4eba\u8138\u6ce8\u518c\u6a21\u578b\u8def\u5f84\n    face_reg_kmodel_path="/sdcard/app/tests/kmodel/face_recognition.kmodel"\n    # \u5176\u5b83\u53c2\u6570\n    anchors_path="/sdcard/app/tests/utils/prior_data_320.bin"\n    database_dir="/sdcard/app/tests/utils/db/"\n    database_img_dir="/sdcard/app/tests/utils/db_img/"\n    face_det_input_size=[320,320]\n    face_reg_input_size=[112,112]\n    confidence_threshold=0.5\n    nms_threshold=0.2\n    anchor_len=4200\n    det_dim=4\n    anchors = np.fromfile(anchors_path, dtype=np.float)\n    anchors = anchors.reshape((anchor_len,det_dim))\n    max_register_face = 100              #\u6570\u636e\u5e93\u6700\u591a\u4eba\u8138\u4e2a\u6570\n    feature_num = 128                    #\u4eba\u8138\u8bc6\u522b\u7279\u5f81\u7ef4\u5ea6\n\n    fr=FaceRegistration(face_det_kmodel_path,face_reg_kmodel_path,det_input_size=face_det_input_size,reg_input_size=face_reg_input_size,database_dir=database_dir,anchors=anchors,confidence_threshold=confidence_threshold,nms_threshold=nms_threshold)\n    try:\n        # \u83b7\u53d6\u56fe\u50cf\u5217\u8868\n        img_list = os.listdir(database_img_dir)\n        for img_file in img_list:\n            #\u672c\u5730\u8bfb\u53d6\u4e00\u5f20\u56fe\u50cf\n            full_img_file = database_img_dir + img_file\n            print(full_img_file)\n            img = image.Image(full_img_file)\n            img.compress_for_ide()\n            # \u8f6crgb888\u7684chw\u683c\u5f0f\n            rgb888p_img_ndarry = fr.image2rgb888array(img)\n            # \u4eba\u8138\u6ce8\u518c\n            fr.run(rgb888p_img_ndarry,img_file)\n            gc.collect()\n    except Exception as e:\n        sys.print_exception(e)\n    finally:\n        fr.face_det.deinit()\n        fr.face_reg.deinit()\n'})}),"\n",(0,i.jsx)(n.h3,{id:"210-\u8dcc\u5012\u68c0\u6d4b",children:"2.10. \u8dcc\u5012\u68c0\u6d4b"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:'from libs.PipeLine import PipeLine, ScopedTiming\nfrom libs.AIBase import AIBase\nfrom libs.AI2D import Ai2d\nimport os\nimport ujson\nfrom media.media import *\nfrom time import *\nimport nncase_runtime as nn\nimport ulab.numpy as np\nimport time\nimport utime\nimport image\nimport random\nimport gc\nimport sys\nimport aicube\n\n# \u81ea\u5b9a\u4e49\u8dcc\u5012\u68c0\u6d4b\u7c7b\uff0c\u7ee7\u627f\u81eaAIBase\u57fa\u7c7b\nclass FallDetectionApp(AIBase):\n    def __init__(self, kmodel_path, model_input_size, labels, anchors, confidence_threshold=0.2, nms_threshold=0.5, nms_option=False, strides=[8,16,32], rgb888p_size=[224,224], display_size=[1920,1080], debug_mode=0):\n        super().__init__(kmodel_path, model_input_size, rgb888p_size, debug_mode)  # \u8c03\u7528\u57fa\u7c7b\u7684\u6784\u9020\u51fd\u6570\n        self.kmodel_path = kmodel_path                      # \u6a21\u578b\u6587\u4ef6\u8def\u5f84\n        self.model_input_size = model_input_size            # \u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.labels = labels                                # \u5206\u7c7b\u6807\u7b7e\n        self.anchors = anchors                              # \u951a\u70b9\u6570\u636e\uff0c\u7528\u4e8e\u8dcc\u5012\u68c0\u6d4b\n        self.strides = strides                              # \u6b65\u957f\u8bbe\u7f6e\n        self.confidence_threshold = confidence_threshold    # \u7f6e\u4fe1\u5ea6\u9608\u503c\n        self.nms_threshold = nms_threshold                  # NMS\uff08\u975e\u6781\u5927\u503c\u6291\u5236\uff09\u9608\u503c\n        self.nms_option = nms_option                        # NMS\u9009\u9879\n        self.rgb888p_size = [ALIGN_UP(rgb888p_size[0], 16), rgb888p_size[1]]  # sensor\u7ed9\u5230AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u5e76\u5bf9\u5bbd\u5ea6\u8fdb\u884c16\u7684\u5bf9\u9f50\n        self.display_size = [ALIGN_UP(display_size[0], 16), display_size[1]]  # \u663e\u793a\u5206\u8fa8\u7387\uff0c\u5e76\u5bf9\u5bbd\u5ea6\u8fdb\u884c16\u7684\u5bf9\u9f50\n        self.debug_mode = debug_mode                                          # \u662f\u5426\u5f00\u542f\u8c03\u8bd5\u6a21\u5f0f\n        self.color = [(255,0, 0, 255), (255,0, 255, 0), (255,255,0, 0), (255,255,0, 255)]  # \u7528\u4e8e\u7ed8\u5236\u4e0d\u540c\u7c7b\u522b\u7684\u989c\u8272\n        # Ai2d\u5b9e\u4f8b\uff0c\u7528\u4e8e\u5b9e\u73b0\u6a21\u578b\u9884\u5904\u7406\n        self.ai2d = Ai2d(debug_mode)\n        # \u8bbe\u7f6eAi2d\u7684\u8f93\u5165\u8f93\u51fa\u683c\u5f0f\u548c\u7c7b\u578b\n        self.ai2d.set_ai2d_dtype(nn.ai2d_format.NCHW_FMT, nn.ai2d_format.NCHW_FMT, np.uint8, np.uint8)\n\n    # \u914d\u7f6e\u9884\u5904\u7406\u64cd\u4f5c\uff0c\u8fd9\u91cc\u4f7f\u7528\u4e86pad\u548cresize\uff0cAi2d\u652f\u6301crop/shift/pad/resize/affine\uff0c\u5177\u4f53\u4ee3\u7801\u8bf7\u6253\u5f00/sdcard/app/libs/AI2D.py\u67e5\u770b\n    def config_preprocess(self, input_image_size=None):\n        with ScopedTiming("set preprocess config", self.debug_mode > 0):                    # \u8ba1\u65f6\u5668\uff0c\u5982\u679cdebug_mode\u5927\u4e8e0\u5219\u5f00\u542f\n            ai2d_input_size = input_image_size if input_image_size else self.rgb888p_size   # \u521d\u59cb\u5316ai2d\u9884\u5904\u7406\u914d\u7f6e\uff0c\u9ed8\u8ba4\u4e3asensor\u7ed9\u5230AI\u7684\u5c3a\u5bf8\uff0c\u53ef\u4ee5\u901a\u8fc7\u8bbe\u7f6einput_image_size\u81ea\u884c\u4fee\u6539\u8f93\u5165\u5c3a\u5bf8\n            top, bottom, left, right = self.get_padding_param()                             # \u83b7\u53d6padding\u53c2\u6570\n            self.ai2d.pad([0, 0, 0, 0, top, bottom, left, right], 0, [0,0,0])               # \u586b\u5145\u8fb9\u7f18\n            self.ai2d.resize(nn.interp_method.tf_bilinear, nn.interp_mode.half_pixel)       # \u7f29\u653e\u56fe\u50cf\n            self.ai2d.build([1,3,ai2d_input_size[1],ai2d_input_size[0]],[1,3,self.model_input_size[1],self.model_input_size[0]])  # \u6784\u5efa\u9884\u5904\u7406\u6d41\u7a0b\n\n    # \u81ea\u5b9a\u4e49\u5f53\u524d\u4efb\u52a1\u7684\u540e\u5904\u7406\uff0cresults\u662f\u6a21\u578b\u8f93\u51faarray\u7684\u5217\u8868\uff0c\u8fd9\u91cc\u4f7f\u7528\u4e86aicube\u5e93\u7684anchorbasedet_post_process\u63a5\u53e3\n    def postprocess(self, results):\n        with ScopedTiming("postprocess", self.debug_mode > 0):\n            dets = aicube.anchorbasedet_post_process(results[0], results[1], results[2], self.model_input_size, self.rgb888p_size, self.strides, len(self.labels), self.confidence_threshold, self.nms_threshold, self.anchors, self.nms_option)\n            return dets\n\n    # \u7ed8\u5236\u68c0\u6d4b\u7ed3\u679c\u5230\u753b\u9762\u4e0a\n    def draw_result(self, pl, dets):\n        with ScopedTiming("display_draw", self.debug_mode > 0):\n            if dets:\n                pl.osd_img.clear()  # \u6e05\u9664OSD\u56fe\u50cf\n                for det_box in dets:\n                    # \u8ba1\u7b97\u663e\u793a\u5206\u8fa8\u7387\u4e0b\u7684\u5750\u6807\n                    x1, y1, x2, y2 = det_box[2], det_box[3], det_box[4], det_box[5]\n                    w = (x2 - x1) * self.display_size[0] // self.rgb888p_size[0]\n                    h = (y2 - y1) * self.display_size[1] // self.rgb888p_size[1]\n                    x1 = int(x1 * self.display_size[0] // self.rgb888p_size[0])\n                    y1 = int(y1 * self.display_size[1] // self.rgb888p_size[1])\n                    x2 = int(x2 * self.display_size[0] // self.rgb888p_size[0])\n                    y2 = int(y2 * self.display_size[1] // self.rgb888p_size[1])\n                    # \u7ed8\u5236\u77e9\u5f62\u6846\u548c\u7c7b\u522b\u6807\u7b7e\n                    pl.osd_img.draw_rectangle(x1, y1, int(w), int(h), color=self.color[det_box[0]], thickness=2)\n                    pl.osd_img.draw_string_advanced(x1, y1-50, 32," " + self.labels[det_box[0]] + " " + str(round(det_box[1],2)), color=self.color[det_box[0]])\n            else:\n                pl.osd_img.clear()\n\n    # \u83b7\u53d6padding\u53c2\u6570\n    def get_padding_param(self):\n        dst_w = self.model_input_size[0]\n        dst_h = self.model_input_size[1]\n        input_width = self.rgb888p_size[0]\n        input_high = self.rgb888p_size[1]\n        ratio_w = dst_w / input_width\n        ratio_h = dst_h / input_high\n        if ratio_w < ratio_h:\n            ratio = ratio_w\n        else:\n            ratio = ratio_h\n        new_w = int(ratio * input_width)\n        new_h = int(ratio * input_high)\n        dw = (dst_w - new_w) / 2\n        dh = (dst_h - new_h) / 2\n        top = int(round(dh - 0.1))\n        bottom = int(round(dh + 0.1))\n        left = int(round(dw - 0.1))\n        right = int(round(dw - 0.1))\n        return top, bottom, left, right\n\nif __name__ == "__main__":\n    # \u663e\u793a\u6a21\u5f0f\uff0c\u9ed8\u8ba4"hdmi",\u53ef\u4ee5\u9009\u62e9"hdmi"\u548c"lcd"\n    display_mode="hdmi"\n    if display_mode=="hdmi":\n        display_size=[1920,1080]\n    else:\n        display_size=[800,480]\n    # \u8bbe\u7f6e\u6a21\u578b\u8def\u5f84\u548c\u5176\u4ed6\u53c2\u6570\n    kmodel_path = "/sdcard/app/tests/kmodel/yolov5n-falldown.kmodel"\n    confidence_threshold = 0.3\n    nms_threshold = 0.45\n    rgb888p_size = [1920, 1080]\n    labels = ["Fall","NoFall"]  # \u6a21\u578b\u8f93\u51fa\u7c7b\u522b\u540d\u79f0\n    anchors = [10, 13, 16, 30, 33, 23, 30, 61, 62, 45, 59, 119, 116, 90, 156, 198, 373, 326]  # anchor\u8bbe\u7f6e\n\n    # \u521d\u59cb\u5316PipeLine\uff0c\u7528\u4e8e\u56fe\u50cf\u5904\u7406\u6d41\u7a0b\n    pl = PipeLine(rgb888p_size=rgb888p_size, display_size=display_size, display_mode=display_mode)\n    pl.create()\n    # \u521d\u59cb\u5316\u81ea\u5b9a\u4e49\u8dcc\u5012\u68c0\u6d4b\u5b9e\u4f8b\n    fall_det = FallDetectionApp(kmodel_path, model_input_size=[640, 640], labels=labels, anchors=anchors, confidence_threshold=confidence_threshold, nms_threshold=nms_threshold, nms_option=False, strides=[8,16,32], rgb888p_size=rgb888p_size, display_size=display_size, debug_mode=0)\n    fall_det.config_preprocess()\n    try:\n        while True:\n            os.exitpoint()                                  # \u68c0\u67e5\u662f\u5426\u6709\u9000\u51fa\u4fe1\u53f7\n            with ScopedTiming("total",1):\n                img = pl.get_frame()                        # \u83b7\u53d6\u5f53\u524d\u5e27\u6570\u636e\n                res = fall_det.run(img)                     # \u63a8\u7406\u5f53\u524d\u5e27\n                fall_det.draw_result(pl, res)               # \u7ed8\u5236\u7ed3\u679c\u5230PipeLine\u7684osd\u56fe\u50cf\n                pl.show_image()                             # \u663e\u793a\u5f53\u524d\u7684\u7ed8\u5236\u7ed3\u679c\n                gc.collect()                                # \u5783\u573e\u56de\u6536\n    except Exception as e:\n        sys.print_exception(e)                              # \u6253\u5370\u5f02\u5e38\u4fe1\u606f\n    finally:\n        fall_det.deinit()                                   # \u53cd\u521d\u59cb\u5316\n        pl.destroy()                                        # \u9500\u6bc1PipeLine\u5b9e\u4f8b\n'})}),"\n",(0,i.jsx)(n.h3,{id:"211-\u731c\u62f3\u6e38\u620f",children:"2.11. \u731c\u62f3\u6e38\u620f"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:'from libs.PipeLine import PipeLine, ScopedTiming\nfrom libs.AIBase import AIBase\nfrom libs.AI2D import Ai2d\nfrom random import randint\nimport os\nimport ujson\nfrom media.media import *\nfrom time import *\nimport nncase_runtime as nn\nimport ulab.numpy as np\nimport time\nimport image\nimport aicube\nimport random\nimport gc\nimport sys\n\n# \u81ea\u5b9a\u4e49\u624b\u638c\u68c0\u6d4b\u4efb\u52a1\u7c7b\nclass HandDetApp(AIBase):\n    def __init__(self,kmodel_path,labels,model_input_size,anchors,confidence_threshold=0.2,nms_threshold=0.5,nms_option=False, strides=[8,16,32],rgb888p_size=[1920,1080],display_size=[1920,1080],debug_mode=0):\n        super().__init__(kmodel_path,model_input_size,rgb888p_size,debug_mode)\n        # kmodel\u8def\u5f84\n        self.kmodel_path=kmodel_path\n        self.labels=labels\n        # \u68c0\u6d4b\u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.model_input_size=model_input_size\n        # \u7f6e\u4fe1\u5ea6\u9608\u503c\n        self.confidence_threshold=confidence_threshold\n        # nms\u9608\u503c\n        self.nms_threshold=nms_threshold\n        self.anchors=anchors            # \u951a\u6846\uff0c\u68c0\u6d4b\u4efb\u52a1\u4f7f\u7528\n        self.strides = strides          # \u7279\u5f81\u4e0b\u91c7\u6837\u500d\u6570\n        self.nms_option = nms_option    # NMS\u9009\u9879\uff0c\u5982\u679c\u4e3aTrue\u505a\u7c7b\u95f4NMS,\u5982\u679c\u4e3aFalse\u505a\u7c7b\u5185NMS\n        # sensor\u7ed9\u5230AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.rgb888p_size=[ALIGN_UP(rgb888p_size[0],16),rgb888p_size[1]]\n        # \u89c6\u9891\u8f93\u51faVO\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.display_size=[ALIGN_UP(display_size[0],16),display_size[1]]\n        # debug\u6a21\u5f0f\n        self.debug_mode=debug_mode\n        # \u5b9e\u4f8b\u5316Ai2d\uff0c\u7528\u4e8e\u5b9e\u73b0\u6a21\u578b\u9884\u5904\u7406\n        self.ai2d=Ai2d(debug_mode)\n        # \u8bbe\u7f6eAi2d\u7684\u8f93\u5165\u8f93\u51fa\u683c\u5f0f\u548c\u7c7b\u578b\n        self.ai2d.set_ai2d_dtype(nn.ai2d_format.NCHW_FMT,nn.ai2d_format.NCHW_FMT,np.uint8, np.uint8)\n\n    # \u914d\u7f6e\u9884\u5904\u7406\u64cd\u4f5c\uff0c\u8fd9\u91cc\u4f7f\u7528\u4e86pad\u548cresize\uff0cAi2d\u652f\u6301crop/shift/pad/resize/affine\uff0c\u5177\u4f53\u4ee3\u7801\u8bf7\u6253\u5f00/sdcard/app/libs/AI2D.py\u67e5\u770b\n    def config_preprocess(self,input_image_size=None):\n        with ScopedTiming("set preprocess config",self.debug_mode > 0):\n            # \u521d\u59cb\u5316ai2d\u9884\u5904\u7406\u914d\u7f6e\uff0c\u9ed8\u8ba4\u4e3asensor\u7ed9\u5230AI\u7684\u5c3a\u5bf8\uff0c\u53ef\u4ee5\u901a\u8fc7\u8bbe\u7f6einput_image_size\u81ea\u884c\u4fee\u6539\u8f93\u5165\u5c3a\u5bf8\n            ai2d_input_size = input_image_size if input_image_size else self.rgb888p_size\n            # \u8ba1\u7b97padding\u53c2\u6570\u5e76\u5e94\u7528pad\u64cd\u4f5c\uff0c\u4ee5\u786e\u4fdd\u8f93\u5165\u56fe\u50cf\u5c3a\u5bf8\u4e0e\u6a21\u578b\u8f93\u5165\u5c3a\u5bf8\u5339\u914d\n            top, bottom, left, right = self.get_padding_param()\n            self.ai2d.pad([0, 0, 0, 0, top, bottom, left, right], 0, [114, 114, 114])\n            # \u4f7f\u7528\u53cc\u7ebf\u6027\u63d2\u503c\u8fdb\u884cresize\u64cd\u4f5c\uff0c\u8c03\u6574\u56fe\u50cf\u5c3a\u5bf8\u4ee5\u7b26\u5408\u6a21\u578b\u8f93\u5165\u8981\u6c42\n            self.ai2d.resize(nn.interp_method.tf_bilinear, nn.interp_mode.half_pixel)\n            # \u6784\u5efa\u9884\u5904\u7406\u6d41\u7a0b\uff0c\u53c2\u6570\u662fai2d\u9884\u5904\u7406\u7684\u8f93\u5165tensor\u7684shape\u548c\u8f93\u51fatensor\u7684shape\n            self.ai2d.build([1,3,ai2d_input_size[1],ai2d_input_size[0]],[1,3,self.model_input_size[1],self.model_input_size[0]])\n\n    # \u81ea\u5b9a\u4e49\u5f53\u524d\u4efb\u52a1\u7684\u540e\u5904\u7406\uff0cresults\u662f\u6a21\u578b\u8f93\u51faarray\u7684\u5217\u8868\uff0c\u8fd9\u91cc\u4f7f\u7528\u4e86aicube\u5e93\u7684anchorbasedet_post_process\u63a5\u53e3\n    def postprocess(self,results):\n        with ScopedTiming("postprocess",self.debug_mode > 0):\n            dets = aicube.anchorbasedet_post_process(results[0], results[1], results[2], self.model_input_size, self.rgb888p_size, self.strides, len(self.labels), self.confidence_threshold, self.nms_threshold, self.anchors, self.nms_option)\n            # \u8fd4\u56de\u624b\u638c\u68c0\u6d4b\u7ed3\u679c\n            return dets\n\n    # \u8ba1\u7b97padding\u53c2\u6570\uff0c\u786e\u4fdd\u8f93\u5165\u56fe\u50cf\u5c3a\u5bf8\u4e0e\u6a21\u578b\u8f93\u5165\u5c3a\u5bf8\u5339\u914d\n    def get_padding_param(self):\n        # \u6839\u636e\u76ee\u6807\u5bbd\u5ea6\u548c\u9ad8\u5ea6\u8ba1\u7b97\u6bd4\u4f8b\u56e0\u5b50\n        dst_w = self.model_input_size[0]\n        dst_h = self.model_input_size[1]\n        input_width = self.rgb888p_size[0]\n        input_high = self.rgb888p_size[1]\n        ratio_w = dst_w / input_width\n        ratio_h = dst_h / input_high\n        # \u9009\u62e9\u8f83\u5c0f\u7684\u6bd4\u4f8b\u56e0\u5b50\uff0c\u4ee5\u786e\u4fdd\u56fe\u50cf\u5185\u5bb9\u5b8c\u6574\n        if ratio_w < ratio_h:\n            ratio = ratio_w\n        else:\n            ratio = ratio_h\n        # \u8ba1\u7b97\u65b0\u7684\u5bbd\u5ea6\u548c\u9ad8\u5ea6\n        new_w = int(ratio * input_width)\n        new_h = int(ratio * input_high)\n        # \u8ba1\u7b97\u5bbd\u5ea6\u548c\u9ad8\u5ea6\u7684\u5dee\u503c\uff0c\u5e76\u786e\u5b9apadding\u7684\u4f4d\u7f6e\n        dw = (dst_w - new_w) / 2\n        dh = (dst_h - new_h) / 2\n        top = int(round(dh - 0.1))\n        bottom = int(round(dh + 0.1))\n        left = int(round(dw - 0.1))\n        right = int(round(dw + 0.1))\n        return top, bottom, left, right\n\n# \u81ea\u5b9a\u4e49\u624b\u52bf\u5173\u952e\u70b9\u5206\u7c7b\u4efb\u52a1\u7c7b\nclass HandKPClassApp(AIBase):\n    def __init__(self,kmodel_path,model_input_size,rgb888p_size=[1920,1080],display_size=[1920,1080],debug_mode=0):\n        super().__init__(kmodel_path,model_input_size,rgb888p_size,debug_mode)\n        # kmodel\u8def\u5f84\n        self.kmodel_path=kmodel_path\n        # \u68c0\u6d4b\u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.model_input_size=model_input_size\n        # sensor\u7ed9\u5230AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.rgb888p_size=[ALIGN_UP(rgb888p_size[0],16),rgb888p_size[1]]\n        # \u89c6\u9891\u8f93\u51faVO\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.display_size=[ALIGN_UP(display_size[0],16),display_size[1]]\n        # crop\u53c2\u6570\u5217\u8868\n        self.crop_params=[]\n        # debug\u6a21\u5f0f\n        self.debug_mode=debug_mode\n        self.ai2d=Ai2d(debug_mode)\n        self.ai2d.set_ai2d_dtype(nn.ai2d_format.NCHW_FMT,nn.ai2d_format.NCHW_FMT,np.uint8, np.uint8)\n\n    # \u914d\u7f6e\u9884\u5904\u7406\u64cd\u4f5c\uff0c\u8fd9\u91cc\u4f7f\u7528\u4e86crop\u548cresize\uff0cAi2d\u652f\u6301crop/shift/pad/resize/affine\uff0c\u5177\u4f53\u4ee3\u7801\u8bf7\u6253\u5f00/sdcard/app/libs/AI2D.py\u67e5\u770b\n    def config_preprocess(self,det,input_image_size=None):\n        with ScopedTiming("set preprocess config",self.debug_mode > 0):\n            # \u521d\u59cb\u5316ai2d\u9884\u5904\u7406\u914d\u7f6e\uff0c\u9ed8\u8ba4\u4e3asensor\u7ed9\u5230AI\u7684\u5c3a\u5bf8\uff0c\u53ef\u4ee5\u901a\u8fc7\u8bbe\u7f6einput_image_size\u81ea\u884c\u4fee\u6539\u8f93\u5165\u5c3a\u5bf8\n            ai2d_input_size=input_image_size if input_image_size else self.rgb888p_size\n            # \u8ba1\u7b97crop\u53c2\u6570\u5e76\u8bbe\u7f6ecrop\u9884\u5904\u7406\n            self.crop_params = self.get_crop_param(det)\n            self.ai2d.crop(self.crop_params[0],self.crop_params[1],self.crop_params[2],self.crop_params[3])\n            # \u8bbe\u7f6eresize\u9884\u5904\u7406\n            self.ai2d.resize(nn.interp_method.tf_bilinear, nn.interp_mode.half_pixel)\n            # \u6784\u5efa\u9884\u5904\u7406\u6d41\u7a0b\uff0c\u53c2\u6570\u662fai2d\u9884\u5904\u7406\u7684\u8f93\u5165tensor\u7684shape\u548c\u8f93\u51fatensor\u7684shape\n            self.ai2d.build([1,3,ai2d_input_size[1],ai2d_input_size[0]],[1,3,self.model_input_size[1],self.model_input_size[0]])\n\n    # \u81ea\u5b9a\u4e49\u540e\u5904\u7406\uff0cresults\u662f\u6a21\u578b\u8f93\u51faarray\u7684\u5217\u8868\n    def postprocess(self,results):\n        with ScopedTiming("postprocess",self.debug_mode > 0):\n            results=results[0].reshape(results[0].shape[0]*results[0].shape[1])\n            results_show = np.zeros(results.shape,dtype=np.int16)\n            results_show[0::2] = results[0::2] * self.crop_params[3] + self.crop_params[0]\n            results_show[1::2] = results[1::2] * self.crop_params[2] + self.crop_params[1]\n            gesture=self.hk_gesture(results_show)\n            results_show[0::2] = results_show[0::2] * (self.display_size[0] / self.rgb888p_size[0])\n            results_show[1::2] = results_show[1::2] * (self.display_size[1] / self.rgb888p_size[1])\n            return results_show,gesture\n\n    # \u8ba1\u7b97crop\u53c2\u6570\n    def get_crop_param(self,det_box):\n        x1, y1, x2, y2 = det_box[2],det_box[3],det_box[4],det_box[5]\n        w,h= int(x2 - x1),int(y2 - y1)\n        w_det = int(float(x2 - x1) * self.display_size[0] // self.rgb888p_size[0])\n        h_det = int(float(y2 - y1) * self.display_size[1] // self.rgb888p_size[1])\n        x_det = int(x1*self.display_size[0] // self.rgb888p_size[0])\n        y_det = int(y1*self.display_size[1] // self.rgb888p_size[1])\n        length = max(w, h)/2\n        cx = (x1+x2)/2\n        cy = (y1+y2)/2\n        ratio_num = 1.26*length\n        x1_kp = int(max(0,cx-ratio_num))\n        y1_kp = int(max(0,cy-ratio_num))\n        x2_kp = int(min(self.rgb888p_size[0]-1, cx+ratio_num))\n        y2_kp = int(min(self.rgb888p_size[1]-1, cy+ratio_num))\n        w_kp = int(x2_kp - x1_kp + 1)\n        h_kp = int(y2_kp - y1_kp + 1)\n        return [x1_kp, y1_kp, w_kp, h_kp]\n\n    # \u6c42\u4e24\u4e2avector\u4e4b\u95f4\u7684\u5939\u89d2\n    def hk_vector_2d_angle(self,v1,v2):\n        with ScopedTiming("hk_vector_2d_angle",self.debug_mode > 0):\n            v1_x,v1_y,v2_x,v2_y = v1[0],v1[1],v2[0],v2[1]\n            v1_norm = np.sqrt(v1_x * v1_x+ v1_y * v1_y)\n            v2_norm = np.sqrt(v2_x * v2_x + v2_y * v2_y)\n            dot_product = v1_x * v2_x + v1_y * v2_y\n            cos_angle = dot_product/(v1_norm*v2_norm)\n            angle = np.acos(cos_angle)*180/np.pi\n            return angle\n\n    # \u6839\u636e\u624b\u638c\u5173\u952e\u70b9\u68c0\u6d4b\u7ed3\u679c\u5224\u65ad\u624b\u52bf\u7c7b\u522b\n    def hk_gesture(self,results):\n        with ScopedTiming("hk_gesture",self.debug_mode > 0):\n            angle_list = []\n            for i in range(5):\n                angle = self.hk_vector_2d_angle([(results[0]-results[i*8+4]), (results[1]-results[i*8+5])],[(results[i*8+6]-results[i*8+8]),(results[i*8+7]-results[i*8+9])])\n                angle_list.append(angle)\n            thr_angle,thr_angle_thumb,thr_angle_s,gesture_str = 65.,53.,49.,None\n            if 65535. not in angle_list:\n                if (angle_list[0]>thr_angle_thumb)  and (angle_list[1]>thr_angle) and (angle_list[2]>thr_angle) and (angle_list[3]>thr_angle) and (angle_list[4]>thr_angle):\n                    gesture_str = "fist"\n                elif (angle_list[0]<thr_angle_s)  and (angle_list[1]<thr_angle_s) and (angle_list[2]<thr_angle_s) and (angle_list[3]<thr_angle_s) and (angle_list[4]<thr_angle_s):\n                    gesture_str = "five"\n                elif (angle_list[0]<thr_angle_s)  and (angle_list[1]<thr_angle_s) and (angle_list[2]>thr_angle) and (angle_list[3]>thr_angle) and (angle_list[4]>thr_angle):\n                    gesture_str = "gun"\n                elif (angle_list[0]<thr_angle_s)  and (angle_list[1]<thr_angle_s) and (angle_list[2]>thr_angle) and (angle_list[3]>thr_angle) and (angle_list[4]<thr_angle_s):\n                    gesture_str = "love"\n                elif (angle_list[0]>5)  and (angle_list[1]<thr_angle_s) and (angle_list[2]>thr_angle) and (angle_list[3]>thr_angle) and (angle_list[4]>thr_angle):\n                    gesture_str = "one"\n                elif (angle_list[0]<thr_angle_s)  and (angle_list[1]>thr_angle) and (angle_list[2]>thr_angle) and (angle_list[3]>thr_angle) and (angle_list[4]<thr_angle_s):\n                    gesture_str = "six"\n                elif (angle_list[0]>thr_angle_thumb)  and (angle_list[1]<thr_angle_s) and (angle_list[2]<thr_angle_s) and (angle_list[3]<thr_angle_s) and (angle_list[4]>thr_angle):\n                    gesture_str = "three"\n                elif (angle_list[0]<thr_angle_s)  and (angle_list[1]>thr_angle) and (angle_list[2]>thr_angle) and (angle_list[3]>thr_angle) and (angle_list[4]>thr_angle):\n                    gesture_str = "thumbUp"\n                elif (angle_list[0]>thr_angle_thumb)  and (angle_list[1]<thr_angle_s) and (angle_list[2]<thr_angle_s) and (angle_list[3]>thr_angle) and (angle_list[4]>thr_angle):\n                    gesture_str = "yeah"\n            return gesture_str\n\n# \u731c\u62f3\u6e38\u620f\u4efb\u52a1\u7c7b\nclass FingerGuess:\n    def __init__(self,hand_det_kmodel,hand_kp_kmodel,det_input_size,kp_input_size,labels,anchors,confidence_threshold=0.25,nms_threshold=0.3,nms_option=False,strides=[8,16,32],guess_mode=3,rgb888p_size=[1280,720],display_size=[1920,1080],debug_mode=0):\n        # \u624b\u638c\u68c0\u6d4b\u6a21\u578b\u8def\u5f84\n        self.hand_det_kmodel=hand_det_kmodel\n        # \u624b\u638c\u5173\u952e\u70b9\u6a21\u578b\u8def\u5f84\n        self.hand_kp_kmodel=hand_kp_kmodel\n        # \u624b\u638c\u68c0\u6d4b\u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.det_input_size=det_input_size\n        # \u624b\u638c\u5173\u952e\u70b9\u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.kp_input_size=kp_input_size\n        self.labels=labels\n        # anchors\n        self.anchors=anchors\n        # \u7f6e\u4fe1\u5ea6\u9608\u503c\n        self.confidence_threshold=confidence_threshold\n        # nms\u9608\u503c\n        self.nms_threshold=nms_threshold\n        # nms\u9009\u9879\n        self.nms_option=nms_option\n        # \u7279\u5f81\u56fe\u9488\u5bf9\u8f93\u5165\u7684\u4e0b\u91c7\u6837\u500d\u6570\n        self.strides=strides\n        # sensor\u7ed9\u5230AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.rgb888p_size=[ALIGN_UP(rgb888p_size[0],16),rgb888p_size[1]]\n        # \u89c6\u9891\u8f93\u51faVO\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.display_size=[ALIGN_UP(display_size[0],16),display_size[1]]\n        # debug_mode\u6a21\u5f0f\n        self.debug_mode=debug_mode\n        self.guess_mode=guess_mode\n        # \u77f3\u5934\u526a\u5200\u5e03\u7684\u8d34\u56fearray\n        self.five_image = self.read_file("/sdcard/app/tests/utils/five.bin")\n        self.fist_image = self.read_file("/sdcard/app/tests/utils/fist.bin")\n        self.shear_image = self.read_file("/sdcard/app/tests/utils/shear.bin")\n        self.counts_guess = -1                                                               # \u731c\u62f3\u6b21\u6570 \u8ba1\u6570\n        self.player_win = 0                                                                  # \u73a9\u5bb6 \u8d62\u6b21\u8ba1\u6570\n        self.k230_win = 0                                                                    # k230 \u8d62\u6b21\u8ba1\u6570\n        self.sleep_end = False                                                               # \u662f\u5426 \u505c\u987f\n        self.set_stop_id = True                                                              # \u662f\u5426 \u6682\u505c\u731c\u62f3\n        self.LIBRARY = ["fist","yeah","five"]                                                # \u731c\u62f3 \u77f3\u5934\u526a\u5200\u5e03 \u4e09\u79cd\u65b9\u6848\u7684dict\n        self.hand_det=HandDetApp(self.hand_det_kmodel,self.labels,model_input_size=self.det_input_size,anchors=self.anchors,confidence_threshold=self.confidence_threshold,nms_threshold=self.nms_threshold,nms_option=self.nms_option,strides=self.strides,rgb888p_size=self.rgb888p_size,display_size=self.display_size,debug_mode=0)\n        self.hand_kp=HandKPClassApp(self.hand_kp_kmodel,model_input_size=self.kp_input_size,rgb888p_size=self.rgb888p_size,display_size=self.display_size)\n        self.hand_det.config_preprocess()\n\n    # run\u51fd\u6570\n    def run(self,input_np):\n        # \u5148\u8fdb\u884c\u624b\u638c\u68c0\u6d4b\n        det_boxes=self.hand_det.run(input_np)\n        boxes=[]\n        gesture_res=[]\n        for det_box in det_boxes:\n            # \u5bf9\u68c0\u6d4b\u7684\u624b\u505a\u624b\u52bf\u8bc6\u522b\n            x1, y1, x2, y2 = det_box[2],det_box[3],det_box[4],det_box[5]\n            w,h= int(x2 - x1),int(y2 - y1)\n            if (h<(0.1*self.rgb888p_size[1])):\n                continue\n            if (w<(0.25*self.rgb888p_size[0]) and ((x1<(0.03*self.rgb888p_size[0])) or (x2>(0.97*self.rgb888p_size[0])))):\n                continue\n            if (w<(0.15*self.rgb888p_size[0]) and ((x1<(0.01*self.rgb888p_size[0])) or (x2>(0.99*self.rgb888p_size[0])))):\n                continue\n            self.hand_kp.config_preprocess(det_box)\n            results_show,gesture=self.hand_kp.run(input_np)\n            boxes.append(det_box)\n            gesture_res.append(gesture)\n        return boxes,gesture_res\n\n    # \u7ed8\u5236\u6548\u679c\n    def draw_result(self,pl,dets,gesture_res):\n        pl.osd_img.clear()\n        # \u624b\u638c\u7684\u624b\u52bf\u5206\u7c7b\u5f97\u5230\u7528\u6237\u7684\u51fa\u62f3\uff0c\u6839\u636e\u4e0d\u540c\u6a21\u5f0f\u7ed9\u51fa\u5f00\u53d1\u677f\u7684\u51fa\u62f3\uff0c\u5e76\u5c06\u5bf9\u5e94\u7684\u8d34\u56fe\u653e\u5230\u5c4f\u5e55\u4e0a\u663e\u793a\n        if (len(dets) >= 2):\n            pl.osd_img.draw_string_advanced( self.display_size[0]//2-50,self.display_size[1]//2-50,60, "\u8bf7\u4fdd\u8bc1\u53ea\u6709\u4e00\u53ea\u624b\u5165\u955c\uff01", color=(255,255,0,0))\n        elif (self.guess_mode == 0):\n            draw_img_np = np.zeros((self.display_size[1],self.display_size[0],4),dtype=np.uint8)\n            draw_img = image.Image(self.display_size[0], self.display_size[1], image.ARGB8888, alloc=image.ALLOC_REF,data = draw_img_np)\n            if (gesture_res[0] == "fist"):\n                draw_img_np[:400,:400,:] = self.shear_image\n            elif (gesture_res[0] == "five"):\n                draw_img_np[:400,:400,:] = self.fist_image\n            elif (gesture_res[0] == "yeah"):\n                draw_img_np[:400,:400,:] = self.five_image\n            pl.osd_img.copy_from(draw_img)\n        elif (self.guess_mode == 1):\n            draw_img_np = np.zeros((self.display_size[1],self.display_size[0],4),dtype=np.uint8)\n            draw_img = image.Image(self.display_size[0], self.display_size[1], image.ARGB8888, alloc=image.ALLOC_REF,data = draw_img_np)\n            if (gesture_res[0] == "fist"):\n                draw_img_np[:400,:400,:] = self.five_image\n            elif (gesture_res[0] == "five"):\n                draw_img_np[:400,:400,:] = self.shear_image\n            elif (gesture_res[0] == "yeah"):\n                draw_img_np[:400,:400,:] = self.fist_image\n            pl.osd_img.copy_from(draw_img)\n        else:\n            draw_img_np = np.zeros((self.display_size[1],self.display_size[0],4),dtype=np.uint8)\n            draw_img = image.Image(self.display_size[0], self.display_size[1], image.ARGB8888, alloc=image.ALLOC_REF,data = draw_img_np)\n            if (self.sleep_end):\n                time.sleep_ms(2000)\n                self.sleep_end = False\n            if (len(dets) == 0):\n                self.set_stop_id = True\n                return\n            if (self.counts_guess == -1 and gesture_res[0] != "fist" and gesture_res[0] != "yeah" and gesture_res[0] != "five"):\n                draw_img.draw_string_advanced( self.display_size[0]//2-50,self.display_size[1]//2-50,60, "\u6e38\u620f\u5f00\u59cb", color=(255,255,0,0))\n                draw_img.draw_string_advanced( self.display_size[0]//2-50,self.display_size[1]//2-50,60, "\u7b2c\u4e00\u56de\u5408", color=(255,255,0,0))\n            elif (self.counts_guess == self.guess_mode):\n                draw_img.clear()\n                if (self.k230_win > self.player_win):\n                    draw_img.draw_string_advanced( self.display_size[0]//2-50,self.display_size[1]//2-50,60, "\u4f60\u8f93\u4e86\uff01", color=(255,255,0,0))\n                elif (self.k230_win < self.player_win):\n                    draw_img.draw_string_advanced( self.display_size[0]//2-50,self.display_size[1]//2-50,60, "\u4f60\u8d62\u4e86\uff01", color=(255,255,0,0))\n                else:\n                    draw_img.draw_string_advanced( self.display_size[0]//2-50,self.display_size[1]//2-50,60, "\u5e73\u5c40", color=(255,255,0,0))\n                self.counts_guess = -1\n                self.player_win = 0\n                self.k230_win = 0\n                self.sleep_end = True\n            else:\n                if (self.set_stop_id):\n                    if (self.counts_guess == -1 and (gesture_res[0] == "fist" or gesture_res[0] == "yeah" or gesture_res[0] == "five")):\n                        self.counts_guess = 0\n                    if (self.counts_guess != -1 and (gesture_res[0] == "fist" or gesture_res[0] == "yeah" or gesture_res[0] == "five")):\n                        k230_guess = randint(1,10000) % 3\n                        if (gesture_res[0] == "fist" and self.LIBRARY[k230_guess] == "yeah"):\n                            self.player_win += 1\n                        elif (gesture_res[0] == "fist" and self.LIBRARY[k230_guess] == "five"):\n                            self.k230_win += 1\n                        if (gesture_res[0] == "yeah" and self.LIBRARY[k230_guess] == "fist"):\n                            self.k230_win += 1\n                        elif (gesture_res[0] == "yeah" and self.LIBRARY[k230_guess] == "five"):\n                            self.player_win += 1\n                        if (gesture_res[0] == "five" and self.LIBRARY[k230_guess] == "fist"):\n                            self.player_win += 1\n                        elif (gesture_res[0] == "five" and self.LIBRARY[k230_guess] == "yeah"):\n                            self.k230_win += 1\n                        if (self.LIBRARY[k230_guess] == "fist"):\n                            draw_img_np[:400,:400,:] = self.fist_image\n                        elif (self.LIBRARY[k230_guess] == "five"):\n                            draw_img_np[:400,:400,:] = self.five_image\n                        elif (self.LIBRARY[k230_guess] == "yeah"):\n                            draw_img_np[:400,:400,:] = self.shear_image\n                        self.counts_guess += 1\n                        draw_img.draw_string_advanced(self.display_size[0]//2-50,self.display_size[1]//2-50,60,"\u7b2c" + str(self.counts_guess) + "\u56de\u5408", color=(255,255,0,0))\n                        self.set_stop_id = False\n                        self.sleep_end = True\n                    else:\n                        draw_img.draw_string_advanced(self.display_size[0]//2-50,self.display_size[1]//2-50,60,"\u7b2c" + str(self.counts_guess+1) + "\u56de\u5408", color=(255,255,0,0))\n            pl.osd_img.copy_from(draw_img)\n\n    # \u8bfb\u53d6\u77f3\u5934\u526a\u5200\u5e03\u7684bin\u6587\u4ef6\u65b9\u6cd5\n    def read_file(self,file_name):\n        image_arr = np.fromfile(file_name,dtype=np.uint8)\n        image_arr = image_arr.reshape((400,400,4))\n        return image_arr\n\n\nif __name__=="__main__":\n    # \u663e\u793a\u6a21\u5f0f\uff0c\u9ed8\u8ba4"hdmi",\u53ef\u4ee5\u9009\u62e9"hdmi"\u548c"lcd"\n    display_mode="hdmi"\n    if display_mode=="hdmi":\n        display_size=[1920,1080]\n    else:\n        display_size=[800,480]\n    # \u624b\u638c\u68c0\u6d4b\u6a21\u578b\u8def\u5f84\n    hand_det_kmodel_path="/sdcard/app/tests/kmodel/hand_det.kmodel"\n    # \u624b\u638c\u5173\u952e\u70b9\u6a21\u578b\u8def\u5f84\n    hand_kp_kmodel_path="/sdcard/app/tests/kmodel/handkp_det.kmodel"\n    # \u5176\u5b83\u53c2\u6570\n    anchors_path="/sdcard/app/tests/utils/prior_data_320.bin"\n    rgb888p_size=[1920,1080]\n    hand_det_input_size=[512,512]\n    hand_kp_input_size=[256,256]\n    confidence_threshold=0.2\n    nms_threshold=0.5\n    labels=["hand"]\n    anchors = [26,27, 53,52, 75,71, 80,99, 106,82, 99,134, 140,113, 161,172, 245,276]\n    # \u731c\u62f3\u6a21\u5f0f  0 \u73a9\u5bb6\u7a33\u8d62 \uff0c 1 \u73a9\u5bb6\u5fc5\u8f93 \uff0c n > 2 \u591a\u5c40\u591a\u80dc\n    guess_mode = 3\n\n    # \u521d\u59cb\u5316PipeLine\uff0c\u53ea\u5173\u6ce8\u4f20\u7ed9AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u663e\u793a\u7684\u5206\u8fa8\u7387\n    pl=PipeLine(rgb888p_size=rgb888p_size,display_size=display_size,display_mode=display_mode)\n    pl.create()\n    hkc=FingerGuess(hand_det_kmodel_path,hand_kp_kmodel_path,det_input_size=hand_det_input_size,kp_input_size=hand_kp_input_size,labels=labels,anchors=anchors,confidence_threshold=confidence_threshold,nms_threshold=nms_threshold,nms_option=False,strides=[8,16,32],guess_mode=guess_mode,rgb888p_size=rgb888p_size,display_size=display_size)\n    try:\n        while True:\n            os.exitpoint()\n            with ScopedTiming("total",1):\n                img=pl.get_frame()                          # \u83b7\u53d6\u5f53\u524d\u5e27\n                det_boxes,gesture_res=hkc.run(img)          # \u63a8\u7406\u5f53\u524d\u5e27\n                hkc.draw_result(pl,det_boxes,gesture_res)   # \u7ed8\u5236\u63a8\u7406\u7ed3\u679c\n                pl.show_image()                             # \u5c55\u793a\u63a8\u7406\u7ed3\u679c\n                gc.collect()\n    except Exception as e:\n        sys.print_exception(e)\n    finally:\n        hkc.hand_det.deinit()\n        hkc.hand_kp.deinit()\n        pl.destroy()\n'})}),"\n",(0,i.jsx)(n.h3,{id:"212-\u624b\u638c\u68c0\u6d4b",children:"2.12. \u624b\u638c\u68c0\u6d4b"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:'from libs.PipeLine import PipeLine, ScopedTiming\nfrom libs.AIBase import AIBase\nfrom libs.AI2D import Ai2d\nimport os\nimport ujson\nfrom media.media import *\nfrom time import *\nimport nncase_runtime as nn\nimport ulab.numpy as np\nimport time\nimport utime\nimport image\nimport random\nimport gc\nimport sys\nimport aicube\n\n# \u81ea\u5b9a\u4e49\u624b\u638c\u68c0\u6d4b\u7c7b\uff0c\u7ee7\u627f\u81eaAIBase\u57fa\u7c7b\nclass HandDetectionApp(AIBase):\n    def __init__(self, kmodel_path, model_input_size, labels, anchors, confidence_threshold=0.2, nms_threshold=0.5, nms_option=False, strides=[8,16,32], rgb888p_size=[224,224], display_size=[1920,1080], debug_mode=0):\n        super().__init__(kmodel_path, model_input_size, rgb888p_size, debug_mode)  # \u8c03\u7528\u57fa\u7c7b\u7684\u6784\u9020\u51fd\u6570\uff0c\u521d\u59cb\u5316\u6a21\u578b\u6587\u4ef6\u8def\u5f84\u3001\u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\u3001RGB\u56fe\u50cf\u5206\u8fa8\u7387\u548c\u8c03\u8bd5\u6a21\u5f0f\n        self.kmodel_path = kmodel_path  # \u6a21\u578b\u6587\u4ef6\u8def\u5f84\n        self.model_input_size = model_input_size  # \u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.labels = labels  # \u6a21\u578b\u8f93\u51fa\u7684\u7c7b\u522b\u6807\u7b7e\u5217\u8868\n        self.anchors = anchors  # \u7528\u4e8e\u76ee\u6807\u68c0\u6d4b\u7684\u951a\u70b9\u5c3a\u5bf8\u5217\u8868\n        self.strides = strides  # \u7279\u5f81\u4e0b\u91c7\u6837\u500d\u6570\n        self.confidence_threshold = confidence_threshold  # \u7f6e\u4fe1\u5ea6\u9608\u503c\uff0c\u7528\u4e8e\u8fc7\u6ee4\u4f4e\u7f6e\u4fe1\u5ea6\u7684\u68c0\u6d4b\u7ed3\u679c\n        self.nms_threshold = nms_threshold  # NMS\uff08\u975e\u6781\u5927\u503c\u6291\u5236\uff09\u9608\u503c\uff0c\u7528\u4e8e\u53bb\u9664\u91cd\u53e0\u7684\u68c0\u6d4b\u6846\n        self.nms_option = nms_option  # NMS\u9009\u9879\uff0c\u53ef\u80fd\u5f71\u54cdNMS\u7684\u5177\u4f53\u5b9e\u73b0\n        self.rgb888p_size = [ALIGN_UP(rgb888p_size[0], 16), rgb888p_size[1]]  # sensor\u7ed9\u5230AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u5bf9\u9f50\u5230\u6700\u8fd1\u768416\u7684\u500d\u6570\n        self.display_size = [ALIGN_UP(display_size[0], 16), display_size[1]]  # \u663e\u793a\u5206\u8fa8\u7387\uff0c\u5bf9\u9f50\u5230\u6700\u8fd1\u768416\u7684\u500d\u6570\n        self.debug_mode = debug_mode  # \u8c03\u8bd5\u6a21\u5f0f\uff0c\u7528\u4e8e\u8f93\u51fa\u8c03\u8bd5\u4fe1\u606f\n        self.ai2d = Ai2d(debug_mode)  # \u5b9e\u4f8b\u5316Ai2d\u7c7b\uff0c\u7528\u4e8e\u5b9e\u73b0\u6a21\u578b\u9884\u5904\u7406\n        # \u8bbe\u7f6eAi2d\u7684\u8f93\u5165\u8f93\u51fa\u683c\u5f0f\u548c\u7c7b\u578b\uff0c\u8fd9\u91cc\u4f7f\u7528NCHW\u683c\u5f0f\uff0c\u6570\u636e\u7c7b\u578b\u4e3auint8\n        self.ai2d.set_ai2d_dtype(nn.ai2d_format.NCHW_FMT, nn.ai2d_format.NCHW_FMT, np.uint8, np.uint8)\n\n    # \u914d\u7f6e\u9884\u5904\u7406\u64cd\u4f5c\uff0c\u8fd9\u91cc\u4f7f\u7528\u4e86pad\u548cresize\n    def config_preprocess(self, input_image_size=None):\n        with ScopedTiming("set preprocess config", self.debug_mode > 0):  # \u4f7f\u7528ScopedTiming\u88c5\u9970\u5668\u6765\u6d4b\u91cf\u9884\u5904\u7406\u914d\u7f6e\u7684\u65f6\u95f4\n            # \u521d\u59cb\u5316ai2d\u9884\u5904\u7406\u914d\u7f6e\uff0c\u9ed8\u8ba4\u4e3asensor\u7ed9\u5230AI\u7684\u5c3a\u5bf8\uff0c\u53ef\u4ee5\u901a\u8fc7\u8bbe\u7f6einput_image_size\u81ea\u884c\u4fee\u6539\u8f93\u5165\u5c3a\u5bf8\n            ai2d_input_size = input_image_size if input_image_size else self.rgb888p_size\n            # \u8ba1\u7b97padding\u53c2\u6570\u5e76\u5e94\u7528pad\u64cd\u4f5c\uff0c\u4ee5\u786e\u4fdd\u8f93\u5165\u56fe\u50cf\u5c3a\u5bf8\u4e0e\u6a21\u578b\u8f93\u5165\u5c3a\u5bf8\u5339\u914d\n            top, bottom, left, right = self.get_padding_param()\n            self.ai2d.pad([0, 0, 0, 0, top, bottom, left, right], 0, [0, 0, 0])\n            # \u4f7f\u7528\u53cc\u7ebf\u6027\u63d2\u503c\u8fdb\u884cresize\u64cd\u4f5c\uff0c\u8c03\u6574\u56fe\u50cf\u5c3a\u5bf8\u4ee5\u7b26\u5408\u6a21\u578b\u8f93\u5165\u8981\u6c42\n            self.ai2d.resize(nn.interp_method.tf_bilinear, nn.interp_mode.half_pixel)\n            # \u6784\u5efa\u9884\u5904\u7406\u6d41\u7a0b\n            self.ai2d.build([1,3,ai2d_input_size[1],ai2d_input_size[0]],[1,3,self.model_input_size[1],self.model_input_size[0]])\n\n    # \u81ea\u5b9a\u4e49\u5f53\u524d\u4efb\u52a1\u7684\u540e\u5904\u7406\uff0c\u7528\u4e8e\u5904\u7406\u6a21\u578b\u8f93\u51fa\u7ed3\u679c\n    def postprocess(self, results):\n        with ScopedTiming("postprocess", self.debug_mode > 0):  # \u4f7f\u7528ScopedTiming\u88c5\u9970\u5668\u6765\u6d4b\u91cf\u540e\u5904\u7406\u7684\u65f6\u95f4\n            # \u4f7f\u7528aicube\u5e93\u7684\u51fd\u6570\u8fdb\u884c\u540e\u5904\u7406\uff0c\u5f97\u5230\u6700\u7ec8\u7684\u68c0\u6d4b\u7ed3\u679c\n            dets = aicube.anchorbasedet_post_process(results[0], results[1], results[2], self.model_input_size, self.rgb888p_size, self.strides, len(self.labels), self.confidence_threshold, self.nms_threshold, self.anchors, self.nms_option)\n            return dets\n\n    # \u7ed8\u5236\u68c0\u6d4b\u7ed3\u679c\u5230\u5c4f\u5e55\u4e0a\n    def draw_result(self, pl, dets):\n        with ScopedTiming("display_draw", self.debug_mode > 0):  # \u4f7f\u7528ScopedTiming\u88c5\u9970\u5668\u6765\u6d4b\u91cf\u7ed8\u5236\u7ed3\u679c\u7684\u65f6\u95f4\n            if dets:  # \u5982\u679c\u5b58\u5728\u68c0\u6d4b\u7ed3\u679c\n                pl.osd_img.clear()  # \u6e05\u9664\u5c4f\u5e55\u4e0a\u7684\u65e7\u5185\u5bb9\n                for det_box in dets:  # \u904d\u5386\u6bcf\u4e2a\u68c0\u6d4b\u6846\n                    # \u6839\u636e\u6a21\u578b\u8f93\u51fa\u8ba1\u7b97\u68c0\u6d4b\u6846\u7684\u50cf\u7d20\u5750\u6807\uff0c\u5e76\u8c03\u6574\u5927\u5c0f\u4ee5\u9002\u5e94\u663e\u793a\u5206\u8fa8\u7387\n                    x1, y1, x2, y2 = det_box[2], det_box[3], det_box[4], det_box[5]\n                    w = float(x2 - x1) * self.display_size[0] // self.rgb888p_size[0]\n                    h = float(y2 - y1) * self.display_size[1] // self.rgb888p_size[1]\n                    x1 = int(x1 * self.display_size[0] // self.rgb888p_size[0])\n                    y1 = int(y1 * self.display_size[1] // self.rgb888p_size[1])\n                    x2 = int(x2 * self.display_size[0] // self.rgb888p_size[0])\n                    y2 = int(y2 * self.display_size[1] // self.rgb888p_size[1])\n                    # \u8fc7\u6ee4\u6389\u592a\u5c0f\u6216\u8005\u4f4d\u7f6e\u4e0d\u5408\u7406\u7684\u68c0\u6d4b\u6846\n                    if (h < (0.1 * self.display_size[0])):\n                        continue\n                    if (w < (0.25 * self.display_size[0]) and ((x1 < (0.03 * self.display_size[0])) or (x2 > (0.97 * self.display_size[0])))):\n                        continue\n                    if (w < (0.15 * self.display_size[0]) and ((x1 < (0.01 * self.display_size[0])) or (x2 > (0.99 * self.display_size[0])))):\n                        continue\n                    # \u7ed8\u5236\u77e9\u5f62\u6846\u548c\u7c7b\u522b\u6807\u7b7e\n                    pl.osd_img.draw_rectangle(x1, y1, int(w), int(h), color=(255, 0, 255, 0), thickness=2)\n                    pl.osd_img.draw_string_advanced(x1, y1-50,32, " " + self.labels[det_box[0]] + " " + str(round(det_box[1], 2)), color=(255, 0, 255, 0))\n            else:\n                pl.osd_img.clear()  # \u5982\u679c\u6ca1\u6709\u68c0\u6d4b\u7ed3\u679c\uff0c\u6e05\u7a7a\u5c4f\u5e55\n\n    # \u8ba1\u7b97padding\u53c2\u6570\uff0c\u786e\u4fdd\u8f93\u5165\u56fe\u50cf\u5c3a\u5bf8\u4e0e\u6a21\u578b\u8f93\u5165\u5c3a\u5bf8\u5339\u914d\n    def get_padding_param(self):\n        # \u6839\u636e\u76ee\u6807\u5bbd\u5ea6\u548c\u9ad8\u5ea6\u8ba1\u7b97\u6bd4\u4f8b\u56e0\u5b50\n        dst_w = self.model_input_size[0]\n        dst_h = self.model_input_size[1]\n        input_width = self.rgb888p_size[0]\n        input_high = self.rgb888p_size[1]\n        ratio_w = dst_w / input_width\n        ratio_h = dst_h / input_high\n        # \u9009\u62e9\u8f83\u5c0f\u7684\u6bd4\u4f8b\u56e0\u5b50\uff0c\u4ee5\u786e\u4fdd\u56fe\u50cf\u5185\u5bb9\u5b8c\u6574\n        if ratio_w < ratio_h:\n            ratio = ratio_w\n        else:\n            ratio = ratio_h\n        # \u8ba1\u7b97\u65b0\u7684\u5bbd\u5ea6\u548c\u9ad8\u5ea6\n        new_w = int(ratio * input_width)\n        new_h = int(ratio * input_high)\n        # \u8ba1\u7b97\u5bbd\u5ea6\u548c\u9ad8\u5ea6\u7684\u5dee\u503c\uff0c\u5e76\u786e\u5b9apadding\u7684\u4f4d\u7f6e\n        dw = (dst_w - new_w) / 2\n        dh = (dst_h - new_h) / 2\n        top = int(round(dh - 0.1))\n        bottom = int(round(dh + 0.1))\n        left = int(round(dw - 0.1))\n        right = int(round(dw + 0.1))\n        return top, bottom, left, right\n\nif __name__=="__main__":\n    # \u663e\u793a\u6a21\u5f0f\uff0c\u9ed8\u8ba4"hdmi",\u53ef\u4ee5\u9009\u62e9"hdmi"\u548c"lcd"\n    display_mode="hdmi"\n    if display_mode=="hdmi":\n        display_size=[1920,1080]\n    else:\n        display_size=[800,480]\n    # \u6a21\u578b\u8def\u5f84\n    kmodel_path="/sdcard/app/tests/kmodel/hand_det.kmodel"\n    # \u5176\u5b83\u53c2\u6570\u8bbe\u7f6e\n    confidence_threshold = 0.2\n    nms_threshold = 0.5\n    rgb888p_size=[1920,1080]\n    labels = ["hand"]\n    anchors = [26,27, 53,52, 75,71, 80,99, 106,82, 99,134, 140,113, 161,172, 245,276]   #anchor\u8bbe\u7f6e\n\n    # \u521d\u59cb\u5316PipeLine\n    pl=PipeLine(rgb888p_size=rgb888p_size,display_size=display_size,display_mode=display_mode)\n    pl.create()\n    # \u521d\u59cb\u5316\u81ea\u5b9a\u4e49\u624b\u638c\u68c0\u6d4b\u5b9e\u4f8b\n    hand_det=HandDetectionApp(kmodel_path,model_input_size=[512,512],labels=labels,anchors=anchors,confidence_threshold=confidence_threshold,nms_threshold=nms_threshold,nms_option=False,strides=[8,16,32],rgb888p_size=rgb888p_size,display_size=display_size,debug_mode=0)\n    hand_det.config_preprocess()\n    try:\n        while True:\n            os.exitpoint()                              # \u68c0\u67e5\u662f\u5426\u6709\u9000\u51fa\u4fe1\u53f7\n            with ScopedTiming("total",1):\n                img=pl.get_frame()                      # \u83b7\u53d6\u5f53\u524d\u5e27\u6570\u636e\n                res=hand_det.run(img)                   # \u63a8\u7406\u5f53\u524d\u5e27\n                hand_det.draw_result(pl,res)            # \u7ed8\u5236\u7ed3\u679c\u5230PipeLine\u7684osd\u56fe\u50cf\n                pl.show_image()                         # \u663e\u793a\u5f53\u524d\u7684\u7ed8\u5236\u7ed3\u679c\n                gc.collect()                            # \u5783\u573e\u56de\u6536\n    except Exception as e:\n        sys.print_exception(e)\n    finally:\n        hand_det.deinit()                               # \u53cd\u521d\u59cb\u5316\n        pl.destroy()                                    # \u9500\u6bc1PipeLine\u5b9e\u4f8b\n'})}),"\n",(0,i.jsx)(n.h3,{id:"213-\u624b\u638c\u5173\u952e\u70b9\u5206\u7c7b",children:"2.13. \u624b\u638c\u5173\u952e\u70b9\u5206\u7c7b"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:'from libs.PipeLine import PipeLine, ScopedTiming\nfrom libs.AIBase import AIBase\nfrom libs.AI2D import Ai2d\nimport os\nimport ujson\nfrom media.media import *\nfrom time import *\nimport nncase_runtime as nn\nimport ulab.numpy as np\nimport time\nimport image\nimport aicube\nimport random\nimport gc\nimport sys\n\n# \u81ea\u5b9a\u4e49\u624b\u638c\u68c0\u6d4b\u4efb\u52a1\u7c7b\nclass HandDetApp(AIBase):\n    def __init__(self,kmodel_path,labels,model_input_size,anchors,confidence_threshold=0.2,nms_threshold=0.5,nms_option=False, strides=[8,16,32],rgb888p_size=[1920,1080],display_size=[1920,1080],debug_mode=0):\n        super().__init__(kmodel_path,model_input_size,rgb888p_size,debug_mode)\n        # kmodel\u8def\u5f84\n        self.kmodel_path=kmodel_path\n        self.labels=labels\n        # \u68c0\u6d4b\u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.model_input_size=model_input_size\n        # \u7f6e\u4fe1\u5ea6\u9608\u503c\n        self.confidence_threshold=confidence_threshold\n        # nms\u9608\u503c\n        self.nms_threshold=nms_threshold\n        # \u951a\u6846,\u76ee\u6807\u68c0\u6d4b\u4efb\u52a1\u4f7f\u7528\n        self.anchors=anchors\n        # \u7279\u5f81\u4e0b\u91c7\u6837\u500d\u6570\n        self.strides = strides\n        # NMS\u9009\u9879\uff0c\u5982\u679c\u4e3aTrue\u505a\u7c7b\u95f4NMS,\u5982\u679c\u4e3aFalse\u505a\u7c7b\u5185NMS\n        self.nms_option = nms_option\n        # sensor\u7ed9\u5230AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.rgb888p_size=[ALIGN_UP(rgb888p_size[0],16),rgb888p_size[1]]\n        # \u89c6\u9891\u8f93\u51faVO\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.display_size=[ALIGN_UP(display_size[0],16),display_size[1]]\n        # debug\u6a21\u5f0f\n        self.debug_mode=debug_mode\n        # Ai2d\u5b9e\u4f8b\u7528\u4e8e\u5b9e\u73b0\u9884\u5904\u7406\n        self.ai2d=Ai2d(debug_mode)\n        # \u8bbe\u7f6eai2d\u7684\u8f93\u5165\u8f93\u51fa\u7684\u683c\u5f0f\u548c\u6570\u636e\u7c7b\u578b\n        self.ai2d.set_ai2d_dtype(nn.ai2d_format.NCHW_FMT,nn.ai2d_format.NCHW_FMT,np.uint8, np.uint8)\n\n    # \u914d\u7f6e\u9884\u5904\u7406\u64cd\u4f5c\uff0c\u8fd9\u91cc\u4f7f\u7528\u4e86pad\u548cresize\uff0cAi2d\u652f\u6301crop/shift/pad/resize/affine\uff0c\u5177\u4f53\u4ee3\u7801\u8bf7\u6253\u5f00/sdcard/app/libs/AI2D.py\u67e5\u770b\n    def config_preprocess(self,input_image_size=None):\n        with ScopedTiming("set preprocess config",self.debug_mode > 0):\n            # \u521d\u59cb\u5316ai2d\u9884\u5904\u7406\u914d\u7f6e\uff0c\u9ed8\u8ba4\u4e3asensor\u7ed9\u5230AI\u7684\u5c3a\u5bf8\uff0c\u53ef\u4ee5\u901a\u8fc7\u8bbe\u7f6einput_image_size\u81ea\u884c\u4fee\u6539\u8f93\u5165\u5c3a\u5bf8\n            ai2d_input_size = input_image_size if input_image_size else self.rgb888p_size\n            # \u8ba1\u7b97padding\u53c2\u6570\u5e76\u5e94\u7528pad\u64cd\u4f5c\uff0c\u4ee5\u786e\u4fdd\u8f93\u5165\u56fe\u50cf\u5c3a\u5bf8\u4e0e\u6a21\u578b\u8f93\u5165\u5c3a\u5bf8\u5339\u914d\n            top, bottom, left, right = self.get_padding_param()\n            self.ai2d.pad([0, 0, 0, 0, top, bottom, left, right], 0, [114, 114, 114])\n            # \u4f7f\u7528\u53cc\u7ebf\u6027\u63d2\u503c\u8fdb\u884cresize\u64cd\u4f5c\uff0c\u8c03\u6574\u56fe\u50cf\u5c3a\u5bf8\u4ee5\u7b26\u5408\u6a21\u578b\u8f93\u5165\u8981\u6c42\n            self.ai2d.resize(nn.interp_method.tf_bilinear, nn.interp_mode.half_pixel)\n            # \u6784\u5efa\u9884\u5904\u7406\u6d41\u7a0b,\u53c2\u6570\u4e3a\u9884\u5904\u7406\u8f93\u5165tensor\u7684shape\u548c\u9884\u5904\u7406\u8f93\u51fa\u7684tensor\u7684shape\n            self.ai2d.build([1,3,ai2d_input_size[1],ai2d_input_size[0]],[1,3,self.model_input_size[1],self.model_input_size[0]])\n\n    # \u81ea\u5b9a\u4e49\u5f53\u524d\u4efb\u52a1\u7684\u540e\u5904\u7406\uff0c\u7528\u4e8e\u5904\u7406\u6a21\u578b\u8f93\u51fa\u7ed3\u679c\uff0c\u8fd9\u91cc\u4f7f\u7528\u4e86aicube\u5e93\u7684anchorbasedet_post_process\u63a5\u53e3\n    def postprocess(self,results):\n        with ScopedTiming("postprocess",self.debug_mode > 0):\n            dets = aicube.anchorbasedet_post_process(results[0], results[1], results[2], self.model_input_size, self.rgb888p_size, self.strides, len(self.labels), self.confidence_threshold, self.nms_threshold, self.anchors, self.nms_option)\n            # \u8fd4\u56de\u624b\u638c\u68c0\u6d4b\u7ed3\u679c\n            return dets\n\n    # \u8ba1\u7b97padding\u53c2\u6570\uff0c\u786e\u4fdd\u8f93\u5165\u56fe\u50cf\u5c3a\u5bf8\u4e0e\u6a21\u578b\u8f93\u5165\u5c3a\u5bf8\u5339\u914d\n    def get_padding_param(self):\n        # \u6839\u636e\u76ee\u6807\u5bbd\u5ea6\u548c\u9ad8\u5ea6\u8ba1\u7b97\u6bd4\u4f8b\u56e0\u5b50\n        dst_w = self.model_input_size[0]\n        dst_h = self.model_input_size[1]\n        input_width = self.rgb888p_size[0]\n        input_high = self.rgb888p_size[1]\n        ratio_w = dst_w / input_width\n        ratio_h = dst_h / input_high\n        # \u9009\u62e9\u8f83\u5c0f\u7684\u6bd4\u4f8b\u56e0\u5b50\uff0c\u4ee5\u786e\u4fdd\u56fe\u50cf\u5185\u5bb9\u5b8c\u6574\n        if ratio_w < ratio_h:\n            ratio = ratio_w\n        else:\n            ratio = ratio_h\n        # \u8ba1\u7b97\u65b0\u7684\u5bbd\u5ea6\u548c\u9ad8\u5ea6\n        new_w = int(ratio * input_width)\n        new_h = int(ratio * input_high)\n        # \u8ba1\u7b97\u5bbd\u5ea6\u548c\u9ad8\u5ea6\u7684\u5dee\u503c\uff0c\u5e76\u786e\u5b9apadding\u7684\u4f4d\u7f6e\n        dw = (dst_w - new_w) / 2\n        dh = (dst_h - new_h) / 2\n        top = int(round(dh - 0.1))\n        bottom = int(round(dh + 0.1))\n        left = int(round(dw - 0.1))\n        right = int(round(dw + 0.1))\n        return top, bottom, left, right\n\n# \u81ea\u5b9a\u4e49\u624b\u52bf\u5173\u952e\u70b9\u5206\u7c7b\u4efb\u52a1\u7c7b\nclass HandKPClassApp(AIBase):\n    def __init__(self,kmodel_path,model_input_size,rgb888p_size=[1920,1080],display_size=[1920,1080],debug_mode=0):\n        super().__init__(kmodel_path,model_input_size,rgb888p_size,debug_mode)\n        # kmodel\u8def\u5f84\n        self.kmodel_path=kmodel_path\n        # \u68c0\u6d4b\u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.model_input_size=model_input_size\n        # sensor\u7ed9\u5230AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.rgb888p_size=[ALIGN_UP(rgb888p_size[0],16),rgb888p_size[1]]\n        # \u89c6\u9891\u8f93\u51faVO\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.display_size=[ALIGN_UP(display_size[0],16),display_size[1]]\n        self.crop_params=[]\n        # debug\u6a21\u5f0f\n        self.debug_mode=debug_mode\n        # Ai2d\u5b9e\u4f8b\u7528\u4e8e\u5b9e\u73b0\u9884\u5904\u7406\n        self.ai2d=Ai2d(debug_mode)\n        # \u8bbe\u7f6eai2d\u7684\u8f93\u5165\u8f93\u51fa\u7684\u683c\u5f0f\u548c\u6570\u636e\u7c7b\u578b\n        self.ai2d.set_ai2d_dtype(nn.ai2d_format.NCHW_FMT,nn.ai2d_format.NCHW_FMT,np.uint8, np.uint8)\n\n    # \u914d\u7f6e\u9884\u5904\u7406\u64cd\u4f5c\uff0c\u8fd9\u91cc\u4f7f\u7528\u4e86crop\u548cresize\uff0cAi2d\u652f\u6301crop/shift/pad/resize/affine\uff0c\u5177\u4f53\u4ee3\u7801\u8bf7\u6253\u5f00/sdcard/app/libs/AI2D.py\u67e5\u770b\n    def config_preprocess(self,det,input_image_size=None):\n        with ScopedTiming("set preprocess config",self.debug_mode > 0):\n            ai2d_input_size=input_image_size if input_image_size else self.rgb888p_size\n            self.crop_params = self.get_crop_param(det)\n            self.ai2d.crop(self.crop_params[0],self.crop_params[1],self.crop_params[2],self.crop_params[3])\n            self.ai2d.resize(nn.interp_method.tf_bilinear, nn.interp_mode.half_pixel)\n            self.ai2d.build([1,3,ai2d_input_size[1],ai2d_input_size[0]],[1,3,self.model_input_size[1],self.model_input_size[0]])\n\n    # \u81ea\u5b9a\u4e49\u540e\u5904\u7406\uff0c\u5f97\u5230\u624b\u638c\u624b\u52bf\u7ed3\u679c\u548c\u624b\u638c\u5173\u952e\u70b9\u6570\u636e\n    def postprocess(self,results):\n        with ScopedTiming("postprocess",self.debug_mode > 0):\n            results=results[0].reshape(results[0].shape[0]*results[0].shape[1])\n            results_show = np.zeros(results.shape,dtype=np.int16)\n            results_show[0::2] = results[0::2] * self.crop_params[3] + self.crop_params[0]\n            results_show[1::2] = results[1::2] * self.crop_params[2] + self.crop_params[1]\n            gesture=self.hk_gesture(results_show)\n            results_show[0::2] = results_show[0::2] * (self.display_size[0] / self.rgb888p_size[0])\n            results_show[1::2] = results_show[1::2] * (self.display_size[1] / self.rgb888p_size[1])\n            return results_show,gesture\n\n    # \u8ba1\u7b97crop\u53c2\u6570\n    def get_crop_param(self,det_box):\n        x1, y1, x2, y2 = det_box[2],det_box[3],det_box[4],det_box[5]\n        w,h= int(x2 - x1),int(y2 - y1)\n        w_det = int(float(x2 - x1) * self.display_size[0] // self.rgb888p_size[0])\n        h_det = int(float(y2 - y1) * self.display_size[1] // self.rgb888p_size[1])\n        x_det = int(x1*self.display_size[0] // self.rgb888p_size[0])\n        y_det = int(y1*self.display_size[1] // self.rgb888p_size[1])\n        length = max(w, h)/2\n        cx = (x1+x2)/2\n        cy = (y1+y2)/2\n        ratio_num = 1.26*length\n        x1_kp = int(max(0,cx-ratio_num))\n        y1_kp = int(max(0,cy-ratio_num))\n        x2_kp = int(min(self.rgb888p_size[0]-1, cx+ratio_num))\n        y2_kp = int(min(self.rgb888p_size[1]-1, cy+ratio_num))\n        w_kp = int(x2_kp - x1_kp + 1)\n        h_kp = int(y2_kp - y1_kp + 1)\n        return [x1_kp, y1_kp, w_kp, h_kp]\n\n    # \u6c42\u4e24\u4e2avector\u4e4b\u95f4\u7684\u5939\u89d2\n    def hk_vector_2d_angle(self,v1,v2):\n        with ScopedTiming("hk_vector_2d_angle",self.debug_mode > 0):\n            v1_x,v1_y,v2_x,v2_y = v1[0],v1[1],v2[0],v2[1]\n            v1_norm = np.sqrt(v1_x * v1_x+ v1_y * v1_y)\n            v2_norm = np.sqrt(v2_x * v2_x + v2_y * v2_y)\n            dot_product = v1_x * v2_x + v1_y * v2_y\n            cos_angle = dot_product/(v1_norm*v2_norm)\n            angle = np.acos(cos_angle)*180/np.pi\n            return angle\n\n    # \u6839\u636e\u624b\u638c\u5173\u952e\u70b9\u68c0\u6d4b\u7ed3\u679c\u5224\u65ad\u624b\u52bf\u7c7b\u522b\n    def hk_gesture(self,results):\n        with ScopedTiming("hk_gesture",self.debug_mode > 0):\n            angle_list = []\n            for i in range(5):\n                angle = self.hk_vector_2d_angle([(results[0]-results[i*8+4]), (results[1]-results[i*8+5])],[(results[i*8+6]-results[i*8+8]),(results[i*8+7]-results[i*8+9])])\n                angle_list.append(angle)\n            thr_angle,thr_angle_thumb,thr_angle_s,gesture_str = 65.,53.,49.,None\n            if 65535. not in angle_list:\n                if (angle_list[0]>thr_angle_thumb)  and (angle_list[1]>thr_angle) and (angle_list[2]>thr_angle) and (angle_list[3]>thr_angle) and (angle_list[4]>thr_angle):\n                    gesture_str = "fist"\n                elif (angle_list[0]<thr_angle_s)  and (angle_list[1]<thr_angle_s) and (angle_list[2]<thr_angle_s) and (angle_list[3]<thr_angle_s) and (angle_list[4]<thr_angle_s):\n                    gesture_str = "five"\n                elif (angle_list[0]<thr_angle_s)  and (angle_list[1]<thr_angle_s) and (angle_list[2]>thr_angle) and (angle_list[3]>thr_angle) and (angle_list[4]>thr_angle):\n                    gesture_str = "gun"\n                elif (angle_list[0]<thr_angle_s)  and (angle_list[1]<thr_angle_s) and (angle_list[2]>thr_angle) and (angle_list[3]>thr_angle) and (angle_list[4]<thr_angle_s):\n                    gesture_str = "love"\n                elif (angle_list[0]>5)  and (angle_list[1]<thr_angle_s) and (angle_list[2]>thr_angle) and (angle_list[3]>thr_angle) and (angle_list[4]>thr_angle):\n                    gesture_str = "one"\n                elif (angle_list[0]<thr_angle_s)  and (angle_list[1]>thr_angle) and (angle_list[2]>thr_angle) and (angle_list[3]>thr_angle) and (angle_list[4]<thr_angle_s):\n                    gesture_str = "six"\n                elif (angle_list[0]>thr_angle_thumb)  and (angle_list[1]<thr_angle_s) and (angle_list[2]<thr_angle_s) and (angle_list[3]<thr_angle_s) and (angle_list[4]>thr_angle):\n                    gesture_str = "three"\n                elif (angle_list[0]<thr_angle_s)  and (angle_list[1]>thr_angle) and (angle_list[2]>thr_angle) and (angle_list[3]>thr_angle) and (angle_list[4]>thr_angle):\n                    gesture_str = "thumbUp"\n                elif (angle_list[0]>thr_angle_thumb)  and (angle_list[1]<thr_angle_s) and (angle_list[2]<thr_angle_s) and (angle_list[3]>thr_angle) and (angle_list[4]>thr_angle):\n                    gesture_str = "yeah"\n            return gesture_str\n\n# \u624b\u638c\u5173\u952e\u70b9\u5206\u7c7b\u4efb\u52a1\nclass HandKeyPointClass:\n    def __init__(self,hand_det_kmodel,hand_kp_kmodel,det_input_size,kp_input_size,labels,anchors,confidence_threshold=0.25,nms_threshold=0.3,nms_option=False,strides=[8,16,32],rgb888p_size=[1280,720],display_size=[1920,1080],debug_mode=0):\n        # \u624b\u638c\u68c0\u6d4b\u6a21\u578b\u8def\u5f84\n        self.hand_det_kmodel=hand_det_kmodel\n        # \u624b\u638c\u5173\u952e\u70b9\u6a21\u578b\u8def\u5f84\n        self.hand_kp_kmodel=hand_kp_kmodel\n        # \u624b\u638c\u68c0\u6d4b\u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.det_input_size=det_input_size\n        # \u624b\u638c\u5173\u952e\u70b9\u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.kp_input_size=kp_input_size\n        self.labels=labels\n        # anchors\n        self.anchors=anchors\n        # \u7f6e\u4fe1\u5ea6\u9608\u503c\n        self.confidence_threshold=confidence_threshold\n        # nms\u9608\u503c\n        self.nms_threshold=nms_threshold\n        self.nms_option=nms_option\n        self.strides=strides\n        # sensor\u7ed9\u5230AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.rgb888p_size=[ALIGN_UP(rgb888p_size[0],16),rgb888p_size[1]]\n        # \u89c6\u9891\u8f93\u51faVO\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.display_size=[ALIGN_UP(display_size[0],16),display_size[1]]\n        # debug_mode\u6a21\u5f0f\n        self.debug_mode=debug_mode\n        self.hand_det=HandDetApp(self.hand_det_kmodel,self.labels,model_input_size=self.det_input_size,anchors=self.anchors,confidence_threshold=self.confidence_threshold,nms_threshold=self.nms_threshold,nms_option=self.nms_option,strides=self.strides,rgb888p_size=self.rgb888p_size,display_size=self.display_size,debug_mode=0)\n        self.hand_kp=HandKPClassApp(self.hand_kp_kmodel,model_input_size=self.kp_input_size,rgb888p_size=self.rgb888p_size,display_size=self.display_size)\n        self.hand_det.config_preprocess()\n\n    # run\u51fd\u6570\n    def run(self,input_np):\n        # \u6267\u884c\u624b\u638c\u68c0\u6d4b\n        det_boxes=self.hand_det.run(input_np)\n        boxes=[]\n        gesture_res=[]\n        for det_box in det_boxes:\n            # \u5bf9\u4e8e\u68c0\u6d4b\u5230\u7684\u6bcf\u4e00\u4e2a\u624b\u638c\u6267\u884c\u5173\u952e\u70b9\u8bc6\u522b\n            x1, y1, x2, y2 = det_box[2],det_box[3],det_box[4],det_box[5]\n            w,h= int(x2 - x1),int(y2 - y1)\n            if (h<(0.1*self.rgb888p_size[1])):\n                continue\n            if (w<(0.25*self.rgb888p_size[0]) and ((x1<(0.03*self.rgb888p_size[0])) or (x2>(0.97*self.rgb888p_size[0])))):\n                continue\n            if (w<(0.15*self.rgb888p_size[0]) and ((x1<(0.01*self.rgb888p_size[0])) or (x2>(0.99*self.rgb888p_size[0])))):\n                continue\n            self.hand_kp.config_preprocess(det_box)\n            results_show,gesture=self.hand_kp.run(input_np)\n            gesture_res.append((results_show,gesture))\n            boxes.append(det_box)\n        return boxes,gesture_res\n\n    # \u7ed8\u5236\u6548\u679c\uff0c\u7ed8\u5236\u5173\u952e\u70b9\u3001\u624b\u638c\u68c0\u6d4b\u6846\u548c\u8bc6\u522b\u7ed3\u679c\n    def draw_result(self,pl,dets,gesture_res):\n        pl.osd_img.clear()\n        if len(dets)>0:\n            for k in range(len(dets)):\n                det_box=dets[k]\n                x1, y1, x2, y2 = det_box[2],det_box[3],det_box[4],det_box[5]\n                w,h= int(x2 - x1),int(y2 - y1)\n                if (h<(0.1*self.rgb888p_size[1])):\n                    continue\n                if (w<(0.25*self.rgb888p_size[0]) and ((x1<(0.03*self.rgb888p_size[0])) or (x2>(0.97*self.rgb888p_size[0])))):\n                    continue\n                if (w<(0.15*self.rgb888p_size[0]) and ((x1<(0.01*self.rgb888p_size[0])) or (x2>(0.99*self.rgb888p_size[0])))):\n                    continue\n                w_det = int(float(x2 - x1) * self.display_size[0] // self.rgb888p_size[0])\n                h_det = int(float(y2 - y1) * self.display_size[1] // self.rgb888p_size[1])\n                x_det = int(x1*self.display_size[0] // self.rgb888p_size[0])\n                y_det = int(y1*self.display_size[1] // self.rgb888p_size[1])\n                pl.osd_img.draw_rectangle(x_det, y_det, w_det, h_det, color=(255, 0, 255, 0), thickness = 2)\n\n                results_show=gesture_res[k][0]\n                for i in range(len(results_show)/2):\n                    pl.osd_img.draw_circle(results_show[i*2], results_show[i*2+1], 1, color=(255, 0, 255, 0),fill=False)\n                for i in range(5):\n                    j = i*8\n                    if i==0:\n                        R = 255; G = 0; B = 0\n                    if i==1:\n                        R = 255; G = 0; B = 255\n                    if i==2:\n                        R = 255; G = 255; B = 0\n                    if i==3:\n                        R = 0; G = 255; B = 0\n                    if i==4:\n                        R = 0; G = 0; B = 255\n                    pl.osd_img.draw_line(results_show[0], results_show[1], results_show[j+2], results_show[j+3], color=(255,R,G,B), thickness = 3)\n                    pl.osd_img.draw_line(results_show[j+2], results_show[j+3], results_show[j+4], results_show[j+5], color=(255,R,G,B), thickness = 3)\n                    pl.osd_img.draw_line(results_show[j+4], results_show[j+5], results_show[j+6], results_show[j+7], color=(255,R,G,B), thickness = 3)\n                    pl.osd_img.draw_line(results_show[j+6], results_show[j+7], results_show[j+8], results_show[j+9], color=(255,R,G,B), thickness = 3)\n\n                gesture_str=gesture_res[k][1]\n                pl.osd_img.draw_string_advanced( x_det , y_det-50,32, " " + str(gesture_str), color=(255,0, 255, 0))\n\n\n\nif __name__=="__main__":\n    # \u663e\u793a\u6a21\u5f0f\uff0c\u9ed8\u8ba4"hdmi",\u53ef\u4ee5\u9009\u62e9"hdmi"\u548c"lcd"\n    display_mode="hdmi"\n    if display_mode=="hdmi":\n        display_size=[1920,1080]\n    else:\n        display_size=[800,480]\n    # \u624b\u638c\u68c0\u6d4b\u6a21\u578b\u8def\u5f84\n    hand_det_kmodel_path="/sdcard/app/tests/kmodel/hand_det.kmodel"\n    # \u624b\u638c\u5173\u952e\u70b9\u6a21\u578b\u8def\u5f84\n    hand_kp_kmodel_path="/sdcard/app/tests/kmodel/handkp_det.kmodel"\n    # \u5176\u4ed6\u53c2\u6570\n    anchors_path="/sdcard/app/tests/utils/prior_data_320.bin"\n    rgb888p_size=[1920,1080]\n    hand_det_input_size=[512,512]\n    hand_kp_input_size=[256,256]\n    confidence_threshold=0.2\n    nms_threshold=0.5\n    labels=["hand"]\n    anchors = [26,27, 53,52, 75,71, 80,99, 106,82, 99,134, 140,113, 161,172, 245,276]\n\n    # \u521d\u59cb\u5316PipeLine\uff0c\u53ea\u5173\u6ce8\u4f20\u7ed9AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u663e\u793a\u7684\u5206\u8fa8\u7387\n    pl=PipeLine(rgb888p_size=rgb888p_size,display_size=display_size,display_mode=display_mode)\n    pl.create()\n    hkc=HandKeyPointClass(hand_det_kmodel_path,hand_kp_kmodel_path,det_input_size=hand_det_input_size,kp_input_size=hand_kp_input_size,labels=labels,anchors=anchors,confidence_threshold=confidence_threshold,nms_threshold=nms_threshold,nms_option=False,strides=[8,16,32],rgb888p_size=rgb888p_size,display_size=display_size)\n    try:\n        while True:\n            os.exitpoint()\n            with ScopedTiming("total",1):\n                img=pl.get_frame()                          # \u83b7\u53d6\u5f53\u524d\u5e27\n                det_boxes,gesture_res=hkc.run(img)          # \u63a8\u7406\u5f53\u524d\u5e27\n                hkc.draw_result(pl,det_boxes,gesture_res)   # \u7ed8\u5236\u5f53\u524d\u5e27\u63a8\u7406\u7ed3\u679c\n                pl.show_image()                             # \u5c55\u793a\u63a8\u7406\u7ed3\u679c\n                gc.collect()\n    except Exception as e:\n        sys.print_exception(e)\n    finally:\n        hkc.hand_det.deinit()\n        hkc.hand_kp.deinit()\n        pl.destroy()\n'})}),"\n",(0,i.jsx)(n.h3,{id:"214-\u624b\u638c\u5173\u952e\u70b9\u68c0\u6d4b",children:"2.14. \u624b\u638c\u5173\u952e\u70b9\u68c0\u6d4b"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:'from libs.PipeLine import PipeLine, ScopedTiming\nfrom libs.AIBase import AIBase\nfrom libs.AI2D import Ai2d\nimport os\nimport ujson\nfrom media.media import *\nfrom time import *\nimport nncase_runtime as nn\nimport ulab.numpy as np\nimport time\nimport image\nimport aicube\nimport random\nimport gc\nimport sys\n\n# \u81ea\u5b9a\u4e49\u624b\u638c\u68c0\u6d4b\u4efb\u52a1\u7c7b\nclass HandDetApp(AIBase):\n    def __init__(self,kmodel_path,labels,model_input_size,anchors,confidence_threshold=0.2,nms_threshold=0.5,nms_option=False, strides=[8,16,32],rgb888p_size=[1920,1080],display_size=[1920,1080],debug_mode=0):\n        super().__init__(kmodel_path,model_input_size,rgb888p_size,debug_mode)\n        # kmodel\u8def\u5f84\n        self.kmodel_path=kmodel_path\n        self.labels=labels\n        # \u68c0\u6d4b\u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.model_input_size=model_input_size\n        # \u7f6e\u4fe1\u5ea6\u9608\u503c\n        self.confidence_threshold=confidence_threshold\n        # nms\u9608\u503c\n        self.nms_threshold=nms_threshold\n        # \u951a\u6846,\u76ee\u6807\u68c0\u6d4b\u4efb\u52a1\u4f7f\u7528\n        self.anchors=anchors\n        # \u7279\u5f81\u4e0b\u91c7\u6837\u500d\u6570\n        self.strides = strides\n        # NMS\u9009\u9879\uff0c\u5982\u679c\u4e3aTrue\u505a\u7c7b\u95f4NMS,\u5982\u679c\u4e3aFalse\u505a\u7c7b\u5185NMS\n        self.nms_option = nms_option\n        # sensor\u7ed9\u5230AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.rgb888p_size=[ALIGN_UP(rgb888p_size[0],16),rgb888p_size[1]]\n        # \u89c6\u9891\u8f93\u51faVO\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.display_size=[ALIGN_UP(display_size[0],16),display_size[1]]\n        # debug\u6a21\u5f0f\n        self.debug_mode=debug_mode\n        # Ai2d\u5b9e\u4f8b\u7528\u4e8e\u5b9e\u73b0\u9884\u5904\u7406\n        self.ai2d=Ai2d(debug_mode)\n        # \u8bbe\u7f6eai2d\u7684\u8f93\u5165\u8f93\u51fa\u7684\u683c\u5f0f\u548c\u6570\u636e\u7c7b\u578b\n        self.ai2d.set_ai2d_dtype(nn.ai2d_format.NCHW_FMT,nn.ai2d_format.NCHW_FMT,np.uint8, np.uint8)\n\n    # \u914d\u7f6e\u9884\u5904\u7406\u64cd\u4f5c\uff0c\u8fd9\u91cc\u4f7f\u7528\u4e86pad\u548cresize\uff0cAi2d\u652f\u6301crop/shift/pad/resize/affine\uff0c\u5177\u4f53\u4ee3\u7801\u8bf7\u6253\u5f00/sdcard/app/libs/AI2D.py\u67e5\u770b\n    def config_preprocess(self,input_image_size=None):\n        with ScopedTiming("set preprocess config",self.debug_mode > 0):\n            # \u521d\u59cb\u5316ai2d\u9884\u5904\u7406\u914d\u7f6e\uff0c\u9ed8\u8ba4\u4e3asensor\u7ed9\u5230AI\u7684\u5c3a\u5bf8\uff0c\u53ef\u4ee5\u901a\u8fc7\u8bbe\u7f6einput_image_size\u81ea\u884c\u4fee\u6539\u8f93\u5165\u5c3a\u5bf8\n            ai2d_input_size = input_image_size if input_image_size else self.rgb888p_size\n            # \u8ba1\u7b97padding\u53c2\u6570\u5e76\u5e94\u7528pad\u64cd\u4f5c\uff0c\u4ee5\u786e\u4fdd\u8f93\u5165\u56fe\u50cf\u5c3a\u5bf8\u4e0e\u6a21\u578b\u8f93\u5165\u5c3a\u5bf8\u5339\u914d\n            top, bottom, left, right = self.get_padding_param()\n            self.ai2d.pad([0, 0, 0, 0, top, bottom, left, right], 0, [114, 114, 114])\n            # \u4f7f\u7528\u53cc\u7ebf\u6027\u63d2\u503c\u8fdb\u884cresize\u64cd\u4f5c\uff0c\u8c03\u6574\u56fe\u50cf\u5c3a\u5bf8\u4ee5\u7b26\u5408\u6a21\u578b\u8f93\u5165\u8981\u6c42\n            self.ai2d.resize(nn.interp_method.tf_bilinear, nn.interp_mode.half_pixel)\n            # \u6784\u5efa\u9884\u5904\u7406\u6d41\u7a0b,\u53c2\u6570\u4e3a\u9884\u5904\u7406\u8f93\u5165tensor\u7684shape\u548c\u9884\u5904\u7406\u8f93\u51fa\u7684tensor\u7684shape\n            self.ai2d.build([1,3,ai2d_input_size[1],ai2d_input_size[0]],[1,3,self.model_input_size[1],self.model_input_size[0]])\n\n    # \u81ea\u5b9a\u4e49\u5f53\u524d\u4efb\u52a1\u7684\u540e\u5904\u7406\uff0c\u7528\u4e8e\u5904\u7406\u6a21\u578b\u8f93\u51fa\u7ed3\u679c\uff0c\u8fd9\u91cc\u4f7f\u7528\u4e86aicube\u5e93\u7684anchorbasedet_post_process\u63a5\u53e3\n    def postprocess(self,results):\n        with ScopedTiming("postprocess",self.debug_mode > 0):\n            dets = aicube.anchorbasedet_post_process(results[0], results[1], results[2], self.model_input_size, self.rgb888p_size, self.strides, len(self.labels), self.confidence_threshold, self.nms_threshold, self.anchors, self.nms_option)\n            # \u8fd4\u56de\u624b\u638c\u68c0\u6d4b\u7ed3\u679c\n            return dets\n\n    # \u8ba1\u7b97padding\u53c2\u6570\uff0c\u786e\u4fdd\u8f93\u5165\u56fe\u50cf\u5c3a\u5bf8\u4e0e\u6a21\u578b\u8f93\u5165\u5c3a\u5bf8\u5339\u914d\n    def get_padding_param(self):\n        # \u6839\u636e\u76ee\u6807\u5bbd\u5ea6\u548c\u9ad8\u5ea6\u8ba1\u7b97\u6bd4\u4f8b\u56e0\u5b50\n        dst_w = self.model_input_size[0]\n        dst_h = self.model_input_size[1]\n        input_width = self.rgb888p_size[0]\n        input_high = self.rgb888p_size[1]\n        ratio_w = dst_w / input_width\n        ratio_h = dst_h / input_high\n        # \u9009\u62e9\u8f83\u5c0f\u7684\u6bd4\u4f8b\u56e0\u5b50\uff0c\u4ee5\u786e\u4fdd\u56fe\u50cf\u5185\u5bb9\u5b8c\u6574\n        if ratio_w < ratio_h:\n            ratio = ratio_w\n        else:\n            ratio = ratio_h\n        # \u8ba1\u7b97\u65b0\u7684\u5bbd\u5ea6\u548c\u9ad8\u5ea6\n        new_w = int(ratio * input_width)\n        new_h = int(ratio * input_high)\n        # \u8ba1\u7b97\u5bbd\u5ea6\u548c\u9ad8\u5ea6\u7684\u5dee\u503c\uff0c\u5e76\u786e\u5b9apadding\u7684\u4f4d\u7f6e\n        dw = (dst_w - new_w) / 2\n        dh = (dst_h - new_h) / 2\n        top = int(round(dh - 0.1))\n        bottom = int(round(dh + 0.1))\n        left = int(round(dw - 0.1))\n        right = int(round(dw + 0.1))\n        return top, bottom, left, right\n\n# \u81ea\u5b9a\u4e49\u624b\u52bf\u5173\u952e\u70b9\u68c0\u6d4b\u4efb\u52a1\u7c7b\nclass HandKPDetApp(AIBase):\n    def __init__(self,kmodel_path,model_input_size,rgb888p_size=[1920,1080],display_size=[1920,1080],debug_mode=0):\n        super().__init__(kmodel_path,model_input_size,rgb888p_size,debug_mode)\n        # kmodel\u8def\u5f84\n        self.kmodel_path=kmodel_path\n        # \u68c0\u6d4b\u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.model_input_size=model_input_size\n        # sensor\u7ed9\u5230AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.rgb888p_size=[ALIGN_UP(rgb888p_size[0],16),rgb888p_size[1]]\n        # \u89c6\u9891\u8f93\u51faVO\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.display_size=[ALIGN_UP(display_size[0],16),display_size[1]]\n        self.crop_params=[]\n        # debug\u6a21\u5f0f\n        self.debug_mode=debug_mode\n        # Ai2d\u5b9e\u4f8b\u7528\u4e8e\u5b9e\u73b0\u9884\u5904\u7406\n        self.ai2d=Ai2d(debug_mode)\n        # \u8bbe\u7f6eai2d\u7684\u8f93\u5165\u8f93\u51fa\u7684\u683c\u5f0f\u548c\u6570\u636e\u7c7b\u578b\n        self.ai2d.set_ai2d_dtype(nn.ai2d_format.NCHW_FMT,nn.ai2d_format.NCHW_FMT,np.uint8, np.uint8)\n\n    # \u914d\u7f6e\u9884\u5904\u7406\u64cd\u4f5c\uff0c\u8fd9\u91cc\u4f7f\u7528\u4e86crop\u548cresize\uff0cAi2d\u652f\u6301crop/shift/pad/resize/affine\uff0c\u5177\u4f53\u4ee3\u7801\u8bf7\u6253\u5f00/sdcard/app/libs/AI2D.py\u67e5\u770b\n    def config_preprocess(self,det,input_image_size=None):\n        with ScopedTiming("set preprocess config",self.debug_mode > 0):\n            ai2d_input_size=input_image_size if input_image_size else self.rgb888p_size\n            self.crop_params = self.get_crop_param(det)\n            self.ai2d.crop(self.crop_params[0],self.crop_params[1],self.crop_params[2],self.crop_params[3])\n            self.ai2d.resize(nn.interp_method.tf_bilinear, nn.interp_mode.half_pixel)\n            self.ai2d.build([1,3,ai2d_input_size[1],ai2d_input_size[0]],[1,3,self.model_input_size[1],self.model_input_size[0]])\n\n    # \u81ea\u5b9a\u4e49\u540e\u5904\u7406\uff0cresults\u662f\u6a21\u578b\u8f93\u51fa\u7684array\u5217\u8868\n    def postprocess(self,results):\n        with ScopedTiming("postprocess",self.debug_mode > 0):\n            results=results[0].reshape(results[0].shape[0]*results[0].shape[1])\n            results_show = np.zeros(results.shape,dtype=np.int16)\n            results_show[0::2] = results[0::2] * self.crop_params[3] + self.crop_params[0]\n            results_show[1::2] = results[1::2] * self.crop_params[2] + self.crop_params[1]\n            results_show[0::2] = results_show[0::2] * (self.display_size[0] / self.rgb888p_size[0])\n            results_show[1::2] = results_show[1::2] * (self.display_size[1] / self.rgb888p_size[1])\n            return results_show\n\n    # \u8ba1\u7b97crop\u53c2\u6570\n    def get_crop_param(self,det_box):\n        x1, y1, x2, y2 = det_box[2],det_box[3],det_box[4],det_box[5]\n        w,h= int(x2 - x1),int(y2 - y1)\n        w_det = int(float(x2 - x1) * self.display_size[0] // self.rgb888p_size[0])\n        h_det = int(float(y2 - y1) * self.display_size[1] // self.rgb888p_size[1])\n        x_det = int(x1*self.display_size[0] // self.rgb888p_size[0])\n        y_det = int(y1*self.display_size[1] // self.rgb888p_size[1])\n        length = max(w, h)/2\n        cx = (x1+x2)/2\n        cy = (y1+y2)/2\n        ratio_num = 1.26*length\n        x1_kp = int(max(0,cx-ratio_num))\n        y1_kp = int(max(0,cy-ratio_num))\n        x2_kp = int(min(self.rgb888p_size[0]-1, cx+ratio_num))\n        y2_kp = int(min(self.rgb888p_size[1]-1, cy+ratio_num))\n        w_kp = int(x2_kp - x1_kp + 1)\n        h_kp = int(y2_kp - y1_kp + 1)\n        return [x1_kp, y1_kp, w_kp, h_kp]\n\n# \u624b\u638c\u5173\u952e\u70b9\u68c0\u6d4b\u4efb\u52a1\nclass HandKeyPointDet:\n    def __init__(self,hand_det_kmodel,hand_kp_kmodel,det_input_size,kp_input_size,labels,anchors,confidence_threshold=0.25,nms_threshold=0.3,nms_option=False,strides=[8,16,32],rgb888p_size=[1280,720],display_size=[1920,1080],debug_mode=0):\n        # \u624b\u638c\u68c0\u6d4b\u6a21\u578b\u8def\u5f84\n        self.hand_det_kmodel=hand_det_kmodel\n        # \u624b\u638c\u5173\u952e\u70b9\u6a21\u578b\u8def\u5f84\n        self.hand_kp_kmodel=hand_kp_kmodel\n        # \u624b\u638c\u68c0\u6d4b\u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.det_input_size=det_input_size\n        # \u624b\u638c\u5173\u952e\u70b9\u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.kp_input_size=kp_input_size\n        self.labels=labels\n        # anchors\n        self.anchors=anchors\n        # \u7f6e\u4fe1\u5ea6\u9608\u503c\n        self.confidence_threshold=confidence_threshold\n        # nms\u9608\u503c\n        self.nms_threshold=nms_threshold\n        # nms\u9009\u9879\n        self.nms_option=nms_option\n        # \u7279\u5f81\u56fe\u5bf9\u4e8e\u8f93\u5165\u7684\u4e0b\u91c7\u6837\u500d\u6570\n        self.strides=strides\n        # sensor\u7ed9\u5230AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.rgb888p_size=[ALIGN_UP(rgb888p_size[0],16),rgb888p_size[1]]\n        # \u89c6\u9891\u8f93\u51faVO\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.display_size=[ALIGN_UP(display_size[0],16),display_size[1]]\n        # debug_mode\u6a21\u5f0f\n        self.debug_mode=debug_mode\n        self.hand_det=HandDetApp(self.hand_det_kmodel,self.labels,model_input_size=self.det_input_size,anchors=self.anchors,confidence_threshold=self.confidence_threshold,nms_threshold=self.nms_threshold,nms_option=self.nms_option,strides=self.strides,rgb888p_size=self.rgb888p_size,display_size=self.display_size,debug_mode=0)\n        self.hand_kp=HandKPDetApp(self.hand_kp_kmodel,model_input_size=self.kp_input_size,rgb888p_size=self.rgb888p_size,display_size=self.display_size)\n        self.hand_det.config_preprocess()\n\n    # run\u51fd\u6570\n    def run(self,input_np):\n        # \u624b\u638c\u68c0\u6d4b\n        det_boxes=self.hand_det.run(input_np)\n        hand_res=[]\n        boxes=[]\n        for det_box in det_boxes:\n            # \u5bf9\u68c0\u6d4b\u5230\u7684\u6bcf\u4e2a\u624b\u638c\u6267\u884c\u624b\u52bf\u5173\u952e\u70b9\u8bc6\u522b\n            x1, y1, x2, y2 = det_box[2],det_box[3],det_box[4],det_box[5]\n            w,h= int(x2 - x1),int(y2 - y1)\n            # \u4e22\u5f03\u4e0d\u5408\u7406\u7684\u6846\n            if (h<(0.1*self.rgb888p_size[1])):\n                continue\n            if (w<(0.25*self.rgb888p_size[0]) and ((x1<(0.03*self.rgb888p_size[0])) or (x2>(0.97*self.rgb888p_size[0])))):\n                continue\n            if (w<(0.15*self.rgb888p_size[0]) and ((x1<(0.01*self.rgb888p_size[0])) or (x2>(0.99*self.rgb888p_size[0])))):\n                continue\n            self.hand_kp.config_preprocess(det_box)\n            results_show=self.hand_kp.run(input_np)\n            boxes.append(det_box)\n            hand_res.append(results_show)\n        return boxes,hand_res\n\n    # \u7ed8\u5236\u6548\u679c\uff0c\u7ed8\u5236\u624b\u638c\u5173\u952e\u70b9\u3001\u68c0\u6d4b\u6846\n    def draw_result(self,pl,dets,hand_res):\n        pl.osd_img.clear()\n        if dets:\n            for k in range(len(dets)):\n                det_box=dets[k]\n                x1, y1, x2, y2 = det_box[2],det_box[3],det_box[4],det_box[5]\n                w,h= int(x2 - x1),int(y2 - y1)\n                w_det = int(float(x2 - x1) * self.display_size[0] // self.rgb888p_size[0])\n                h_det = int(float(y2 - y1) * self.display_size[1] // self.rgb888p_size[1])\n                x_det = int(x1*self.display_size[0] // self.rgb888p_size[0])\n                y_det = int(y1*self.display_size[1] // self.rgb888p_size[1])\n                pl.osd_img.draw_rectangle(x_det, y_det, w_det, h_det, color=(255, 0, 255, 0), thickness = 2)\n\n                results_show=hand_res[k]\n                for i in range(len(results_show)/2):\n                    pl.osd_img.draw_circle(results_show[i*2], results_show[i*2+1], 1, color=(255, 0, 255, 0),fill=False)\n                for i in range(5):\n                    j = i*8\n                    if i==0:\n                        R = 255; G = 0; B = 0\n                    if i==1:\n                        R = 255; G = 0; B = 255\n                    if i==2:\n                        R = 255; G = 255; B = 0\n                    if i==3:\n                        R = 0; G = 255; B = 0\n                    if i==4:\n                        R = 0; G = 0; B = 255\n                    pl.osd_img.draw_line(results_show[0], results_show[1], results_show[j+2], results_show[j+3], color=(255,R,G,B), thickness = 3)\n                    pl.osd_img.draw_line(results_show[j+2], results_show[j+3], results_show[j+4], results_show[j+5], color=(255,R,G,B), thickness = 3)\n                    pl.osd_img.draw_line(results_show[j+4], results_show[j+5], results_show[j+6], results_show[j+7], color=(255,R,G,B), thickness = 3)\n                    pl.osd_img.draw_line(results_show[j+6], results_show[j+7], results_show[j+8], results_show[j+9], color=(255,R,G,B), thickness = 3)\n\n\n\nif __name__=="__main__":\n    # \u663e\u793a\u6a21\u5f0f\uff0c\u9ed8\u8ba4"hdmi",\u53ef\u4ee5\u9009\u62e9"hdmi"\u548c"lcd"\n    display_mode="hdmi"\n    if display_mode=="hdmi":\n        display_size=[1920,1080]\n    else:\n        display_size=[800,480]\n    # \u624b\u638c\u68c0\u6d4b\u6a21\u578b\u8def\u5f84\n    hand_det_kmodel_path="/sdcard/app/tests/kmodel/hand_det.kmodel"\n    # \u624b\u90e8\u5173\u952e\u70b9\u6a21\u578b\u8def\u5f84\n    hand_kp_kmodel_path="/sdcard/app/tests/kmodel/handkp_det.kmodel"\n    # \u5176\u5b83\u53c2\u6570\n    anchors_path="/sdcard/app/tests/utils/prior_data_320.bin"\n    rgb888p_size=[1920,1080]\n    hand_det_input_size=[512,512]\n    hand_kp_input_size=[256,256]\n    confidence_threshold=0.2\n    nms_threshold=0.5\n    labels=["hand"]\n    anchors = [26,27, 53,52, 75,71, 80,99, 106,82, 99,134, 140,113, 161,172, 245,276]\n\n    # \u521d\u59cb\u5316PipeLine\uff0c\u53ea\u5173\u6ce8\u4f20\u7ed9AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u663e\u793a\u7684\u5206\u8fa8\u7387\n    pl=PipeLine(rgb888p_size=rgb888p_size,display_size=display_size,display_mode=display_mode)\n    pl.create()\n    hkd=HandKeyPointDet(hand_det_kmodel_path,hand_kp_kmodel_path,det_input_size=hand_det_input_size,kp_input_size=hand_kp_input_size,labels=labels,anchors=anchors,confidence_threshold=confidence_threshold,nms_threshold=nms_threshold,nms_option=False,strides=[8,16,32],rgb888p_size=rgb888p_size,display_size=display_size)\n    try:\n        while True:\n            os.exitpoint()\n            with ScopedTiming("total",1):\n                img=pl.get_frame()                      # \u83b7\u53d6\u5f53\u524d\u5e27\n                det_boxes,hand_res=hkd.run(img)         # \u63a8\u7406\u5f53\u524d\u5e27\n                hkd.draw_result(pl,det_boxes,hand_res)  # \u7ed8\u5236\u63a8\u7406\u7ed3\u679c\n                pl.show_image()                         # \u5c55\u793a\u63a8\u7406\u7ed3\u679c\n                gc.collect()\n    except Exception as e:\n        sys.print_exception(e)\n    finally:\n        hkd.hand_det.deinit()\n        hkd.hand_kp.deinit()\n        pl.destroy()\n'})}),"\n",(0,i.jsx)(n.h3,{id:"215-\u624b\u52bf\u8bc6\u522b",children:"2.15. \u624b\u52bf\u8bc6\u522b"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:'from libs.PipeLine import PipeLine, ScopedTiming\nfrom libs.AIBase import AIBase\nfrom libs.AI2D import Ai2d\nimport os\nimport ujson\nfrom media.media import *\nfrom time import *\nimport nncase_runtime as nn\nimport ulab.numpy as np\nimport time\nimport image\nimport aicube\nimport random\nimport gc\nimport sys\n\n# \u81ea\u5b9a\u4e49\u624b\u638c\u68c0\u6d4b\u4efb\u52a1\u7c7b\nclass HandDetApp(AIBase):\n    def __init__(self,kmodel_path,model_input_size,anchors,confidence_threshold=0.2,nms_threshold=0.5,nms_option=False, strides=[8,16,32],rgb888p_size=[1920,1080],display_size=[1920,1080],debug_mode=0):\n        super().__init__(kmodel_path,model_input_size,rgb888p_size,debug_mode)\n        # kmodel\u8def\u5f84\n        self.kmodel_path=kmodel_path\n        # \u68c0\u6d4b\u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.model_input_size=model_input_size\n        # \u7f6e\u4fe1\u5ea6\u9608\u503c\n        self.confidence_threshold=confidence_threshold\n        # nms\u9608\u503c\n        self.nms_threshold=nms_threshold\n        # \u951a\u6846,\u76ee\u6807\u68c0\u6d4b\u4efb\u52a1\u4f7f\u7528\n        self.anchors=anchors\n        # \u7279\u5f81\u4e0b\u91c7\u6837\u500d\u6570\n        self.strides = strides\n        # NMS\u9009\u9879\uff0c\u5982\u679c\u4e3aTrue\u505a\u7c7b\u95f4NMS,\u5982\u679c\u4e3aFalse\u505a\u7c7b\u5185NMS\n        self.nms_option = nms_option\n        # sensor\u7ed9\u5230AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.rgb888p_size=[ALIGN_UP(rgb888p_size[0],16),rgb888p_size[1]]\n        # \u89c6\u9891\u8f93\u51faVO\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.display_size=[ALIGN_UP(display_size[0],16),display_size[1]]\n        # debug\u6a21\u5f0f\n        self.debug_mode=debug_mode\n        # Ai2d\u5b9e\u4f8b\u7528\u4e8e\u5b9e\u73b0\u9884\u5904\u7406\n        self.ai2d=Ai2d(debug_mode)\n        # \u8bbe\u7f6eai2d\u7684\u8f93\u5165\u8f93\u51fa\u7684\u683c\u5f0f\u548c\u6570\u636e\u7c7b\u578b\n        self.ai2d.set_ai2d_dtype(nn.ai2d_format.NCHW_FMT,nn.ai2d_format.NCHW_FMT,np.uint8, np.uint8)\n\n    # \u914d\u7f6e\u9884\u5904\u7406\u64cd\u4f5c\uff0c\u8fd9\u91cc\u4f7f\u7528\u4e86pad\u548cresize\uff0cAi2d\u652f\u6301crop/shift/pad/resize/affine\uff0c\u5177\u4f53\u4ee3\u7801\u8bf7\u6253\u5f00/sdcard/app/libs/AI2D.py\u67e5\u770b\n    def config_preprocess(self,input_image_size=None):\n        with ScopedTiming("set preprocess config",self.debug_mode > 0):\n            # \u521d\u59cb\u5316ai2d\u9884\u5904\u7406\u914d\u7f6e\uff0c\u9ed8\u8ba4\u4e3asensor\u7ed9\u5230AI\u7684\u5c3a\u5bf8\uff0c\u53ef\u4ee5\u901a\u8fc7\u8bbe\u7f6einput_image_size\u81ea\u884c\u4fee\u6539\u8f93\u5165\u5c3a\u5bf8\n            ai2d_input_size = input_image_size if input_image_size else self.rgb888p_size\n            # \u8ba1\u7b97padding\u53c2\u6570\u5e76\u5e94\u7528pad\u64cd\u4f5c\uff0c\u4ee5\u786e\u4fdd\u8f93\u5165\u56fe\u50cf\u5c3a\u5bf8\u4e0e\u6a21\u578b\u8f93\u5165\u5c3a\u5bf8\u5339\u914d\n            top, bottom, left, right = self.get_padding_param()\n            self.ai2d.pad([0, 0, 0, 0, top, bottom, left, right], 0, [114, 114, 114])\n            # \u4f7f\u7528\u53cc\u7ebf\u6027\u63d2\u503c\u8fdb\u884cresize\u64cd\u4f5c\uff0c\u8c03\u6574\u56fe\u50cf\u5c3a\u5bf8\u4ee5\u7b26\u5408\u6a21\u578b\u8f93\u5165\u8981\u6c42\n            self.ai2d.resize(nn.interp_method.tf_bilinear, nn.interp_mode.half_pixel)\n            # \u6784\u5efa\u9884\u5904\u7406\u6d41\u7a0b,\u53c2\u6570\u4e3a\u9884\u5904\u7406\u8f93\u5165tensor\u7684shape\u548c\u9884\u5904\u7406\u8f93\u51fa\u7684tensor\u7684shape\n            self.ai2d.build([1,3,ai2d_input_size[1],ai2d_input_size[0]],[1,3,self.model_input_size[1],self.model_input_size[0]])\n\n    # \u81ea\u5b9a\u4e49\u5f53\u524d\u4efb\u52a1\u7684\u540e\u5904\u7406\uff0c\u7528\u4e8e\u5904\u7406\u6a21\u578b\u8f93\u51fa\u7ed3\u679c\uff0c\u8fd9\u91cc\u4f7f\u7528\u4e86aicube\u5e93\u7684anchorbasedet_post_process\u63a5\u53e3\n    def postprocess(self,results):\n        with ScopedTiming("postprocess",self.debug_mode > 0):\n            dets = aicube.anchorbasedet_post_process(results[0], results[1], results[2], self.model_input_size, self.rgb888p_size, self.strides,1, self.confidence_threshold, self.nms_threshold, self.anchors, self.nms_option)\n            # \u8fd4\u56de\u624b\u638c\u68c0\u6d4b\u7ed3\u679c\n            return dets\n\n    # \u8ba1\u7b97padding\u53c2\u6570\uff0c\u786e\u4fdd\u8f93\u5165\u56fe\u50cf\u5c3a\u5bf8\u4e0e\u6a21\u578b\u8f93\u5165\u5c3a\u5bf8\u5339\u914d\n    def get_padding_param(self):\n        # \u6839\u636e\u76ee\u6807\u5bbd\u5ea6\u548c\u9ad8\u5ea6\u8ba1\u7b97\u6bd4\u4f8b\u56e0\u5b50\n        dst_w = self.model_input_size[0]\n        dst_h = self.model_input_size[1]\n        input_width = self.rgb888p_size[0]\n        input_high = self.rgb888p_size[1]\n        ratio_w = dst_w / input_width\n        ratio_h = dst_h / input_high\n        # \u9009\u62e9\u8f83\u5c0f\u7684\u6bd4\u4f8b\u56e0\u5b50\uff0c\u4ee5\u786e\u4fdd\u56fe\u50cf\u5185\u5bb9\u5b8c\u6574\n        if ratio_w < ratio_h:\n            ratio = ratio_w\n        else:\n            ratio = ratio_h\n        # \u8ba1\u7b97\u65b0\u7684\u5bbd\u5ea6\u548c\u9ad8\u5ea6\n        new_w = int(ratio * input_width)\n        new_h = int(ratio * input_high)\n        # \u8ba1\u7b97\u5bbd\u5ea6\u548c\u9ad8\u5ea6\u7684\u5dee\u503c\uff0c\u5e76\u786e\u5b9apadding\u7684\u4f4d\u7f6e\n        dw = (dst_w - new_w) / 2\n        dh = (dst_h - new_h) / 2\n        top = int(round(dh - 0.1))\n        bottom = int(round(dh + 0.1))\n        left = int(round(dw - 0.1))\n        right = int(round(dw + 0.1))\n        return top, bottom, left, right\n\n# \u81ea\u5b9a\u4e49\u624b\u52bf\u8bc6\u522b\u4efb\u52a1\u7c7b\nclass HandRecognitionApp(AIBase):\n    def __init__(self,kmodel_path,model_input_size,labels,rgb888p_size=[1920,1080],display_size=[1920,1080],debug_mode=0):\n        super().__init__(kmodel_path,model_input_size,rgb888p_size,debug_mode)\n        # kmodel\u8def\u5f84\n        self.kmodel_path=kmodel_path\n        # \u68c0\u6d4b\u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.model_input_size=model_input_size\n        self.labels=labels\n        # sensor\u7ed9\u5230AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.rgb888p_size=[ALIGN_UP(rgb888p_size[0],16),rgb888p_size[1]]\n        # \u89c6\u9891\u8f93\u51faVO\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.display_size=[ALIGN_UP(display_size[0],16),display_size[1]]\n        self.crop_params=[]\n        # debug\u6a21\u5f0f\n        self.debug_mode=debug_mode\n        # Ai2d\u5b9e\u4f8b\u7528\u4e8e\u5b9e\u73b0\u9884\u5904\u7406\n        self.ai2d=Ai2d(debug_mode)\n        # \u8bbe\u7f6eai2d\u7684\u8f93\u5165\u8f93\u51fa\u7684\u683c\u5f0f\u548c\u6570\u636e\u7c7b\u578b\n        self.ai2d.set_ai2d_dtype(nn.ai2d_format.NCHW_FMT,nn.ai2d_format.NCHW_FMT,np.uint8, np.uint8)\n\n    # \u914d\u7f6e\u9884\u5904\u7406\u64cd\u4f5c\uff0c\u8fd9\u91cc\u4f7f\u7528\u4e86crop\u548cresize\uff0cAi2d\u652f\u6301crop/shift/pad/resize/affine\uff0c\u5177\u4f53\u4ee3\u7801\u8bf7\u6253\u5f00/sdcard/app/libs/AI2D.py\u67e5\u770b\n    def config_preprocess(self,det,input_image_size=None):\n        with ScopedTiming("set preprocess config",self.debug_mode > 0):\n            ai2d_input_size=input_image_size if input_image_size else self.rgb888p_size\n            self.crop_params = self.get_crop_param(det)\n            self.ai2d.crop(self.crop_params[0],self.crop_params[1],self.crop_params[2],self.crop_params[3])\n            self.ai2d.resize(nn.interp_method.tf_bilinear, nn.interp_mode.half_pixel)\n            self.ai2d.build([1,3,ai2d_input_size[1],ai2d_input_size[0]],[1,3,self.model_input_size[1],self.model_input_size[0]])\n\n    # \u81ea\u5b9a\u4e49\u540e\u5904\u7406\uff0cresults\u662f\u6a21\u578b\u8f93\u51fa\u7684array\u5217\u8868\n    def postprocess(self,results):\n        with ScopedTiming("postprocess",self.debug_mode > 0):\n            result=results[0].reshape(results[0].shape[0]*results[0].shape[1])\n            x_softmax = self.softmax(result)\n            idx = np.argmax(x_softmax)\n            text = " " + self.labels[idx] + ": " + str(round(x_softmax[idx],2))\n            return text\n\n    # \u8ba1\u7b97crop\u53c2\u6570\n    def get_crop_param(self,det_box):\n        x1, y1, x2, y2 = det_box[2],det_box[3],det_box[4],det_box[5]\n        w,h= int(x2 - x1),int(y2 - y1)\n        w_det = int(float(x2 - x1) * self.display_size[0] // self.rgb888p_size[0])\n        h_det = int(float(y2 - y1) * self.display_size[1] // self.rgb888p_size[1])\n        x_det = int(x1*self.display_size[0] // self.rgb888p_size[0])\n        y_det = int(y1*self.display_size[1] // self.rgb888p_size[1])\n        length = max(w, h)/2\n        cx = (x1+x2)/2\n        cy = (y1+y2)/2\n        ratio_num = 1.26*length\n        x1_kp = int(max(0,cx-ratio_num))\n        y1_kp = int(max(0,cy-ratio_num))\n        x2_kp = int(min(self.rgb888p_size[0]-1, cx+ratio_num))\n        y2_kp = int(min(self.rgb888p_size[1]-1, cy+ratio_num))\n        w_kp = int(x2_kp - x1_kp + 1)\n        h_kp = int(y2_kp - y1_kp + 1)\n        return [x1_kp, y1_kp, w_kp, h_kp]\n\n    # softmax\u5b9e\u73b0\n    def softmax(self,x):\n        x -= np.max(x)\n        x = np.exp(x) / np.sum(np.exp(x))\n        return x\n\nclass HandRecognition:\n    def __init__(self,hand_det_kmodel,hand_kp_kmodel,det_input_size,kp_input_size,labels,anchors,confidence_threshold=0.25,nms_threshold=0.3,nms_option=False,strides=[8,16,32],rgb888p_size=[1280,720],display_size=[1920,1080],debug_mode=0):\n        # \u624b\u638c\u68c0\u6d4b\u6a21\u578b\u8def\u5f84\n        self.hand_det_kmodel=hand_det_kmodel\n        # \u624b\u638c\u5173\u952e\u70b9\u6a21\u578b\u8def\u5f84\n        self.hand_kp_kmodel=hand_kp_kmodel\n        # \u624b\u638c\u68c0\u6d4b\u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.det_input_size=det_input_size\n        # \u624b\u638c\u5173\u952e\u70b9\u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.kp_input_size=kp_input_size\n        self.labels=labels\n        # anchors\n        self.anchors=anchors\n        # \u7f6e\u4fe1\u5ea6\u9608\u503c\n        self.confidence_threshold=confidence_threshold\n        # nms\u9608\u503c\n        self.nms_threshold=nms_threshold\n        # nms\u9009\u9879\n        self.nms_option=nms_option\n        # \u7279\u5f81\u56fe\u9488\u5bf9\u8f93\u51fa\u7684\u4e0b\u91c7\u6837\u500d\u6570\n        self.strides=strides\n        # sensor\u7ed9\u5230AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.rgb888p_size=[ALIGN_UP(rgb888p_size[0],16),rgb888p_size[1]]\n        # \u89c6\u9891\u8f93\u51faVO\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.display_size=[ALIGN_UP(display_size[0],16),display_size[1]]\n        # debug_mode\u6a21\u5f0f\n        self.debug_mode=debug_mode\n        self.hand_det=HandDetApp(self.hand_det_kmodel,model_input_size=self.det_input_size,anchors=self.anchors,confidence_threshold=self.confidence_threshold,nms_threshold=self.nms_threshold,nms_option=self.nms_option,strides=self.strides,rgb888p_size=self.rgb888p_size,display_size=self.display_size,debug_mode=0)\n        self.hand_rec=HandRecognitionApp(self.hand_kp_kmodel,model_input_size=self.kp_input_size,labels=self.labels,rgb888p_size=self.rgb888p_size,display_size=self.display_size)\n        self.hand_det.config_preprocess()\n\n    # run\u51fd\u6570\n    def run(self,input_np):\n        # \u6267\u884c\u624b\u638c\u68c0\u6d4b\n        det_boxes=self.hand_det.run(input_np)\n        hand_rec_res=[]\n        hand_det_res=[]\n        for det_box in det_boxes:\n            # \u5bf9\u68c0\u6d4b\u5230\u7684\u6bcf\u4e00\u4e2a\u624b\u638c\u6267\u884c\u624b\u52bf\u8bc6\u522b\n            x1, y1, x2, y2 = det_box[2],det_box[3],det_box[4],det_box[5]\n            w,h= int(x2 - x1),int(y2 - y1)\n            if (h<(0.1*self.rgb888p_size[1])):\n                continue\n            if (w<(0.25*self.rgb888p_size[0]) and ((x1<(0.03*self.rgb888p_size[0])) or (x2>(0.97*self.rgb888p_size[0])))):\n                continue\n            if (w<(0.15*self.rgb888p_size[0]) and ((x1<(0.01*self.rgb888p_size[0])) or (x2>(0.99*self.rgb888p_size[0])))):\n                continue\n            self.hand_rec.config_preprocess(det_box)\n            text=self.hand_rec.run(input_np)\n            hand_det_res.append(det_box)\n            hand_rec_res.append(text)\n        return hand_det_res,hand_rec_res\n\n    # \u7ed8\u5236\u6548\u679c\uff0c\u7ed8\u5236\u8bc6\u522b\u7ed3\u679c\u548c\u68c0\u6d4b\u6846\n    def draw_result(self,pl,hand_det_res,hand_rec_res):\n        pl.osd_img.clear()\n        if hand_det_res:\n            for k in range(len(hand_det_res)):\n                det_box=hand_det_res[k]\n                x1, y1, x2, y2 = det_box[2],det_box[3],det_box[4],det_box[5]\n                w,h= int(x2 - x1),int(y2 - y1)\n                w_det = int(float(x2 - x1) * self.display_size[0] // self.rgb888p_size[0])\n                h_det = int(float(y2 - y1) * self.display_size[1] // self.rgb888p_size[1])\n                x_det = int(x1*self.display_size[0] // self.rgb888p_size[0])\n                y_det = int(y1*self.display_size[1] // self.rgb888p_size[1])\n                pl.osd_img.draw_rectangle(x_det, y_det, w_det, h_det, color=(255, 0, 255, 0), thickness = 2)\n                pl.osd_img.draw_string_advanced( x_det, y_det-50, 32,hand_rec_res[k], color=(255,0, 255, 0))\n\n\nif __name__=="__main__":\n    # \u663e\u793a\u6a21\u5f0f\uff0c\u9ed8\u8ba4"hdmi",\u53ef\u4ee5\u9009\u62e9"hdmi"\u548c"lcd"\n    display_mode="hdmi"\n    if display_mode=="hdmi":\n        display_size=[1920,1080]\n    else:\n        display_size=[800,480]\n    # \u624b\u638c\u68c0\u6d4b\u6a21\u578b\u8def\u5f84\n    hand_det_kmodel_path="/sdcard/app/tests/kmodel/hand_det.kmodel"\n    # \u624b\u52bf\u8bc6\u522b\u6a21\u578b\u8def\u5f84\n    hand_rec_kmodel_path="/sdcard/app/tests/kmodel/hand_reco.kmodel"\n    # \u5176\u5b83\u53c2\u6570\n    anchors_path="/sdcard/app/tests/utils/prior_data_320.bin"\n    rgb888p_size=[1920,1080]\n    hand_det_input_size=[512,512]\n    hand_rec_input_size=[224,224]\n    confidence_threshold=0.2\n    nms_threshold=0.5\n    labels=["gun","other","yeah","five"]\n    anchors = [26,27, 53,52, 75,71, 80,99, 106,82, 99,134, 140,113, 161,172, 245,276]\n\n    # \u521d\u59cb\u5316PipeLine\uff0c\u53ea\u5173\u6ce8\u4f20\u7ed9AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u663e\u793a\u7684\u5206\u8fa8\u7387\n    pl=PipeLine(rgb888p_size=rgb888p_size,display_size=display_size,display_mode=display_mode)\n    pl.create()\n    hr=HandRecognition(hand_det_kmodel_path,hand_rec_kmodel_path,det_input_size=hand_det_input_size,kp_input_size=hand_rec_input_size,labels=labels,anchors=anchors,confidence_threshold=confidence_threshold,nms_threshold=nms_threshold,nms_option=False,strides=[8,16,32],rgb888p_size=rgb888p_size,display_size=display_size)\n    try:\n        while True:\n            os.exitpoint()\n            with ScopedTiming("total",1):\n                img=pl.get_frame()                              # \u83b7\u53d6\u5f53\u524d\u5e27\n                hand_det_res,hand_rec_res=hr.run(img)           # \u63a8\u7406\u5f53\u524d\u5e27\n                hr.draw_result(pl,hand_det_res,hand_rec_res)    # \u7ed8\u5236\u63a8\u7406\u7ed3\u679c\n                pl.show_image()                                 # \u5c55\u793a\u63a8\u7406\u7ed3\u679c\n                gc.collect()\n    except Exception as e:\n        sys.print_exception(e)\n    finally:\n        hr.hand_det.deinit()\n        hr.hand_rec.deinit()\n        pl.destroy()\n'})}),"\n",(0,i.jsx)(n.h3,{id:"216-\u5173\u952e\u8bcd\u5524\u9192",children:"2.16 \u5173\u952e\u8bcd\u5524\u9192"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:'from libs.PipeLine import ScopedTiming\nfrom libs.AIBase import AIBase\nfrom libs.AI2D import Ai2d\nfrom media.pyaudio import *                     # \u97f3\u9891\u6a21\u5757\nfrom media.media import *                       # \u8f6f\u4ef6\u62bd\u8c61\u6a21\u5757\uff0c\u4e3b\u8981\u5c01\u88c5\u5a92\u4f53\u6570\u636e\u94fe\u8def\u4ee5\u53ca\u5a92\u4f53\u7f13\u51b2\u533a\nimport media.wave as wave                       # wav\u97f3\u9891\u5904\u7406\u6a21\u5757\nimport nncase_runtime as nn                     # nncase\u8fd0\u884c\u6a21\u5757\uff0c\u5c01\u88c5\u4e86kpu\uff08kmodel\u63a8\u7406\uff09\u548cai2d\uff08\u56fe\u7247\u9884\u5904\u7406\u52a0\u901f\uff09\u64cd\u4f5c\nimport ulab.numpy as np                         # \u7c7b\u4f3cpython numpy\u64cd\u4f5c\uff0c\u4f46\u4e5f\u4f1a\u6709\u4e00\u4e9b\u63a5\u53e3\u4e0d\u540c\nimport aidemo                                   # aidemo\u6a21\u5757\uff0c\u5c01\u88c5ai demo\u76f8\u5173\u524d\u5904\u7406\u3001\u540e\u5904\u7406\u7b49\u64cd\u4f5c\nimport time                                     # \u65f6\u95f4\u7edf\u8ba1\nimport struct                                   # \u5b57\u8282\u5b57\u7b26\u8f6c\u6362\u6a21\u5757\nimport gc                                       # \u5783\u573e\u56de\u6536\u6a21\u5757\nimport os,sys                                   # \u64cd\u4f5c\u7cfb\u7edf\u63a5\u53e3\u6a21\u5757\n\n# \u81ea\u5b9a\u4e49\u5173\u952e\u8bcd\u5524\u9192\u7c7b\uff0c\u7ee7\u627f\u81eaAIBase\u57fa\u7c7b\nclass KWSApp(AIBase):\n    def __init__(self, kmodel_path, threshold, debug_mode=0):\n        super().__init__(kmodel_path)  # \u8c03\u7528\u57fa\u7c7b\u7684\u6784\u9020\u51fd\u6570\n        self.kmodel_path = kmodel_path  # \u6a21\u578b\u6587\u4ef6\u8def\u5f84\n        self.threshold=threshold\n        self.debug_mode = debug_mode  # \u662f\u5426\u5f00\u542f\u8c03\u8bd5\u6a21\u5f0f\n        self.cache_np = np.zeros((1, 256, 105), dtype=np.float)\n\n    # \u81ea\u5b9a\u4e49\u9884\u5904\u7406\uff0c\u8fd4\u56de\u6a21\u578b\u8f93\u5165tensor\u5217\u8868\n    def preprocess(self,pcm_data):\n        pcm_data_list=[]\n        # \u83b7\u53d6\u97f3\u9891\u6d41\u6570\u636e\n        for i in range(0, len(pcm_data), 2):\n            # \u6bcf\u4e24\u4e2a\u5b57\u8282\u7ec4\u7ec7\u6210\u4e00\u4e2a\u6709\u7b26\u53f7\u6574\u6570\uff0c\u7136\u540e\u5c06\u5176\u8f6c\u6362\u4e3a\u6d6e\u70b9\u6570\uff0c\u5373\u4e3a\u4e00\u6b21\u91c7\u6837\u7684\u6570\u636e\uff0c\u52a0\u5165\u5230\u5f53\u524d\u4e00\u5e27\uff080.3s\uff09\u7684\u6570\u636e\u5217\u8868\u4e2d\n            int_pcm_data = struct.unpack("<h", pcm_data[i:i+2])[0]\n            float_pcm_data = float(int_pcm_data)\n            pcm_data_list.append(float_pcm_data)\n        # \u5c06pcm\u6570\u636e\u5904\u7406\u4e3a\u6a21\u578b\u8f93\u5165\u7684\u7279\u5f81\u5411\u91cf\n        mp_feats = aidemo.kws_preprocess(fp, pcm_data_list)[0]\n        mp_feats_np = np.array(mp_feats).reshape((1, 30, 40))\n        audio_input_tensor = nn.from_numpy(mp_feats_np)\n        cache_input_tensor = nn.from_numpy(self.cache_np)\n        return [audio_input_tensor,cache_input_tensor]\n\n    # \u81ea\u5b9a\u4e49\u5f53\u524d\u4efb\u52a1\u7684\u540e\u5904\u7406\uff0cresults\u662f\u6a21\u578b\u8f93\u51faarray\u5217\u8868\n    def postprocess(self, results):\n        with ScopedTiming("postprocess", self.debug_mode > 0):\n            logits_np = results[0]\n            self.cache_np= results[1]\n            max_logits = np.max(logits_np, axis=1)[0]\n            max_p = np.max(max_logits)\n            idx = np.argmax(max_logits)\n            # \u5982\u679c\u5206\u6570\u5927\u4e8e\u9608\u503c\uff0c\u4e14idx==1(\u5373\u5305\u542b\u5524\u9192\u8bcd)\uff0c\u64ad\u653e\u56de\u590d\u97f3\u9891\n            if max_p > self.threshold and idx == 1:\n                return 1\n            else:\n                return 0\n\n\nif __name__ == "__main__":\n    os.exitpoint(os.EXITPOINT_ENABLE)\n    nn.shrink_memory_pool()\n    # \u8bbe\u7f6e\u6a21\u578b\u8def\u5f84\u548c\u5176\u4ed6\u53c2\u6570\n    kmodel_path = "/sdcard/app/tests/kmodel/kws.kmodel"\n    # \u5176\u5b83\u53c2\u6570\n    THRESH = 0.5                # \u68c0\u6d4b\u9608\u503c\n    SAMPLE_RATE = 16000         # \u91c7\u6837\u738716000Hz,\u5373\u6bcf\u79d2\u91c7\u683716000\u6b21\n    CHANNELS = 1                # \u901a\u9053\u6570 1\u4e3a\u5355\u58f0\u9053\uff0c2\u4e3a\u7acb\u4f53\u58f0\n    FORMAT = paInt16            # \u97f3\u9891\u8f93\u5165\u8f93\u51fa\u683c\u5f0f paInt16\n    CHUNK = int(0.3 * 16000)    # \u6bcf\u6b21\u8bfb\u53d6\u97f3\u9891\u6570\u636e\u7684\u5e27\u6570\uff0c\u8bbe\u7f6e\u4e3a0.3s\u7684\u5e27\u657016000*0.3=4800\n    reply_wav_file = "/sdcard/app/tests/utils/wozai.wav"         # kws\u5524\u9192\u8bcd\u56de\u590d\u97f3\u9891\u8def\u5f84\n\n    # \u521d\u59cb\u5316\u97f3\u9891\u9884\u5904\u7406\u63a5\u53e3\n    fp = aidemo.kws_fp_create()\n    # \u521d\u59cb\u5316\u97f3\u9891\u6d41\n    p = PyAudio()\n    p.initialize(CHUNK)\n    MediaManager.init()    #vb buffer\u521d\u59cb\u5316\n    # \u7528\u4e8e\u91c7\u96c6\u5b9e\u65f6\u97f3\u9891\u6570\u636e\n    input_stream = p.open(format=FORMAT,channels=CHANNELS,rate=SAMPLE_RATE,input=True,frames_per_buffer=CHUNK)\n    # \u7528\u4e8e\u64ad\u653e\u56de\u590d\u97f3\u9891\n    output_stream = p.open(format=FORMAT,channels=CHANNELS,rate=SAMPLE_RATE,output=True,frames_per_buffer=CHUNK)\n    # \u521d\u59cb\u5316\u81ea\u5b9a\u4e49\u5173\u952e\u8bcd\u5524\u9192\u5b9e\u4f8b\n    kws = KWSApp(kmodel_path,threshold=THRESH,debug_mode=0)\n\n    try:\n        while True:\n            os.exitpoint()                      # \u68c0\u67e5\u662f\u5426\u6709\u9000\u51fa\u4fe1\u53f7\n            with ScopedTiming("total",1):\n                pcm_data=input_stream.read()\n                res=kws.run(pcm_data)\n                if res:\n                    print("====Detected XiaonanXiaonan!====")\n                    wf = wave.open(reply_wav_file, "rb")\n                    wav_data = wf.read_frames(CHUNK)\n                    while wav_data:\n                        output_stream.write(wav_data)\n                        wav_data = wf.read_frames(CHUNK)\n                    time.sleep(1) # \u65f6\u95f4\u7f13\u51b2\uff0c\u7528\u4e8e\u64ad\u653e\u56de\u590d\u58f0\u97f3\n                    wf.close()\n                else:\n                    print("Deactivated!")\n                gc.collect()                    # \u5783\u573e\u56de\u6536\n    except Exception as e:\n        sys.print_exception(e)                  # \u6253\u5370\u5f02\u5e38\u4fe1\u606f\n    finally:\n        input_stream.stop_stream()\n        output_stream.stop_stream()\n        input_stream.close()\n        output_stream.close()\n        p.terminate()\n        MediaManager.deinit()              #\u91ca\u653evb buffer\n        aidemo.kws_fp_destroy(fp)\n        kws.deinit()                       # \u53cd\u521d\u59cb\u5316\n'})}),"\n",(0,i.jsx)(n.h3,{id:"217-\u8f66\u724c\u68c0\u6d4b",children:"2.17. \u8f66\u724c\u68c0\u6d4b"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:'from libs.PipeLine import PipeLine, ScopedTiming\nfrom libs.AIBase import AIBase\nfrom libs.AI2D import Ai2d\nimport os\nimport ujson\nfrom media.media import *\nfrom time import *\nimport nncase_runtime as nn\nimport ulab.numpy as np\nimport time\nimport utime\nimport image\nimport random\nimport gc\nimport sys\nimport aidemo\n\n# \u81ea\u5b9a\u4e49\u8f66\u724c\u68c0\u6d4b\u7c7b\nclass LicenceDetectionApp(AIBase):\n    # \u521d\u59cb\u5316\u51fd\u6570\uff0c\u8bbe\u7f6e\u8f66\u724c\u68c0\u6d4b\u5e94\u7528\u7684\u53c2\u6570\n    def __init__(self, kmodel_path, model_input_size, confidence_threshold=0.5, nms_threshold=0.2, rgb888p_size=[224,224], display_size=[1920,1080], debug_mode=0):\n        super().__init__(kmodel_path, model_input_size, rgb888p_size, debug_mode)  # \u8c03\u7528\u57fa\u7c7b\u7684\u521d\u59cb\u5316\u51fd\u6570\n        self.kmodel_path = kmodel_path  # \u6a21\u578b\u8def\u5f84\n        # \u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.model_input_size = model_input_size\n        # \u5206\u7c7b\u9608\u503c\n        self.confidence_threshold = confidence_threshold\n        self.nms_threshold = nms_threshold\n        # sensor\u7ed9\u5230AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\n        self.rgb888p_size = [ALIGN_UP(rgb888p_size[0], 16), rgb888p_size[1]]\n        # \u663e\u793a\u5206\u8fa8\u7387\n        self.display_size = [ALIGN_UP(display_size[0], 16), display_size[1]]\n        self.debug_mode = debug_mode\n        # Ai2d\u5b9e\u4f8b\uff0c\u7528\u4e8e\u5b9e\u73b0\u6a21\u578b\u9884\u5904\u7406\n        self.ai2d = Ai2d(debug_mode)\n        # \u8bbe\u7f6eAi2d\u7684\u8f93\u5165\u8f93\u51fa\u683c\u5f0f\u548c\u7c7b\u578b\n        self.ai2d.set_ai2d_dtype(nn.ai2d_format.NCHW_FMT, nn.ai2d_format.NCHW_FMT, np.uint8, np.uint8)\n\n    # \u914d\u7f6e\u9884\u5904\u7406\u64cd\u4f5c\uff0c\u8fd9\u91cc\u4f7f\u7528\u4e86pad\u548cresize\uff0cAi2d\u652f\u6301crop/shift/pad/resize/affine\n    def config_preprocess(self, input_image_size=None):\n        with ScopedTiming("set preprocess config", self.debug_mode > 0):\n            # \u521d\u59cb\u5316ai2d\u9884\u5904\u7406\u914d\u7f6e\uff0c\u9ed8\u8ba4\u4e3asensor\u7ed9\u5230AI\u7684\u5c3a\u5bf8\uff0c\u53ef\u4ee5\u901a\u8fc7\u8bbe\u7f6einput_image_size\u81ea\u884c\u4fee\u6539\u8f93\u5165\u5c3a\u5bf8\n            ai2d_input_size = input_image_size if input_image_size else self.rgb888p_size\n            self.ai2d.resize(nn.interp_method.tf_bilinear, nn.interp_mode.half_pixel)\n            self.ai2d.build([1,3,ai2d_input_size[1],ai2d_input_size[0]],[1,3,self.model_input_size[1],self.model_input_size[0]])\n\n    # \u81ea\u5b9a\u4e49\u5f53\u524d\u4efb\u52a1\u7684\u540e\u5904\u7406\n    def postprocess(self, results):\n        with ScopedTiming("postprocess", self.debug_mode > 0):\n            # \u5bf9\u68c0\u6d4b\u7ed3\u679c\u8fdb\u884c\u540e\u5904\u7406\n            det_res = aidemo.licence_det_postprocess(results, [self.rgb888p_size[1], self.rgb888p_size[0]], self.model_input_size, self.confidence_threshold, self.nms_threshold)\n            return det_res\n\n    # \u7ed8\u5236\u68c0\u6d4b\u7ed3\u679c\u5230\u5c4f\u5e55\u4e0a\n    def draw_result(self, pl, dets):\n        with ScopedTiming("display_draw", self.debug_mode > 0):\n            if dets:\n                pl.osd_img.clear()  # \u6e05\u9664\u5c4f\u5e55\n                point_8 = np.zeros((8), dtype=np.int16)\n                for det in dets:\n                    # \u5c06\u68c0\u6d4b\u6846\u5750\u6807\u4ecesensor\u56fe\u50cf\u5206\u8fa8\u7387\u8f6c\u6362\u4e3a\u663e\u793a\u5206\u8fa8\u7387\n                    for i in range(4):\n                        x = det[i * 2 + 0] / self.rgb888p_size[0] * self.display_size[0]\n                        y = det[i * 2 + 1] / self.rgb888p_size[1] * self.display_size[1]\n                        point_8[i * 2 + 0] = int(x)\n                        point_8[i * 2 + 1] = int(y)\n                    # \u5728\u5c4f\u5e55\u4e0a\u7ed8\u5236\u68c0\u6d4b\u6846\n                    for i in range(4):\n                        pl.osd_img.draw_line(point_8[i * 2 + 0], point_8[i * 2 + 1], point_8[(i + 1) % 4 * 2 + 0], point_8[(i + 1) % 4 * 2 + 1], color=(255, 0, 255, 0), thickness=4)\n            else:\n                pl.osd_img.clear()  # \u5982\u679c\u6ca1\u6709\u68c0\u6d4b\u7ed3\u679c\uff0c\u5219\u6e05\u7a7a\u5c4f\u5e55\n\nif __name__=="__main__":\n    # \u663e\u793a\u6a21\u5f0f\uff0c\u9ed8\u8ba4"hdmi",\u53ef\u4ee5\u9009\u62e9"hdmi"\u548c"lcd"\n    display_mode="hdmi"\n    if display_mode=="hdmi":\n        display_size=[1920,1080]\n    else:\n        display_size=[800,480]\n    # \u6a21\u578b\u8def\u5f84\n    kmodel_path="/sdcard/app/tests/kmodel/LPD_640.kmodel"\n    # \u5176\u5b83\u53c2\u6570\u8bbe\u7f6e\n    confidence_threshold = 0.2\n    nms_threshold = 0.2\n    rgb888p_size=[1920,1080]\n\n    # \u521d\u59cb\u5316PipeLine\n    pl=PipeLine(rgb888p_size=rgb888p_size,display_size=display_size,display_mode=display_mode)\n    pl.create()\n    # \u521d\u59cb\u5316\u81ea\u5b9a\u4e49\u8f66\u724c\u68c0\u6d4b\u5b9e\u4f8b\n    licence_det=LicenceDetectionApp(kmodel_path,model_input_size=[640,640],confidence_threshold=confidence_threshold,nms_threshold=nms_threshold,rgb888p_size=rgb888p_size,display_size=display_size,debug_mode=0)\n    licence_det.config_preprocess()\n    try:\n        while True:\n            os.exitpoint()\n            with ScopedTiming("total",1):\n                # \u83b7\u53d6\u5f53\u524d\u5e27\u6570\u636e\n                img=pl.get_frame()\n                # \u63a8\u7406\u5f53\u524d\u5e27\n                res=licence_det.run(img)\n                # \u7ed8\u5236\u7ed3\u679c\u5230PipeLine\u7684osd\u56fe\u50cf\n                licence_det.draw_result(pl,res)\n                # \u663e\u793a\u5f53\u524d\u7684\u7ed8\u5236\u7ed3\u679c\n                pl.show_image()\n                gc.collect()\n    except Exception as e:\n        sys.print_exception(e)\n    finally:\n        licence_det.deinit()\n        pl.destroy()\n'})}),"\n",(0,i.jsx)(n.h3,{id:"218-\u8f66\u724c\u8bc6\u522b",children:"2.18. \u8f66\u724c\u8bc6\u522b"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:'from libs.PipeLine import PipeLine, ScopedTiming\nfrom libs.AIBase import AIBase\nfrom libs.AI2D import Ai2d\nimport os\nimport ujson\nfrom media.media import *\nfrom time import *\nimport nncase_runtime as nn\nimport ulab.numpy as np\nimport time\nimport image\nimport aidemo\nimport random\nimport gc\nimport sys\n\n# \u81ea\u5b9a\u4e49\u8f66\u724c\u68c0\u6d4b\u7c7b\nclass LicenceDetectionApp(AIBase):\n    # \u521d\u59cb\u5316\u51fd\u6570\uff0c\u8bbe\u7f6e\u8f66\u724c\u68c0\u6d4b\u5e94\u7528\u7684\u53c2\u6570\n    def __init__(self, kmodel_path, model_input_size, confidence_threshold=0.5, nms_threshold=0.2, rgb888p_size=[224,224], display_size=[1920,1080], debug_mode=0):\n        super().__init__(kmodel_path, model_input_size, rgb888p_size, debug_mode)  # \u8c03\u7528\u57fa\u7c7b\u7684\u521d\u59cb\u5316\u51fd\u6570\n        self.kmodel_path = kmodel_path  # \u6a21\u578b\u8def\u5f84\n        # \u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.model_input_size = model_input_size\n        # \u5206\u7c7b\u9608\u503c\n        self.confidence_threshold = confidence_threshold\n        self.nms_threshold = nms_threshold\n        # sensor\u7ed9\u5230AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\n        self.rgb888p_size = [ALIGN_UP(rgb888p_size[0], 16), rgb888p_size[1]]\n        # \u663e\u793a\u5206\u8fa8\u7387\n        self.display_size = [ALIGN_UP(display_size[0], 16), display_size[1]]\n        self.debug_mode = debug_mode\n        # Ai2d\u5b9e\u4f8b\uff0c\u7528\u4e8e\u5b9e\u73b0\u6a21\u578b\u9884\u5904\u7406\n        self.ai2d = Ai2d(debug_mode)\n        # \u8bbe\u7f6eAi2d\u7684\u8f93\u5165\u8f93\u51fa\u683c\u5f0f\u548c\u7c7b\u578b\n        self.ai2d.set_ai2d_dtype(nn.ai2d_format.NCHW_FMT, nn.ai2d_format.NCHW_FMT, np.uint8, np.uint8)\n\n    # \u914d\u7f6e\u9884\u5904\u7406\u64cd\u4f5c\uff0c\u8fd9\u91cc\u4f7f\u7528\u4e86pad\u548cresize\uff0cAi2d\u652f\u6301crop/shift/pad/resize/affine\n    def config_preprocess(self, input_image_size=None):\n        with ScopedTiming("set preprocess config", self.debug_mode > 0):\n            # \u521d\u59cb\u5316ai2d\u9884\u5904\u7406\u914d\u7f6e\uff0c\u9ed8\u8ba4\u4e3asensor\u7ed9\u5230AI\u7684\u5c3a\u5bf8\uff0c\u53ef\u4ee5\u901a\u8fc7\u8bbe\u7f6einput_image_size\u81ea\u884c\u4fee\u6539\u8f93\u5165\u5c3a\u5bf8\n            ai2d_input_size = input_image_size if input_image_size else self.rgb888p_size\n            self.ai2d.resize(nn.interp_method.tf_bilinear, nn.interp_mode.half_pixel)\n            self.ai2d.build([1,3,ai2d_input_size[1],ai2d_input_size[0]],[1,3,self.model_input_size[1],self.model_input_size[0]])\n\n    # \u81ea\u5b9a\u4e49\u5f53\u524d\u4efb\u52a1\u7684\u540e\u5904\u7406\n    def postprocess(self, results):\n        with ScopedTiming("postprocess", self.debug_mode > 0):\n            # \u5bf9\u68c0\u6d4b\u7ed3\u679c\u8fdb\u884c\u540e\u5904\u7406\n            det_res = aidemo.licence_det_postprocess(results, [self.rgb888p_size[1], self.rgb888p_size[0]], self.model_input_size, self.confidence_threshold, self.nms_threshold)\n            return det_res\n\n# \u81ea\u5b9a\u4e49\u8f66\u724c\u8bc6\u522b\u4efb\u52a1\u7c7b\nclass LicenceRecognitionApp(AIBase):\n    def __init__(self,kmodel_path,model_input_size,rgb888p_size=[1920,1080],display_size=[1920,1080],debug_mode=0):\n        super().__init__(kmodel_path,model_input_size,rgb888p_size,debug_mode)\n        # kmodel\u8def\u5f84\n        self.kmodel_path=kmodel_path\n        # \u68c0\u6d4b\u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.model_input_size=model_input_size\n        # sensor\u7ed9\u5230AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.rgb888p_size=[ALIGN_UP(rgb888p_size[0],16),rgb888p_size[1]]\n        # \u89c6\u9891\u8f93\u51faVO\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.display_size=[ALIGN_UP(display_size[0],16),display_size[1]]\n        # debug\u6a21\u5f0f\n        self.debug_mode=debug_mode\n        # \u8f66\u724c\u5b57\u7b26\u5b57\u5178\n        self.dict_rec = ["\u6302", "\u4f7f", "\u9886", "\u6fb3", "\u6e2f", "\u7696", "\u6caa", "\u6d25", "\u6e1d", "\u5180", "\u664b", "\u8499", "\u8fbd", "\u5409", "\u9ed1", "\u82cf", "\u6d59", "\u4eac", "\u95fd", "\u8d63", "\u9c81", "\u8c6b", "\u9102", "\u6e58", "\u7ca4", "\u6842", "\u743c", "\u5ddd", "\u8d35", "\u4e91", "\u85cf", "\u9655", "\u7518", "\u9752", "\u5b81", "\u65b0", "\u8b66", "\u5b66", "0", "1", "2", "3", "4", "5", "6", "7", "8", "9", "A", "B", "C", "D", "E", "F", "G", "H", "J", "K", "L", "M", "N", "P", "Q", "R", "S", "T", "U", "V", "W", "X", "Y", "Z", "_", "-"]\n        self.dict_size = len(self.dict_rec)\n        self.ai2d=Ai2d(debug_mode)\n        self.ai2d.set_ai2d_dtype(nn.ai2d_format.NCHW_FMT,nn.ai2d_format.NCHW_FMT,np.uint8, np.uint8)\n\n    # \u914d\u7f6e\u9884\u5904\u7406\u64cd\u4f5c\uff0c\u8fd9\u91cc\u4f7f\u7528\u4e86resize\uff0cAi2d\u652f\u6301crop/shift/pad/resize/affine\n    def config_preprocess(self,input_image_size=None):\n        with ScopedTiming("set preprocess config",self.debug_mode > 0):\n            ai2d_input_size=input_image_size if input_image_size else self.rgb888p_size\n            self.ai2d.resize(nn.interp_method.tf_bilinear, nn.interp_mode.half_pixel)\n            self.ai2d.build([1,3,ai2d_input_size[1],ai2d_input_size[0]],[1,3,self.model_input_size[1],self.model_input_size[0]])\n\n    # \u81ea\u5b9a\u4e49\u540e\u5904\u7406\uff0cresults\u662f\u6a21\u578b\u8f93\u51fa\u7684array\u5217\u8868\n    def postprocess(self,results):\n        with ScopedTiming("postprocess",self.debug_mode > 0):\n            output_data=results[0].reshape((-1,self.dict_size))\n            max_indices = np.argmax(output_data, axis=1)\n            result_str = ""\n            for i in range(max_indices.shape[0]):\n                index = max_indices[i]\n                if index > 0 and (i == 0 or index != max_indices[i - 1]):\n                    result_str += self.dict_rec[index - 1]\n            return result_str\n\n# \u8f66\u724c\u8bc6\u522b\u4efb\u52a1\u7c7b\nclass LicenceRec:\n    def __init__(self,licence_det_kmodel,licence_rec_kmodel,det_input_size,rec_input_size,confidence_threshold=0.25,nms_threshold=0.3,rgb888p_size=[1920,1080],display_size=[1920,1080],debug_mode=0):\n        # \u8f66\u724c\u68c0\u6d4b\u6a21\u578b\u8def\u5f84\n        self.licence_det_kmodel=licence_det_kmodel\n        # \u8f66\u724c\u8bc6\u522b\u6a21\u578b\u8def\u5f84\n        self.licence_rec_kmodel=licence_rec_kmodel\n        # \u4eba\u8138\u68c0\u6d4b\u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.det_input_size=det_input_size\n        # \u4eba\u8138\u59ff\u6001\u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.rec_input_size=rec_input_size\n        # \u7f6e\u4fe1\u5ea6\u9608\u503c\n        self.confidence_threshold=confidence_threshold\n        # nms\u9608\u503c\n        self.nms_threshold=nms_threshold\n        # sensor\u7ed9\u5230AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.rgb888p_size=[ALIGN_UP(rgb888p_size[0],16),rgb888p_size[1]]\n        # \u89c6\u9891\u8f93\u51faVO\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.display_size=[ALIGN_UP(display_size[0],16),display_size[1]]\n        # debug_mode\u6a21\u5f0f\n        self.debug_mode=debug_mode\n        self.licence_det=LicenceDetectionApp(self.licence_det_kmodel,model_input_size=self.det_input_size,confidence_threshold=self.confidence_threshold,nms_threshold=self.nms_threshold,rgb888p_size=self.rgb888p_size,display_size=self.display_size,debug_mode=0)\n        self.licence_rec=LicenceRecognitionApp(self.licence_rec_kmodel,model_input_size=self.rec_input_size,rgb888p_size=self.rgb888p_size)\n        self.licence_det.config_preprocess()\n\n    # run\u51fd\u6570\n    def run(self,input_np):\n        # \u6267\u884c\u8f66\u724c\u68c0\u6d4b\n        det_boxes=self.licence_det.run(input_np)\n        # \u5c06\u8f66\u724c\u90e8\u5206\u62a0\u51fa\u6765\n        imgs_array_boxes = aidemo.ocr_rec_preprocess(input_np,[self.rgb888p_size[1],self.rgb888p_size[0]],det_boxes)\n        imgs_array = imgs_array_boxes[0]\n        boxes = imgs_array_boxes[1]\n        rec_res = []\n        for img_array in imgs_array:\n            # \u5bf9\u6bcf\u4e00\u4e2a\u68c0\u6d4b\u5230\u7684\u8f66\u724c\u8fdb\u884c\u8bc6\u522b\n            self.licence_rec.config_preprocess(input_image_size=[img_array.shape[3],img_array.shape[2]])\n            licence_str=self.licence_rec.run(img_array)\n            rec_res.append(licence_str)\n            gc.collect()\n        return det_boxes,rec_res\n\n    # \u7ed8\u5236\u8f66\u724c\u68c0\u6d4b\u8bc6\u522b\u6548\u679c\n    def draw_result(self,pl,det_res,rec_res):\n        pl.osd_img.clear()\n        if det_res:\n            point_8 = np.zeros((8),dtype=np.int16)\n            for det_index in range(len(det_res)):\n                for i in range(4):\n                    x = det_res[det_index][i * 2 + 0]/self.rgb888p_size[0]*self.display_size[0]\n                    y = det_res[det_index][i * 2 + 1]/self.rgb888p_size[1]*self.display_size[1]\n                    point_8[i * 2 + 0] = int(x)\n                    point_8[i * 2 + 1] = int(y)\n                for i in range(4):\n                    pl.osd_img.draw_line(point_8[i * 2 + 0],point_8[i * 2 + 1],point_8[(i+1) % 4 * 2 + 0],point_8[(i+1) % 4 * 2 + 1],color=(255, 0, 255, 0),thickness=4)\n                pl.osd_img.draw_string_advanced( point_8[6], point_8[7] + 20, 40,rec_res[det_index] , color=(255,255,153,18))\n\n\nif __name__=="__main__":\n    # \u663e\u793a\u6a21\u5f0f\uff0c\u9ed8\u8ba4"hdmi",\u53ef\u4ee5\u9009\u62e9"hdmi"\u548c"lcd"\n    display_mode="hdmi"\n    if display_mode=="hdmi":\n        display_size=[1920,1080]\n    else:\n        display_size=[800,480]\n    # \u8f66\u724c\u68c0\u6d4b\u6a21\u578b\u8def\u5f84\n    licence_det_kmodel_path="/sdcard/app/tests/kmodel/LPD_640.kmodel"\n    # \u8f66\u724c\u8bc6\u522b\u6a21\u578b\u8def\u5f84\n    licence_rec_kmodel_path="/sdcard/app/tests/kmodel/licence_reco.kmodel"\n    # \u5176\u5b83\u53c2\u6570\n    rgb888p_size=[640,360]\n    licence_det_input_size=[640,640]\n    licence_rec_input_size=[220,32]\n    confidence_threshold=0.2\n    nms_threshold=0.2\n\n    # \u521d\u59cb\u5316PipeLine\uff0c\u53ea\u5173\u6ce8\u4f20\u7ed9AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u663e\u793a\u7684\u5206\u8fa8\u7387\n    pl=PipeLine(rgb888p_size=rgb888p_size,display_size=display_size,display_mode=display_mode)\n    pl.create()\n    lr=LicenceRec(licence_det_kmodel_path,licence_rec_kmodel_path,det_input_size=licence_det_input_size,rec_input_size=licence_rec_input_size,confidence_threshold=confidence_threshold,nms_threshold=nms_threshold,rgb888p_size=rgb888p_size,display_size=display_size)\n    try:\n        while True:\n            os.exitpoint()\n            with ScopedTiming("total",1):\n                img=pl.get_frame()                  # \u83b7\u53d6\u5f53\u524d\u5e27\n                det_res,rec_res=lr.run(img)         # \u63a8\u7406\u5f53\u524d\u5e27\n                lr.draw_result(pl,det_res,rec_res)  # \u7ed8\u5236\u5f53\u524d\u5e27\u63a8\u7406\u7ed3\u679c\n                pl.show_image()                     # \u5c55\u793a\u63a8\u7406\u7ed3\u679c\n                gc.collect()\n    except Exception as e:\n        sys.print_exception(e)\n    finally:\n        lr.licence_det.deinit()\n        lr.licence_rec.deinit()\n        pl.destroy()\n'})}),"\n",(0,i.jsx)(n.h3,{id:"219-\u5355\u76ee\u6807\u8ddf\u8e2a",children:"2.19. \u5355\u76ee\u6807\u8ddf\u8e2a"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:'from libs.PipeLine import PipeLine, ScopedTiming\nfrom libs.AIBase import AIBase\nfrom libs.AI2D import Ai2d\nfrom random import randint\nimport os\nimport ujson\nfrom media.media import *\nfrom time import *\nimport nncase_runtime as nn\nimport ulab.numpy as np\nimport time\nimport image\nimport aidemo\nimport random\nimport gc\nimport sys\n\n# \u81ea\u5b9a\u4e49\u8ddf\u8e2a\u6a21\u7248\u4efb\u52a1\u7c7b\nclass TrackCropApp(AIBase):\n    def __init__(self,kmodel_path,model_input_size,ratio_src_crop,center_xy_wh,rgb888p_size=[1280,720],display_size=[1920,1080],debug_mode=0):\n        super().__init__(kmodel_path,model_input_size,rgb888p_size,debug_mode)\n        # kmodel\u8def\u5f84\n        self.kmodel_path=kmodel_path\n        # \u8ddf\u8e2a\u6a21\u677f\u8f93\u5165\u5206\u8fa8\u7387\n        self.model_input_size=model_input_size\n        # sensor\u7ed9\u5230AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.rgb888p_size=[ALIGN_UP(rgb888p_size[0],16),rgb888p_size[1]]\n        # \u89c6\u9891\u8f93\u51faVO\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.display_size=[ALIGN_UP(display_size[0],16),display_size[1]]\n        # debug\u6a21\u5f0f\n        self.debug_mode=debug_mode\n        # \u8ddf\u8e2a\u6846\u5bbd\u3001\u9ad8\u8c03\u6574\u7cfb\u6570\n        self.CONTEXT_AMOUNT = 0.5\n        #src\u6a21\u578b\u548ccrop\u6a21\u578b\u8f93\u5165\u6bd4\u503c\n        self.ratio_src_crop = ratio_src_crop\n        self.center_xy_wh=center_xy_wh\n        # padding\u548ccrop\u53c2\u6570\n        self.pad_crop_params=[]\n        # \u6ce8\u610f\uff1aai2d\u8bbe\u7f6e\u591a\u4e2a\u9884\u5904\u7406\u65f6\u6267\u884c\u7684\u987a\u5e8f\u4e3a\uff1acrop->shift->resize/affine->pad\uff0c\u5982\u679c\u4e0d\u7b26\u5408\u8be5\u987a\u5e8f\uff0c\u9700\u8981\u914d\u7f6e\u591a\u4e2aai2d\u5bf9\u8c61;\n        # \u5982\u4e0b\u6a21\u578b\u9884\u5904\u7406\u8981\u5148\u505aresize+padding\u518d\u505aresize+crop\uff0c\u56e0\u6b64\u8981\u914d\u7f6e\u4e24\u4e2aAi2d\u5bf9\u8c61\n        self.ai2d_pad=Ai2d(debug_mode)\n        self.ai2d_pad.set_ai2d_dtype(nn.ai2d_format.NCHW_FMT,nn.ai2d_format.NCHW_FMT,np.uint8, np.uint8)\n        self.ai2d_crop=Ai2d(debug_mode)\n        self.ai2d_crop.set_ai2d_dtype(nn.ai2d_format.NCHW_FMT,nn.ai2d_format.NCHW_FMT,np.uint8, np.uint8)\n        self.need_pad=False\n\n    # \u914d\u7f6e\u9884\u5904\u7406\u64cd\u4f5c\uff0c\u8fd9\u91cc\u4f7f\u7528\u4e86crop\u3001pad\u548cresize\uff0cAi2d\u652f\u6301crop/shift/pad/resize/affine\uff0c\u5177\u4f53\u4ee3\u7801\u8bf7\u6253\u5f00/sdcard/app/libs/AI2D.py\u67e5\u770b\n    def config_preprocess(self,input_image_size=None):\n        with ScopedTiming("set preprocess config",self.debug_mode > 0):\n            # \u521d\u59cb\u5316ai2d\u9884\u5904\u7406\u914d\u7f6e\uff0c\u9ed8\u8ba4\u4e3asensor\u7ed9\u5230AI\u7684\u5c3a\u5bf8\uff0c\u53ef\u4ee5\u901a\u8fc7\u8bbe\u7f6einput_image_size\u81ea\u884c\u4fee\u6539\u8f93\u5165\u5c3a\u5bf8\n            ai2d_input_size = input_image_size if input_image_size else self.rgb888p_size\n            # \u8ba1\u7b97padding\u53c2\u6570\u5e76\u5e94\u7528pad\u64cd\u4f5c\uff0c\u4ee5\u786e\u4fdd\u8f93\u5165\u56fe\u50cf\u5c3a\u5bf8\u4e0e\u6a21\u578b\u8f93\u5165\u5c3a\u5bf8\u5339\u914d\n            self.pad_crop_params= self.get_padding_crop_param()\n            # \u5982\u679c\u9700\u8981padding,\u914d\u7f6epadding\u90e8\u5206\uff0c\u5426\u5219\u53ea\u8d70crop\n            if (self.pad_crop_params[0] != 0 or self.pad_crop_params[1] != 0 or self.pad_crop_params[2] != 0 or self.pad_crop_params[3] != 0):\n                self.need_pad=True\n                self.ai2d_pad.resize(nn.interp_method.tf_bilinear, nn.interp_mode.half_pixel)\n                self.ai2d_pad.pad([0, 0, 0, 0, self.pad_crop_params[0], self.pad_crop_params[1], self.pad_crop_params[2], self.pad_crop_params[3]], 0, [114, 114, 114])\n                output_size=[self.rgb888p_size[0]+self.pad_crop_params[2]+self.pad_crop_params[3],self.rgb888p_size[1]+self.pad_crop_params[0]+self.pad_crop_params[1]]\n                self.ai2d_pad.build([1,3,ai2d_input_size[1],ai2d_input_size[0]],[1,3,output_size[1],output_size[0]])\n\n                self.ai2d_crop.resize(nn.interp_method.tf_bilinear, nn.interp_mode.half_pixel)\n                self.ai2d_crop.crop(int(self.pad_crop_params[4]),int(self.pad_crop_params[6]),int(self.pad_crop_params[5]-self.pad_crop_params[4]+1),int(self.pad_crop_params[7]-self.pad_crop_params[6]+1))\n                self.ai2d_crop.build([1,3,output_size[1],output_size[0]],[1,3,self.model_input_size[1],self.model_input_size[0]])\n            else:\n                self.need_pad=False\n                self.ai2d_crop.resize(nn.interp_method.tf_bilinear, nn.interp_mode.half_pixel)\n                self.ai2d_crop.crop(int(self.center_xy_wh[0]-self.pad_crop_params[8]/2.0),int(self.center_xy_wh[1]-self.pad_crop_params[8]/2.0),int(self.pad_crop_params[8]),int(self.pad_crop_params[8]))\n                self.ai2d_crop.build([1,3,ai2d_input_size[1],ai2d_input_size[0]],[1,3,self.model_input_size[1],self.model_input_size[0]])\n\n    # \u91cd\u5199\u9884\u5904\u7406\u51fd\u6570preprocess\uff0c\u56e0\u4e3a\u8be5\u90e8\u5206\u4e0d\u662f\u5355\u7eaf\u7684\u8d70\u4e00\u4e2aai2d\u505a\u9884\u5904\u7406\uff0c\u6240\u4ee5\u8be5\u51fd\u6570\u9700\u8981\u91cd\u5199\n    def preprocess(self,input_np):\n        if self.need_pad:\n            pad_output=self.ai2d_pad.run(input_np).to_numpy()\n            return [self.ai2d_crop.run(pad_output)]\n        else:\n            return [self.ai2d_crop.run(input_np)]\n\n    # \u81ea\u5b9a\u4e49\u540e\u5904\u7406\uff0cresults\u662f\u6a21\u578b\u8f93\u51faarray\u7684\u5217\u8868\n    def postprocess(self,results):\n        with ScopedTiming("postprocess",self.debug_mode > 0):\n            return results[0]\n\n    # \u8ba1\u7b97padding\u548ccrop\u53c2\u6570\n    def get_padding_crop_param(self):\n        s_z = round(np.sqrt((self.center_xy_wh[2] + self.CONTEXT_AMOUNT * (self.center_xy_wh[2] + self.center_xy_wh[3])) * (self.center_xy_wh[3] + self.CONTEXT_AMOUNT * (self.center_xy_wh[2] + self.center_xy_wh[3]))))\n        c = (s_z + 1) / 2\n        context_xmin = np.floor(self.center_xy_wh[0] - c + 0.5)\n        context_xmax = int(context_xmin + s_z - 1)\n        context_ymin = np.floor(self.center_xy_wh[1] - c + 0.5)\n        context_ymax = int(context_ymin + s_z - 1)\n        left_pad = int(max(0, -context_xmin))\n        top_pad = int(max(0, -context_ymin))\n        right_pad = int(max(0, int(context_xmax - self.rgb888p_size[0] + 1)))\n        bottom_pad = int(max(0, int(context_ymax - self.rgb888p_size[1] + 1)))\n        context_xmin = context_xmin + left_pad\n        context_xmax = context_xmax + left_pad\n        context_ymin = context_ymin + top_pad\n        context_ymax = context_ymax + top_pad\n        return [top_pad,bottom_pad,left_pad,right_pad,context_xmin,context_xmax,context_ymin,context_ymax,s_z]\n\n    #\u91cd\u5199deinit\n    def deinit(self):\n        with ScopedTiming("deinit",self.debug_mode > 0):\n            del self.ai2d_pad\n            del self.ai2d_crop\n            super().deinit()\n\n# \u81ea\u5b9a\u4e49\u8ddf\u8e2a\u5b9e\u65f6\u4efb\u52a1\u7c7b\nclass TrackSrcApp(AIBase):\n    def __init__(self,kmodel_path,model_input_size,ratio_src_crop,rgb888p_size=[1280,720],display_size=[1920,1080],debug_mode=0):\n        super().__init__(kmodel_path,model_input_size,rgb888p_size,debug_mode)\n        # kmodel\u8def\u5f84\n        self.kmodel_path=kmodel_path\n        # \u68c0\u6d4b\u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.model_input_size=model_input_size\n        # sensor\u7ed9\u5230AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.rgb888p_size=[ALIGN_UP(rgb888p_size[0],16),rgb888p_size[1]]\n        # \u89c6\u9891\u8f93\u51faVO\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.display_size=[ALIGN_UP(display_size[0],16),display_size[1]]\n        # padding\u548ccrop\u53c2\u6570\u5217\u8868\n        self.pad_crop_params=[]\n        # \u8ddf\u8e2a\u6846\u5bbd\u3001\u9ad8\u8c03\u6574\u7cfb\u6570\n        self.CONTEXT_AMOUNT = 0.5\n        # src\u548ccrop\u6a21\u578b\u7684\u8f93\u5165\u5c3a\u5bf8\u6bd4\u4f8b\n        self.ratio_src_crop = ratio_src_crop\n        # debug\u6a21\u5f0f\n        self.debug_mode=debug_mode\n        # \u6ce8\u610f\uff1aai2d\u8bbe\u7f6e\u591a\u4e2a\u9884\u5904\u7406\u65f6\u6267\u884c\u7684\u987a\u5e8f\u4e3a\uff1acrop->shift->resize/affine->pad\uff0c\u5982\u679c\u4e0d\u7b26\u5408\u8be5\u987a\u5e8f\uff0c\u9700\u8981\u914d\u7f6e\u591a\u4e2aai2d\u5bf9\u8c61;\n        # \u5982\u4e0b\u6a21\u578b\u9884\u5904\u7406\u8981\u5148\u505aresize+padding\u518d\u505aresize+crop\uff0c\u56e0\u6b64\u8981\u914d\u7f6e\u4e24\u4e2aAi2d\u5bf9\u8c61\n        self.ai2d_pad=Ai2d(debug_mode)\n        self.ai2d_pad.set_ai2d_dtype(nn.ai2d_format.NCHW_FMT,nn.ai2d_format.NCHW_FMT,np.uint8, np.uint8)\n        self.ai2d_crop=Ai2d(debug_mode)\n        self.ai2d_crop.set_ai2d_dtype(nn.ai2d_format.NCHW_FMT,nn.ai2d_format.NCHW_FMT,np.uint8, np.uint8)\n        self.need_pad=False\n\n    # \u914d\u7f6e\u9884\u5904\u7406\u64cd\u4f5c\uff0c\u8fd9\u91cc\u4f7f\u7528\u4e86crop\u3001pad\u548cresize\uff0cAi2d\u652f\u6301crop/shift/pad/resize/affine\uff0c\u5177\u4f53\u4ee3\u7801\u8bf7\u6253\u5f00/sdcard/app/libs/AI2D.py\u67e5\u770b\n    def config_preprocess(self,center_xy_wh,input_image_size=None):\n        with ScopedTiming("set preprocess config",self.debug_mode > 0):\n            # \u521d\u59cb\u5316ai2d\u9884\u5904\u7406\u914d\u7f6e\uff0c\u9ed8\u8ba4\u4e3asensor\u7ed9\u5230AI\u7684\u5c3a\u5bf8\uff0c\u53ef\u4ee5\u901a\u8fc7\u8bbe\u7f6einput_image_size\u81ea\u884c\u4fee\u6539\u8f93\u5165\u5c3a\u5bf8\n            ai2d_input_size=input_image_size if input_image_size else self.rgb888p_size\n            # \u8ba1\u7b97padding\u53c2\u6570\u5e76\u5e94\u7528pad\u64cd\u4f5c\uff0c\u4ee5\u786e\u4fdd\u8f93\u5165\u56fe\u50cf\u5c3a\u5bf8\u4e0e\u6a21\u578b\u8f93\u5165\u5c3a\u5bf8\u5339\u914d\n            self.pad_crop_params= self.get_padding_crop_param(center_xy_wh)\n            # \u5982\u679c\u9700\u8981padding,\u914d\u7f6epadding\u90e8\u5206\uff0c\u5426\u5219\u53ea\u8d70crop\n            if (self.pad_crop_params[0] != 0 or self.pad_crop_params[1] != 0 or self.pad_crop_params[2] != 0 or self.pad_crop_params[3] != 0):\n                self.need_pad=True\n                self.ai2d_pad.resize(nn.interp_method.tf_bilinear, nn.interp_mode.half_pixel)\n                self.ai2d_pad.pad([0, 0, 0, 0, self.pad_crop_params[0], self.pad_crop_params[1], self.pad_crop_params[2], self.pad_crop_params[3]], 0, [114, 114, 114])\n                output_size=[self.rgb888p_size[0]+self.pad_crop_params[2]+self.pad_crop_params[3],self.rgb888p_size[1]+self.pad_crop_params[0]+self.pad_crop_params[1]]\n\n                self.ai2d_pad.build([1,3,ai2d_input_size[1],ai2d_input_size[0]],[1,3,output_size[1],output_size[0]])\n                self.ai2d_crop.resize(nn.interp_method.tf_bilinear, nn.interp_mode.half_pixel)\n                self.ai2d_crop.crop(int(self.pad_crop_params[4]),int(self.pad_crop_params[6]),int(self.pad_crop_params[5]-self.pad_crop_params[4]+1),int(self.pad_crop_params[7]-self.pad_crop_params[6]+1))\n                self.ai2d_crop.build([1,3,output_size[1],output_size[0]],[1,3,self.model_input_size[1],self.model_input_size[0]])\n            else:\n                self.need_pad=False\n                self.ai2d_crop.resize(nn.interp_method.tf_bilinear, nn.interp_mode.half_pixel)\n                self.ai2d_crop.crop(int(center_xy_wh[0]-self.pad_crop_params[8]/2.0),int(center_xy_wh[1]-self.pad_crop_params[8]/2.0),int(self.pad_crop_params[8]),int(self.pad_crop_params[8]))\n                self.ai2d_crop.build([1,3,ai2d_input_size[1],ai2d_input_size[0]],[1,3,self.model_input_size[1],self.model_input_size[0]])\n\n    # \u91cd\u5199\u9884\u5904\u7406\u51fd\u6570preprocess\uff0c\u56e0\u4e3a\u8be5\u90e8\u5206\u4e0d\u662f\u5355\u7eaf\u7684\u8d70\u4e00\u4e2aai2d\u505a\u9884\u5904\u7406\uff0c\u6240\u4ee5\u8be5\u51fd\u6570\u9700\u8981\u91cd\u5199\n    def preprocess(self,input_np):\n        with ScopedTiming("preprocess",self.debug_mode>0):\n            if self.need_pad:\n                pad_output=self.ai2d_pad.run(input_np).to_numpy()\n                return [self.ai2d_crop.run(pad_output)]\n            else:\n                return [self.ai2d_crop.run(input_np)]\n\n    # \u81ea\u5b9a\u4e49\u540e\u5904\u7406\uff0cresults\u662f\u6a21\u578b\u8f93\u51faarray\u7684\u5217\u8868\n    def postprocess(self,results):\n        with ScopedTiming("postprocess",self.debug_mode > 0):\n            return results[0]\n\n    # \u8ba1\u7b97padding\u548ccrop\u53c2\u6570\n    def get_padding_crop_param(self,center_xy_wh):\n        s_z = round(np.sqrt((center_xy_wh[2] + self.CONTEXT_AMOUNT * (center_xy_wh[2] + center_xy_wh[3])) * (center_xy_wh[3] + self.CONTEXT_AMOUNT * (center_xy_wh[2] + center_xy_wh[3])))) * self.ratio_src_crop\n        c = (s_z + 1) / 2\n        context_xmin = np.floor(center_xy_wh[0] - c + 0.5)\n        context_xmax = int(context_xmin + s_z - 1)\n        context_ymin = np.floor(center_xy_wh[1] - c + 0.5)\n        context_ymax = int(context_ymin + s_z - 1)\n        left_pad = int(max(0, -context_xmin))\n        top_pad = int(max(0, -context_ymin))\n        right_pad = int(max(0, int(context_xmax - self.rgb888p_size[0] + 1)))\n        bottom_pad = int(max(0, int(context_ymax - self.rgb888p_size[1] + 1)))\n        context_xmin = context_xmin + left_pad\n        context_xmax = context_xmax + left_pad\n        context_ymin = context_ymin + top_pad\n        context_ymax = context_ymax + top_pad\n        return [top_pad,bottom_pad,left_pad,right_pad,context_xmin,context_xmax,context_ymin,context_ymax,s_z]\n\n    # \u91cd\u5199deinit\n    def deinit(self):\n        with ScopedTiming("deinit",self.debug_mode > 0):\n            del self.ai2d_pad\n            del self.ai2d_crop\n            super().deinit()\n\n\nclass TrackerApp(AIBase):\n    def __init__(self,kmodel_path,crop_input_size,thresh,rgb888p_size=[1280,720],display_size=[1920,1080],debug_mode=0):\n        super().__init__(kmodel_path,rgb888p_size,debug_mode)\n        # kmodel\u8def\u5f84\n        self.kmodel_path=kmodel_path\n        # crop\u6a21\u578b\u7684\u8f93\u5165\u5c3a\u5bf8\n        self.crop_input_size=crop_input_size\n        # \u8ddf\u8e2a\u6846\u9608\u503c\n        self.thresh=thresh\n        # \u8ddf\u8e2a\u6846\u5bbd\u3001\u9ad8\u8c03\u6574\u7cfb\u6570\n        self.CONTEXT_AMOUNT = 0.5\n        # sensor\u7ed9\u5230AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.rgb888p_size=[ALIGN_UP(rgb888p_size[0],16),rgb888p_size[1]]\n        # \u89c6\u9891\u8f93\u51faVO\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.display_size=[ALIGN_UP(display_size[0],16),display_size[1]]\n        # debug\u6a21\u5f0f\n        self.debug_mode=debug_mode\n        self.ai2d=Ai2d(debug_mode)\n        self.ai2d.set_ai2d_dtype(nn.ai2d_format.NCHW_FMT,nn.ai2d_format.NCHW_FMT,np.uint8, np.uint8)\n\n    def config_preprocess(self,input_image_size=None):\n        with ScopedTiming("set preprocess config",self.debug_mode > 0):\n            pass\n\n    # \u91cd\u5199run\u51fd\u6570\uff0c\u56e0\u4e3a\u6ca1\u6709\u9884\u5904\u7406\u8fc7\u7a0b\uff0c\u6240\u4ee5\u539f\u6765run\u64cd\u4f5c\u4e2d\u5305\u542b\u7684preprocess->inference->postprocess\u4e0d\u5408\u9002\uff0c\u8fd9\u91cc\u53ea\u5305\u542binference->postprocess\n    def run(self,input_np_1,input_np_2,center_xy_wh):\n        input_tensors=[]\n        input_tensors.append(nn.from_numpy(input_np_1))\n        input_tensors.append(nn.from_numpy(input_np_2))\n        results=self.inference(input_tensors)\n        return self.postprocess(results,center_xy_wh)\n\n\n    # \u81ea\u5b9a\u4e49\u540e\u5904\u7406\uff0cresults\u662f\u6a21\u578b\u8f93\u51faarray\u7684\u5217\u8868,\u8fd9\u91cc\u4f7f\u7528\u4e86aidemo\u7684nanotracker_postprocess\u5217\u8868\n    def postprocess(self,results,center_xy_wh):\n        with ScopedTiming("postprocess",self.debug_mode > 0):\n            det = aidemo.nanotracker_postprocess(results[0],results[1],[self.rgb888p_size[1],self.rgb888p_size[0]],self.thresh,center_xy_wh,self.crop_input_size[0],self.CONTEXT_AMOUNT)\n            return det\n\nclass NanoTracker:\n    def __init__(self,track_crop_kmodel,track_src_kmodel,tracker_kmodel,crop_input_size,src_input_size,threshold=0.25,rgb888p_size=[1280,720],display_size=[1920,1080],debug_mode=0):\n        # \u8ddf\u8e2a\u6a21\u7248\u6a21\u578b\u8def\u5f84\n        self.track_crop_kmodel=track_crop_kmodel\n        # \u8ddf\u8e2a\u5b9e\u65f6\u6a21\u578b\u8def\u5f84\n        self.track_src_kmodel=track_src_kmodel\n        # \u8ddf\u8e2a\u6a21\u578b\u8def\u5f84\n        self.tracker_kmodel=tracker_kmodel\n        # \u8ddf\u8e2a\u6a21\u7248\u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.crop_input_size=crop_input_size\n        # \u8ddf\u8e2a\u5b9e\u65f6\u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.src_input_size=src_input_size\n        self.threshold=threshold\n\n        self.CONTEXT_AMOUNT=0.5       # \u8ddf\u8e2a\u6846\u5bbd\u3001\u9ad8\u8c03\u6574\u7cfb\u6570\n        self.ratio_src_crop = 0.0     # src\u6a21\u578b\u548ccrop\u6a21\u578b\u8f93\u5165\u6bd4\u503c\n        self.track_x1 = float(600)    # \u8d77\u59cb\u8ddf\u8e2a\u76ee\u6807\u6846\u5de6\u4e0a\u89d2\u70b9x\n        self.track_y1 = float(300)    # \u8d77\u59cb\u8ddf\u8e2a\u76ee\u6807\u6846\u5de6\u4e0a\u89d2\u70b9y\n        self.track_w = float(100)     # \u8d77\u59cb\u8ddf\u8e2a\u76ee\u6807\u6846w\n        self.track_h = float(100)     # \u8d77\u59cb\u8ddf\u8e2a\u76ee\u6807\u6846h\n        self.draw_mean=[]             # \u521d\u59cb\u76ee\u6807\u6846\u4f4d\u7f6e\u5217\u8868\n        self.center_xy_wh = []\n        self.track_boxes = []\n        self.center_xy_wh_tmp = []\n        self.track_boxes_tmp=[]\n        self.crop_output=None\n        self.src_output=None\n        # \u8ddf\u8e2a\u6846\u521d\u59cb\u5316\u65f6\u95f4\n        self.seconds = 8\n        self.endtime = time.time() + self.seconds\n        self.enter_init = True\n\n        # sensor\u7ed9\u5230AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.rgb888p_size=[ALIGN_UP(rgb888p_size[0],16),rgb888p_size[1]]\n        # \u89c6\u9891\u8f93\u51faVO\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.display_size=[ALIGN_UP(display_size[0],16),display_size[1]]\n        self.init_param()\n\n        self.track_crop=TrackCropApp(self.track_crop_kmodel,model_input_size=self.crop_input_size,ratio_src_crop=self.ratio_src_crop,center_xy_wh=self.center_xy_wh,rgb888p_size=self.rgb888p_size,display_size=self.display_size,debug_mode=0)\n        self.track_src=TrackSrcApp(self.track_src_kmodel,model_input_size=self.src_input_size,ratio_src_crop=self.ratio_src_crop,rgb888p_size=self.rgb888p_size,display_size=self.display_size,debug_mode=0)\n        self.tracker=TrackerApp(self.tracker_kmodel,crop_input_size=self.crop_input_size,thresh=self.threshold,rgb888p_size=self.rgb888p_size,display_size=self.display_size)\n        self.track_crop.config_preprocess()\n\n    # run\u51fd\u6570\n    def run(self,input_np):\n        # \u5728\u521d\u59cb\u5316\u65f6\u95f4\u5185\uff0ccrop\u6a21\u7248\u90e8\u5206\u7684\u5230\u8ddf\u8e2a\u6a21\u7248\u7279\u5f81\uff0c\u5426\u5219\uff0c\u5bf9\u5f53\u524d\u5e27\u8fdb\u884csrc\u63a8\u7406\u5f97\u5230\u7279\u5f81\u5e76\u4f7f\u7528tracker\u5bf9\u4e24\u4e2a\u7279\u5f81\u63a8\u7406\uff0c\u5f97\u5230\u8ddf\u8e2a\u6846\u7684\u5750\u6807\n        nowtime = time.time()\n        if (self.enter_init and nowtime <= self.endtime):\n            print("\u5012\u8ba1\u65f6: " + str(self.endtime - nowtime) + " \u79d2")\n            self.crop_output=self.track_crop.run(input_np)\n            time.sleep(1)\n            return self.draw_mean\n        else:\n            self.track_src.config_preprocess(self.center_xy_wh)\n            self.src_output=self.track_src.run(input_np)\n            det=self.tracker.run(self.crop_output,self.src_output,self.center_xy_wh)\n            return det\n\n    # \u7ed8\u5236\u6548\u679c\uff0c\u7ed8\u5236\u8ddf\u8e2a\u6846\u4f4d\u7f6e\n    def draw_result(self,pl,box):\n        pl.osd_img.clear()\n        if self.enter_init:\n            pl.osd_img.draw_rectangle(box[0],box[1],box[2],box[3],color=(255, 0, 255, 0),thickness = 4)\n            if (time.time() > self.endtime):\n                self.enter_init = False\n        else:\n            self.track_boxes = box[0]\n            self.center_xy_wh = box[1]\n            track_bool = True\n            if (len(self.track_boxes) != 0):\n                track_bool = self.track_boxes[0] > 10 and self.track_boxes[1] > 10 and self.track_boxes[0] + self.track_boxes[2] < self.rgb888p_size[0] - 10 and self.track_boxes[1] + self.track_boxes[3] < self.rgb888p_size[1] - 10\n            else:\n                track_bool = False\n\n            if (len(self.center_xy_wh) != 0):\n                track_bool = track_bool and self.center_xy_wh[2] * self.center_xy_wh[3] < 40000\n            else:\n                track_bool = False\n            if (track_bool):\n                self.center_xy_wh_tmp = self.center_xy_wh\n                self.track_boxes_tmp = self.track_boxes\n                x1 = int(float(self.track_boxes[0]) * self.display_size[0] / self.rgb888p_size[0])\n                y1 = int(float(self.track_boxes[1]) * self.display_size[1] / self.rgb888p_size[1])\n                w = int(float(self.track_boxes[2]) * self.display_size[0] / self.rgb888p_size[0])\n                h = int(float(self.track_boxes[3]) * self.display_size[1] / self.rgb888p_size[1])\n                pl.osd_img.draw_rectangle(x1, y1, w, h, color=(255, 255, 0, 0),thickness = 4)\n            else:\n                self.center_xy_wh = self.center_xy_wh_tmp\n                self.track_boxes = self.track_boxes_tmp\n                x1 = int(float(self.track_boxes[0]) * self.display_size[0] / self.rgb888p_size[0])\n                y1 = int(float(self.track_boxes[1]) * self.display_size[1] / self.rgb888p_size[1])\n                w = int(float(self.track_boxes[2]) * self.display_size[0] / self.rgb888p_size[0])\n                h = int(float(self.track_boxes[3]) * self.display_size[1] / self.rgb888p_size[1])\n                pl.osd_img.draw_rectangle(x1, y1, w, h, color=(255, 255, 0, 0),thickness = 4)\n                pl.osd_img.draw_string_advanced( x1 , y1-50,32, "\u8bf7\u8fdc\u79bb\u6444\u50cf\u5934\uff0c\u4fdd\u6301\u8ddf\u8e2a\u7269\u4f53\u5927\u5c0f\u57fa\u672c\u4e00\u81f4!" , color=(255, 255 ,0 , 0))\n                pl.osd_img.draw_string_advanced( x1 , y1-100,32, "\u8bf7\u9760\u8fd1\u4e2d\u5fc3!" , color=(255, 255 ,0 , 0))\n\n    # crop\u53c2\u6570\u521d\u59cb\u5316\n    def init_param(self):\n        self.ratio_src_crop = float(self.src_input_size[0])/float(self.crop_input_size[0])\n        print(self.ratio_src_crop)\n        if (self.track_x1 < 50 or self.track_y1 < 50 or self.track_x1+self.track_w >= self.rgb888p_size[0]-50 or self.track_y1+self.track_h >= self.rgb888p_size[1]-50):\n                    print("**\u526a\u5207\u8303\u56f4\u8d85\u51fa\u56fe\u50cf\u8303\u56f4**")\n        else:\n            track_mean_x = self.track_x1 + self.track_w / 2.0\n            track_mean_y = self.track_y1 + self.track_h / 2.0\n            draw_mean_w = int(self.track_w / self.rgb888p_size[0] * self.display_size[0])\n            draw_mean_h = int(self.track_h / self.rgb888p_size[1] * self.display_size[1])\n            draw_mean_x = int(track_mean_x / self.rgb888p_size[0] * self.display_size[0] - draw_mean_w / 2.0)\n            draw_mean_y = int(track_mean_y / self.rgb888p_size[1] * self.display_size[1] - draw_mean_h / 2.0)\n            self.draw_mean=[draw_mean_x,draw_mean_y,draw_mean_w,draw_mean_h]\n            self.center_xy_wh = [track_mean_x,track_mean_y,self.track_w,self.track_h]\n            self.center_xy_wh_tmp=[track_mean_x,track_mean_y,self.track_w,self.track_h]\n\n            self.track_boxes = [self.track_x1,self.track_y1,self.track_w,self.track_h,1]\n            self.track_boxes_tmp=np.array([self.track_x1,self.track_y1,self.track_w,self.track_h,1])\n\n\nif __name__=="__main__":\n    # \u663e\u793a\u6a21\u5f0f\uff0c\u9ed8\u8ba4"hdmi",\u53ef\u4ee5\u9009\u62e9"hdmi"\u548c"lcd"\n    display_mode="hdmi"\n    if display_mode=="hdmi":\n        display_size=[1920,1080]\n    else:\n        display_size=[800,480]\n    # \u8ddf\u8e2a\u6a21\u677f\u6a21\u578b\u8def\u5f84\n    track_crop_kmodel_path="/sdcard/app/tests/kmodel/cropped_test127.kmodel"\n    # \u8ddf\u8e2a\u5b9e\u65f6\u6a21\u578b\u8def\u5f84\n    track_src_kmodel_path="/sdcard/app/tests/kmodel/nanotrack_backbone_sim.kmodel"\n    # \u8ddf\u8e2a\u6a21\u578b\u8def\u5f84\n    tracker_kmodel_path="/sdcard/app/tests/kmodel/nanotracker_head_calib_k230.kmodel"\n    # \u5176\u4ed6\u53c2\u6570\n    rgb888p_size=[1280,720]\n    track_crop_input_size=[127,127]\n    track_src_input_size=[255,255]\n    threshold=0.1\n\n    # \u521d\u59cb\u5316PipeLine\uff0c\u53ea\u5173\u6ce8\u4f20\u7ed9AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u663e\u793a\u7684\u5206\u8fa8\u7387\n    pl=PipeLine(rgb888p_size=rgb888p_size,display_size=display_size,display_mode=display_mode)\n    pl.create()\n    track=NanoTracker(track_crop_kmodel_path,track_src_kmodel_path,tracker_kmodel_path,crop_input_size=track_crop_input_size,src_input_size=track_src_input_size,threshold=threshold,rgb888p_size=rgb888p_size,display_size=display_size)\n    try:\n        while True:\n            os.exitpoint()\n            with ScopedTiming("total",1):\n                img=pl.get_frame()              # \u83b7\u53d6\u5f53\u524d\u5e27\n                output=track.run(img)           # \u63a8\u7406\u5f53\u524d\u5e27\n                track.draw_result(pl,output)    # \u7ed8\u5236\u5f53\u524d\u5e27\u63a8\u7406\u7ed3\u679c\n                pl.show_image()                 # \u5c55\u793a\u63a8\u7406\u7ed3\u679c\n                gc.collect()\n    except Exception as e:\n        sys.print_exception(e)\n    finally:\n        track.track_crop.deinit()\n        track.track_src.deinit()\n        track.tracker.deinit()\n        pl.destroy()\n'})}),"\n",(0,i.jsx)(n.h3,{id:"220-yolov8n\u76ee\u6807\u68c0\u6d4b",children:"2.20. yolov8n\u76ee\u6807\u68c0\u6d4b"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:'from libs.PipeLine import PipeLine, ScopedTiming\nfrom libs.AIBase import AIBase\nfrom libs.AI2D import Ai2d\nimport os\nimport ujson\nfrom media.media import *\nfrom time import *\nimport nncase_runtime as nn\nimport ulab.numpy as np\nimport time\nimport utime\nimport image\nimport random\nimport gc\nimport sys\nimport aidemo\n\n# \u81ea\u5b9a\u4e49YOLOv8\u68c0\u6d4b\u7c7b\nclass ObjectDetectionApp(AIBase):\n    def __init__(self,kmodel_path,labels,model_input_size,max_boxes_num,confidence_threshold=0.5,nms_threshold=0.2,rgb888p_size=[224,224],display_size=[1920,1080],debug_mode=0):\n        super().__init__(kmodel_path,model_input_size,rgb888p_size,debug_mode)\n        self.kmodel_path=kmodel_path\n        self.labels=labels\n        # \u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.model_input_size=model_input_size\n        # \u9608\u503c\u8bbe\u7f6e\n        self.confidence_threshold=confidence_threshold\n        self.nms_threshold=nms_threshold\n        self.max_boxes_num=max_boxes_num\n        # sensor\u7ed9\u5230AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\n        self.rgb888p_size=[ALIGN_UP(rgb888p_size[0],16),rgb888p_size[1]]\n        # \u663e\u793a\u5206\u8fa8\u7387\n        self.display_size=[ALIGN_UP(display_size[0],16),display_size[1]]\n        self.debug_mode=debug_mode\n        # \u68c0\u6d4b\u6846\u9884\u7f6e\u989c\u8272\u503c\n        self.color_four=[(255, 220, 20, 60), (255, 119, 11, 32), (255, 0, 0, 142), (255, 0, 0, 230),\n                         (255, 106, 0, 228), (255, 0, 60, 100), (255, 0, 80, 100), (255, 0, 0, 70),\n                         (255, 0, 0, 192), (255, 250, 170, 30), (255, 100, 170, 30), (255, 220, 220, 0),\n                         (255, 175, 116, 175), (255, 250, 0, 30), (255, 165, 42, 42), (255, 255, 77, 255),\n                         (255, 0, 226, 252), (255, 182, 182, 255), (255, 0, 82, 0), (255, 120, 166, 157)]\n        # \u5bbd\u9ad8\u7f29\u653e\u6bd4\u4f8b\n        self.x_factor = float(self.rgb888p_size[0])/self.model_input_size[0]\n        self.y_factor = float(self.rgb888p_size[1])/self.model_input_size[1]\n        # Ai2d\u5b9e\u4f8b\uff0c\u7528\u4e8e\u5b9e\u73b0\u6a21\u578b\u9884\u5904\u7406\n        self.ai2d=Ai2d(debug_mode)\n        # \u8bbe\u7f6eAi2d\u7684\u8f93\u5165\u8f93\u51fa\u683c\u5f0f\u548c\u7c7b\u578b\n        self.ai2d.set_ai2d_dtype(nn.ai2d_format.NCHW_FMT,nn.ai2d_format.NCHW_FMT,np.uint8, np.uint8)\n\n    # \u914d\u7f6e\u9884\u5904\u7406\u64cd\u4f5c\uff0c\u8fd9\u91cc\u4f7f\u7528\u4e86resize\uff0cAi2d\u652f\u6301crop/shift/pad/resize/affine\uff0c\u5177\u4f53\u4ee3\u7801\u8bf7\u6253\u5f00/sdcard/app/libs/AI2D.py\u67e5\u770b\n    def config_preprocess(self,input_image_size=None):\n        with ScopedTiming("set preprocess config",self.debug_mode > 0):\n            # \u521d\u59cb\u5316ai2d\u9884\u5904\u7406\u914d\u7f6e\uff0c\u9ed8\u8ba4\u4e3asensor\u7ed9\u5230AI\u7684\u5c3a\u5bf8\uff0c\u60a8\u53ef\u4ee5\u901a\u8fc7\u8bbe\u7f6einput_image_size\u81ea\u884c\u4fee\u6539\u8f93\u5165\u5c3a\u5bf8\n            ai2d_input_size=input_image_size if input_image_size else self.rgb888p_size\n            self.ai2d.resize(nn.interp_method.tf_bilinear, nn.interp_mode.half_pixel)\n            self.ai2d.build([1,3,ai2d_input_size[1],ai2d_input_size[0]],[1,3,self.model_input_size[1],self.model_input_size[0]])\n\n    # \u81ea\u5b9a\u4e49\u5f53\u524d\u4efb\u52a1\u7684\u540e\u5904\u7406\n    def postprocess(self,results):\n        with ScopedTiming("postprocess",self.debug_mode > 0):\n            result=results[0]\n            result = result.reshape((result.shape[0] * result.shape[1], result.shape[2]))\n            output_data = result.transpose()\n            boxes_ori = output_data[:,0:4]\n            scores_ori = output_data[:,4:]\n            confs_ori = np.max(scores_ori,axis=-1)\n            inds_ori = np.argmax(scores_ori,axis=-1)\n            boxes,scores,inds = [],[],[]\n            for i in range(len(boxes_ori)):\n                if confs_ori[i] > confidence_threshold:\n                    scores.append(confs_ori[i])\n                    inds.append(inds_ori[i])\n                    x = boxes_ori[i,0]\n                    y = boxes_ori[i,1]\n                    w = boxes_ori[i,2]\n                    h = boxes_ori[i,3]\n                    left = int((x - 0.5 * w) * self.x_factor)\n                    top = int((y - 0.5 * h) * self.y_factor)\n                    right = int((x + 0.5 * w) * self.x_factor)\n                    bottom = int((y + 0.5 * h) * self.y_factor)\n                    boxes.append([left,top,right,bottom])\n            if len(boxes)==0:\n                return []\n            boxes = np.array(boxes)\n            scores = np.array(scores)\n            inds = np.array(inds)\n            # NMS\u8fc7\u7a0b\n            keep = self.nms(boxes,scores,nms_threshold)\n            dets = np.concatenate((boxes, scores.reshape((len(boxes),1)), inds.reshape((len(boxes),1))), axis=1)\n            dets_out = []\n            for keep_i in keep:\n                dets_out.append(dets[keep_i])\n            dets_out = np.array(dets_out)\n            dets_out = dets_out[:self.max_boxes_num, :]\n            return dets_out\n\n    # \u7ed8\u5236\u7ed3\u679c\n    def draw_result(self,pl,dets):\n        with ScopedTiming("display_draw",self.debug_mode >0):\n            if dets:\n                pl.osd_img.clear()\n                for det in dets:\n                    x1, y1, x2, y2 = map(lambda x: int(round(x, 0)), det[:4])\n                    x= x1*self.display_size[0] // self.rgb888p_size[0]\n                    y= y1*self.display_size[1] // self.rgb888p_size[1]\n                    w = (x2 - x1) * self.display_size[0] // self.rgb888p_size[0]\n                    h = (y2 - y1) * self.display_size[1] // self.rgb888p_size[1]\n                    pl.osd_img.draw_rectangle(x,y, w, h, color=self.get_color(int(det[5])),thickness=4)\n                    pl.osd_img.draw_string_advanced( x , y-50,32," " + self.labels[int(det[5])] + " " + str(round(det[4],2)) , color=self.get_color(int(det[5])))\n            else:\n                pl.osd_img.clear()\n\n\n    # \u591a\u76ee\u6807\u68c0\u6d4b \u975e\u6700\u5927\u503c\u6291\u5236\u65b9\u6cd5\u5b9e\u73b0\n    def nms(self,boxes,scores,thresh):\n        """Pure Python NMS baseline."""\n        x1,y1,x2,y2 = boxes[:, 0],boxes[:, 1],boxes[:, 2],boxes[:, 3]\n        areas = (x2 - x1 + 1) * (y2 - y1 + 1)\n        order = np.argsort(scores,axis = 0)[::-1]\n        keep = []\n        while order.size > 0:\n            i = order[0]\n            keep.append(i)\n            new_x1,new_y1,new_x2,new_y2,new_areas = [],[],[],[],[]\n            for order_i in order:\n                new_x1.append(x1[order_i])\n                new_x2.append(x2[order_i])\n                new_y1.append(y1[order_i])\n                new_y2.append(y2[order_i])\n                new_areas.append(areas[order_i])\n            new_x1 = np.array(new_x1)\n            new_x2 = np.array(new_x2)\n            new_y1 = np.array(new_y1)\n            new_y2 = np.array(new_y2)\n            xx1 = np.maximum(x1[i], new_x1)\n            yy1 = np.maximum(y1[i], new_y1)\n            xx2 = np.minimum(x2[i], new_x2)\n            yy2 = np.minimum(y2[i], new_y2)\n            w = np.maximum(0.0, xx2 - xx1 + 1)\n            h = np.maximum(0.0, yy2 - yy1 + 1)\n            inter = w * h\n            new_areas = np.array(new_areas)\n            ovr = inter / (areas[i] + new_areas - inter)\n            new_order = []\n            for ovr_i,ind in enumerate(ovr):\n                if ind < thresh:\n                    new_order.append(order[ovr_i])\n            order = np.array(new_order,dtype=np.uint8)\n        return keep\n\n    # \u6839\u636e\u5f53\u524d\u7c7b\u522b\u7d22\u5f15\u83b7\u53d6\u6846\u7684\u989c\u8272\n    def get_color(self, x):\n        idx=x%len(self.color_four)\n        return self.color_four[idx]\n\n\nif __name__=="__main__":\n    # \u663e\u793a\u6a21\u5f0f\uff0c\u9ed8\u8ba4"hdmi",\u53ef\u4ee5\u9009\u62e9"hdmi"\u548c"lcd"\n    display_mode="hdmi"\n    if display_mode=="hdmi":\n        display_size=[1920,1080]\n    else:\n        display_size=[800,480]\n    # \u6a21\u578b\u8def\u5f84\n    kmodel_path="/sdcard/app/tests/kmodel/yolov8n_320.kmodel"\n    labels = ["person", "bicycle", "car", "motorcycle", "airplane", "bus", "train", "truck", "boat", "traffic light", "fire hydrant", "stop sign", "parking meter", "bench", "bird", "cat", "dog", "horse", "sheep", "cow", "elephant", "bear", "zebra", "giraffe", "backpack", "umbrella", "handbag", "tie", "suitcase", "frisbee", "skis", "snowboard", "sports ball", "kite", "baseball bat", "baseball glove", "skateboard", "surfboard", "tennis racket", "bottle", "wine glass", "cup", "fork", "knife", "spoon", "bowl", "banana", "apple", "sandwich", "orange", "broccoli", "carrot", "hot dog", "pizza", "donut", "cake", "chair", "couch", "potted plant", "bed", "dining table", "toilet", "tv", "laptop", "mouse", "remote", "keyboard", "cell phone", "microwave", "oven", "toaster", "sink", "refrigerator", "book", "clock", "vase", "scissors", "teddy bear", "hair drier", "toothbrush"]\n    # \u5176\u5b83\u53c2\u6570\u8bbe\u7f6e\n    confidence_threshold = 0.2\n    nms_threshold = 0.2\n    max_boxes_num = 50\n    rgb888p_size=[320,320]\n\n    # \u521d\u59cb\u5316PipeLine\n    pl=PipeLine(rgb888p_size=rgb888p_size,display_size=display_size,display_mode=display_mode)\n    pl.create()\n    # \u521d\u59cb\u5316\u81ea\u5b9a\u4e49\u76ee\u6807\u68c0\u6d4b\u5b9e\u4f8b\n    ob_det=ObjectDetectionApp(kmodel_path,labels=labels,model_input_size=[320,320],max_boxes_num=max_boxes_num,confidence_threshold=confidence_threshold,nms_threshold=nms_threshold,rgb888p_size=rgb888p_size,display_size=display_size,debug_mode=0)\n    ob_det.config_preprocess()\n    try:\n        while True:\n            os.exitpoint()\n            with ScopedTiming("total",1):\n                # \u83b7\u53d6\u5f53\u524d\u5e27\u6570\u636e\n                img=pl.get_frame()\n                # \u63a8\u7406\u5f53\u524d\u5e27\n                res=ob_det.run(img)\n                # \u7ed8\u5236\u7ed3\u679c\u5230PipeLine\u7684osd\u56fe\u50cf\n                ob_det.draw_result(pl,res)\n                # \u663e\u793a\u5f53\u524d\u7684\u7ed8\u5236\u7ed3\u679c\n                pl.show_image()\n                gc.collect()\n    except Exception as e:\n        sys.print_exception(e)\n    finally:\n        ob_det.deinit()\n        pl.destroy()\n'})}),"\n",(0,i.jsx)(n.h3,{id:"221-ocr\u68c0\u6d4b",children:"2.21. OCR\u68c0\u6d4b"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:'from libs.PipeLine import PipeLine, ScopedTiming\nfrom libs.AIBase import AIBase\nfrom libs.AI2D import Ai2d\nimport os\nimport ujson\nfrom media.media import *\nfrom time import *\nimport nncase_runtime as nn\nimport ulab.numpy as np\nimport time\nimport utime\nimport image\nimport random\nimport gc\nimport sys\nimport aicube\n\n# \u81ea\u5b9a\u4e49OCR\u68c0\u6d4b\u7c7b\nclass OCRDetectionApp(AIBase):\n    def __init__(self,kmodel_path,model_input_size,mask_threshold=0.5,box_threshold=0.2,rgb888p_size=[224,224],display_size=[1920,1080],debug_mode=0):\n        super().__init__(kmodel_path,model_input_size,rgb888p_size,debug_mode)\n        self.kmodel_path=kmodel_path\n        # \u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.model_input_size=model_input_size\n        # \u5206\u7c7b\u9608\u503c\n        self.mask_threshold=mask_threshold\n        self.box_threshold=box_threshold\n        # sensor\u7ed9\u5230AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\n        self.rgb888p_size=[ALIGN_UP(rgb888p_size[0],16),rgb888p_size[1]]\n        # \u663e\u793a\u5206\u8fa8\u7387\n        self.display_size=[ALIGN_UP(display_size[0],16),display_size[1]]\n        self.debug_mode=debug_mode\n        # Ai2d\u5b9e\u4f8b\uff0c\u7528\u4e8e\u5b9e\u73b0\u6a21\u578b\u9884\u5904\u7406\n        self.ai2d=Ai2d(debug_mode)\n        # \u8bbe\u7f6eAi2d\u7684\u8f93\u5165\u8f93\u51fa\u683c\u5f0f\u548c\u7c7b\u578b\n        self.ai2d.set_ai2d_dtype(nn.ai2d_format.NCHW_FMT,nn.ai2d_format.NCHW_FMT,np.uint8, np.uint8)\n\n    # \u914d\u7f6e\u9884\u5904\u7406\u64cd\u4f5c\uff0c\u8fd9\u91cc\u4f7f\u7528\u4e86pad\u548cresize\uff0cAi2d\u652f\u6301crop/shift/pad/resize/affine\uff0c\u5177\u4f53\u4ee3\u7801\u8bf7\u6253\u5f00/sdcard/app/libs/AI2D.py\u67e5\u770b\n    def config_preprocess(self,input_image_size=None):\n        with ScopedTiming("set preprocess config",self.debug_mode > 0):\n            # \u521d\u59cb\u5316ai2d\u9884\u5904\u7406\u914d\u7f6e\uff0c\u9ed8\u8ba4\u4e3asensor\u7ed9\u5230AI\u7684\u5c3a\u5bf8\uff0c\u60a8\u53ef\u4ee5\u901a\u8fc7\u8bbe\u7f6einput_image_size\u81ea\u884c\u4fee\u6539\u8f93\u5165\u5c3a\u5bf8\n            ai2d_input_size=input_image_size if input_image_size else self.rgb888p_size\n            top,bottom,left,right=self.get_padding_param()\n            self.ai2d.pad([0,0,0,0,top,bottom,left,right], 0, [0,0,0])\n            self.ai2d.resize(nn.interp_method.tf_bilinear, nn.interp_mode.half_pixel)\n            self.ai2d.build([1,3,ai2d_input_size[1],ai2d_input_size[0]],[1,3,self.model_input_size[1],self.model_input_size[0]])\n\n    # \u81ea\u5b9a\u4e49\u5f53\u524d\u4efb\u52a1\u7684\u540e\u5904\u7406\n    def postprocess(self,results):\n        with ScopedTiming("postprocess",self.debug_mode > 0):\n            # chw2hwc\n            hwc_array=self.chw2hwc(self.cur_img)\n            # \u8fd9\u91cc\u4f7f\u7528\u4e86aicube\u5c01\u88c5\u7684\u63a5\u53e3seg_post_process\u505a\u540e\u5904\u7406\uff0c\u8fd4\u56de\u4e00\u4e2a\u548cdisplay_size\u76f8\u540c\u5206\u8fa8\u7387\u7684mask\u56fe\n            # det_boxes\u7ed3\u6784\u4e3a[[crop_array_nhwc,[p1_x,p1_y,p2_x,p2_y,p3_x,p3_y,p4_x,p4_y]],...]\n            det_boxes = aicube.ocr_post_process(results[0][:,:,:,0].reshape(-1), hwc_array.reshape(-1),self.model_input_size,self.rgb888p_size, self.mask_threshold, self.box_threshold)\n            all_boxes_pos=[]\n            for det_box in det_boxes:\n                all_boxes_pos.append(det_box[1])\n            return all_boxes_pos\n\n    # \u7ed8\u5236\u7ed3\u679c\n    def draw_result(self,pl,all_boxes_pos):\n        with ScopedTiming("display_draw",self.debug_mode >0):\n            pl.osd_img.clear()\n            # \u4e00\u6b21\u7ed8\u5236\u56db\u6761\u8fb9\uff0c\u5f97\u5230\u6587\u672c\u68c0\u6d4b\u7684\u56db\u8fb9\u5f62\n            for i in range(len(all_boxes_pos)):\n                for j in range(4):\n                    x1=all_boxes_pos[i][2*j]*self.display_size[0]//self.rgb888p_size[0]\n                    y1=all_boxes_pos[i][2*j+1]*self.display_size[1]//self.rgb888p_size[1]\n                    x2=all_boxes_pos[i][(2*j+2)%8]*self.display_size[0]//self.rgb888p_size[0]\n                    y2=all_boxes_pos[i][(2*j+3)%8]*self.display_size[1]//self.rgb888p_size[1]\n                    pl.osd_img.draw_line(int(x1),int(y1),int(x2),int(y2),color=(255,255,0,0),thickness=4)\n\n    # \u8ba1\u7b97padding\u53c2\u6570\n    def get_padding_param(self):\n        # \u53f3padding\u6216\u4e0bpadding\n        dst_w = self.model_input_size[0]\n        dst_h = self.model_input_size[1]\n        input_width = self.rgb888p_size[0]\n        input_high = self.rgb888p_size[1]\n        ratio_w = dst_w / input_width\n        ratio_h = dst_h / input_high\n        if ratio_w < ratio_h:\n            ratio = ratio_w\n        else:\n            ratio = ratio_h\n        new_w = (int)(ratio * input_width)\n        new_h = (int)(ratio * input_high)\n        dw = (dst_w - new_w) / 2\n        dh = (dst_h - new_h) / 2\n        top = (int)(round(0))\n        bottom = (int)(round(dh * 2 + 0.1))\n        left = (int)(round(0))\n        right = (int)(round(dw * 2 - 0.1))\n        return  top, bottom, left, right\n\n    # chw2hwc\n    def chw2hwc(self,features):\n        ori_shape = (features.shape[0], features.shape[1], features.shape[2])\n        c_hw_ = features.reshape((ori_shape[0], ori_shape[1] * ori_shape[2]))\n        hw_c_ = c_hw_.transpose()\n        new_array = hw_c_.copy()\n        hwc_array = new_array.reshape((ori_shape[1], ori_shape[2], ori_shape[0]))\n        del c_hw_\n        del hw_c_\n        del new_array\n        return hwc_array\n\nif __name__=="__main__":\n    # \u663e\u793a\u6a21\u5f0f\uff0c\u9ed8\u8ba4"hdmi",\u53ef\u4ee5\u9009\u62e9"hdmi"\u548c"lcd"\n    display_mode="hdmi"\n    if display_mode=="hdmi":\n        display_size=[1920,1080]\n    else:\n        display_size=[800,480]\n    # \u6a21\u578b\u8def\u5f84\n    kmodel_path="/sdcard/app/tests/kmodel/ocr_det_int16.kmodel"\n    # kmodel\u5176\u5b83\u53c2\u6570\u8bbe\u7f6e\n    mask_threshold = 0.25\n    box_threshold = 0.3\n    rgb888p_size=[640,360]\n\n    # \u521d\u59cb\u5316PipeLine\n    pl=PipeLine(rgb888p_size=rgb888p_size,display_size=display_size,display_mode=display_mode)\n    pl.create()\n    # \u521d\u59cb\u5316\u81ea\u5b9a\u4e49OCR\u68c0\u6d4b\u5b9e\u4f8b\n    ocr_det=OCRDetectionApp(kmodel_path,model_input_size=[640,640],mask_threshold=mask_threshold,box_threshold=box_threshold,rgb888p_size=rgb888p_size,display_size=display_size,debug_mode=0)\n    ocr_det.config_preprocess()\n    try:\n        while True:\n            os.exitpoint()\n            with ScopedTiming("total",1):\n                # \u83b7\u53d6\u5f53\u524d\u5e27\u6570\u636e\n                img=pl.get_frame()\n                # \u63a8\u7406\u5f53\u524d\u5e27\n                res=ocr_det.run(img)\n                # \u7ed8\u5236\u7ed3\u679c\u5230PipeLine\u7684osd\u56fe\u50cf\n                ocr_det.draw_result(pl,res)\n                # \u663e\u793a\u5f53\u524d\u7684\u7ed8\u5236\u7ed3\u679c\n                pl.show_image()\n                gc.collect()\n    except Exception as e:\n        sys.print_exception(e)\n    finally:\n        ocr_det.deinit()\n        pl.destroy()\n'})}),"\n",(0,i.jsx)(n.h3,{id:"222-ocr\u8bc6\u522b",children:"2.22. OCR\u8bc6\u522b"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:'from libs.PipeLine import PipeLine, ScopedTiming\nfrom libs.AIBase import AIBase\nfrom libs.AI2D import Ai2d\nimport os\nimport ujson\nfrom media.media import *\nfrom time import *\nimport nncase_runtime as nn\nimport ulab.numpy as np\nimport time\nimport image\nimport aicube\nimport random\nimport gc\nimport sys\n\n# \u81ea\u5b9a\u4e49OCR\u68c0\u6d4b\u7c7b\nclass OCRDetectionApp(AIBase):\n    def __init__(self,kmodel_path,model_input_size,mask_threshold=0.5,box_threshold=0.2,rgb888p_size=[224,224],display_size=[1920,1080],debug_mode=0):\n        super().__init__(kmodel_path,model_input_size,rgb888p_size,debug_mode)\n        self.kmodel_path=kmodel_path\n        # \u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.model_input_size=model_input_size\n        # \u5206\u7c7b\u9608\u503c\n        self.mask_threshold=mask_threshold\n        self.box_threshold=box_threshold\n        # sensor\u7ed9\u5230AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\n        self.rgb888p_size=[ALIGN_UP(rgb888p_size[0],16),rgb888p_size[1]]\n        # \u663e\u793a\u5206\u8fa8\u7387\n        self.display_size=[ALIGN_UP(display_size[0],16),display_size[1]]\n        self.debug_mode=debug_mode\n        # Ai2d\u5b9e\u4f8b\uff0c\u7528\u4e8e\u5b9e\u73b0\u6a21\u578b\u9884\u5904\u7406\n        self.ai2d=Ai2d(debug_mode)\n        # \u8bbe\u7f6eAi2d\u7684\u8f93\u5165\u8f93\u51fa\u683c\u5f0f\u548c\u7c7b\u578b\n        self.ai2d.set_ai2d_dtype(nn.ai2d_format.NCHW_FMT,nn.ai2d_format.NCHW_FMT,np.uint8, np.uint8)\n\n    # \u914d\u7f6e\u9884\u5904\u7406\u64cd\u4f5c\uff0c\u8fd9\u91cc\u4f7f\u7528\u4e86pad\u548cresize\uff0cAi2d\u652f\u6301crop/shift/pad/resize/affine\uff0c\u5177\u4f53\u4ee3\u7801\u8bf7\u6253\u5f00/sdcard/app/libs/AI2D.py\u67e5\u770b\n    def config_preprocess(self,input_image_size=None):\n        with ScopedTiming("set preprocess config",self.debug_mode > 0):\n            # \u521d\u59cb\u5316ai2d\u9884\u5904\u7406\u914d\u7f6e\uff0c\u9ed8\u8ba4\u4e3asensor\u7ed9\u5230AI\u7684\u5c3a\u5bf8\uff0c\u60a8\u53ef\u4ee5\u901a\u8fc7\u8bbe\u7f6einput_image_size\u81ea\u884c\u4fee\u6539\u8f93\u5165\u5c3a\u5bf8\n            ai2d_input_size=input_image_size if input_image_size else self.rgb888p_size\n            top,bottom,left,right=self.get_padding_param()\n            self.ai2d.pad([0,0,0,0,top,bottom,left,right], 0, [0,0,0])\n            self.ai2d.resize(nn.interp_method.tf_bilinear, nn.interp_mode.half_pixel)\n            self.ai2d.build([1,3,ai2d_input_size[1],ai2d_input_size[0]],[1,3,self.model_input_size[1],self.model_input_size[0]])\n\n    # \u81ea\u5b9a\u4e49\u5f53\u524d\u4efb\u52a1\u7684\u540e\u5904\u7406\n    def postprocess(self,results):\n        with ScopedTiming("postprocess",self.debug_mode > 0):\n            # chw2hwc\n            hwc_array=self.chw2hwc(self.cur_img)\n            # \u8fd9\u91cc\u4f7f\u7528\u4e86aicube\u5c01\u88c5\u7684\u63a5\u53e3ocr_post_process\u505a\u540e\u5904\u7406,\u8fd4\u56de\u7684det_boxes\u7ed3\u6784\u4e3a[[crop_array_nhwc,[p1_x,p1_y,p2_x,p2_y,p3_x,p3_y,p4_x,p4_y]],...]\n            det_boxes = aicube.ocr_post_process(results[0][:,:,:,0].reshape(-1), hwc_array.reshape(-1),self.model_input_size,self.rgb888p_size, self.mask_threshold, self.box_threshold)\n            return det_boxes\n\n    # \u8ba1\u7b97padding\u53c2\u6570\n    def get_padding_param(self):\n        # \u53f3padding\u6216\u4e0bpadding\n        dst_w = self.model_input_size[0]\n        dst_h = self.model_input_size[1]\n        input_width = self.rgb888p_size[0]\n        input_high = self.rgb888p_size[1]\n        ratio_w = dst_w / input_width\n        ratio_h = dst_h / input_high\n        if ratio_w < ratio_h:\n            ratio = ratio_w\n        else:\n            ratio = ratio_h\n        new_w = (int)(ratio * input_width)\n        new_h = (int)(ratio * input_high)\n        dw = (dst_w - new_w) / 2\n        dh = (dst_h - new_h) / 2\n        top = (int)(round(0))\n        bottom = (int)(round(dh * 2 + 0.1))\n        left = (int)(round(0))\n        right = (int)(round(dw * 2 - 0.1))\n        return  top, bottom, left, right\n\n    # chw2hwc\n    def chw2hwc(self,features):\n        ori_shape = (features.shape[0], features.shape[1], features.shape[2])\n        c_hw_ = features.reshape((ori_shape[0], ori_shape[1] * ori_shape[2]))\n        hw_c_ = c_hw_.transpose()\n        new_array = hw_c_.copy()\n        hwc_array = new_array.reshape((ori_shape[1], ori_shape[2], ori_shape[0]))\n        del c_hw_\n        del hw_c_\n        del new_array\n        return hwc_array\n\n# \u81ea\u5b9a\u4e49OCR\u8bc6\u522b\u4efb\u52a1\u7c7b\nclass OCRRecognitionApp(AIBase):\n    def __init__(self,kmodel_path,model_input_size,dict_path,rgb888p_size=[1920,1080],display_size=[1920,1080],debug_mode=0):\n        super().__init__(kmodel_path,model_input_size,rgb888p_size,debug_mode)\n        # kmodel\u8def\u5f84\n        self.kmodel_path=kmodel_path\n        # \u8bc6\u522b\u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.model_input_size=model_input_size\n        self.dict_path=dict_path\n        # sensor\u7ed9\u5230AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.rgb888p_size=[ALIGN_UP(rgb888p_size[0],16),rgb888p_size[1]]\n        # \u89c6\u9891\u8f93\u51faVO\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.display_size=[ALIGN_UP(display_size[0],16),display_size[1]]\n        # debug\u6a21\u5f0f\n        self.debug_mode=debug_mode\n        self.dict_word=None\n        # \u8bfb\u53d6OCR\u7684\u5b57\u5178\n        self.read_dict()\n        self.ai2d=Ai2d(debug_mode)\n        self.ai2d.set_ai2d_dtype(nn.ai2d_format.RGB_packed,nn.ai2d_format.NCHW_FMT,np.uint8, np.uint8)\n\n    # \u914d\u7f6e\u9884\u5904\u7406\u64cd\u4f5c\uff0c\u8fd9\u91cc\u4f7f\u7528\u4e86pad\u548cresize\uff0cAi2d\u652f\u6301crop/shift/pad/resize/affine\uff0c\u5177\u4f53\u4ee3\u7801\u8bf7\u6253\u5f00/sdcard/app/libs/AI2D.py\u67e5\u770b\n    def config_preprocess(self,input_image_size=None,input_np=None):\n        with ScopedTiming("set preprocess config",self.debug_mode > 0):\n            ai2d_input_size=input_image_size if input_image_size else self.rgb888p_size\n            top,bottom,left,right=self.get_padding_param(ai2d_input_size,self.model_input_size)\n            self.ai2d.pad([0,0,0,0,top,bottom,left,right], 0, [0,0,0])\n            self.ai2d.resize(nn.interp_method.tf_bilinear, nn.interp_mode.half_pixel)\n            # \u5982\u679c\u4f20\u5165input_np\uff0c\u8f93\u5165shape\u4e3ainput_np\u7684shape,\u5982\u679c\u4e0d\u4f20\u5165\uff0c\u8f93\u5165shape\u4e3a[1,3,ai2d_input_size[1],ai2d_input_size[0]]\n            self.ai2d.build([input_np.shape[0],input_np.shape[1],input_np.shape[2],input_np.shape[3]],[1,3,self.model_input_size[1],self.model_input_size[0]])\n\n    # \u81ea\u5b9a\u4e49\u540e\u5904\u7406\uff0cresults\u662f\u6a21\u578b\u8f93\u51fa\u7684array\u5217\u8868\n    def postprocess(self,results):\n        with ScopedTiming("postprocess",self.debug_mode > 0):\n            preds = np.argmax(results[0], axis=2).reshape((-1))\n            output_txt = ""\n            for i in range(len(preds)):\n                # \u5f53\u524d\u8bc6\u522b\u5b57\u7b26\u4e0d\u662f\u5b57\u5178\u7684\u6700\u540e\u4e00\u4e2a\u5b57\u7b26\u5e76\u4e14\u548c\u524d\u4e00\u4e2a\u5b57\u7b26\u4e0d\u91cd\u590d\uff08\u53bb\u91cd\uff09\uff0c\u52a0\u5165\u8bc6\u522b\u7ed3\u679c\u5b57\u7b26\u4e32\n                if preds[i] != (len(self.dict_word) - 1) and (not (i > 0 and preds[i - 1] == preds[i])):\n                    output_txt = output_txt + self.dict_word[preds[i]]\n            return output_txt\n\n    # \u8ba1\u7b97padding\u53c2\u6570\n    def get_padding_param(self,src_size,dst_size):\n        # \u53f3padding\u6216\u4e0bpadding\n        dst_w = dst_size[0]\n        dst_h = dst_size[1]\n        input_width = src_size[0]\n        input_high = src_size[1]\n        ratio_w = dst_w / input_width\n        ratio_h = dst_h / input_high\n        if ratio_w < ratio_h:\n            ratio = ratio_w\n        else:\n            ratio = ratio_h\n        new_w = (int)(ratio * input_width)\n        new_h = (int)(ratio * input_high)\n        dw = (dst_w - new_w) / 2\n        dh = (dst_h - new_h) / 2\n        top = (int)(round(0))\n        bottom = (int)(round(dh * 2 + 0.1))\n        left = (int)(round(0))\n        right = (int)(round(dw * 2 - 0.1))\n        return  top, bottom, left, right\n\n    def read_dict(self):\n        if self.dict_path!="":\n            with open(dict_path, \'r\') as file:\n                line_one = file.read(100000)\n                line_list = line_one.split("\\r\\n")\n            self.dict_word = {num: char.replace("\\r", "").replace("\\n", "") for num, char in enumerate(line_list)}\n\n\nclass OCRDetRec:\n    def __init__(self,ocr_det_kmodel,ocr_rec_kmodel,det_input_size,rec_input_size,dict_path,mask_threshold=0.25,box_threshold=0.3,rgb888p_size=[1920,1080],display_size=[1920,1080],debug_mode=0):\n        # OCR\u68c0\u6d4b\u6a21\u578b\u8def\u5f84\n        self.ocr_det_kmodel=ocr_det_kmodel\n        # OCR\u8bc6\u522b\u6a21\u578b\u8def\u5f84\n        self.ocr_rec_kmodel=ocr_rec_kmodel\n        # OCR\u68c0\u6d4b\u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.det_input_size=det_input_size\n        # OCR\u8bc6\u522b\u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.rec_input_size=rec_input_size\n        # \u5b57\u5178\u8def\u5f84\n        self.dict_path=dict_path\n        # \u7f6e\u4fe1\u5ea6\u9608\u503c\n        self.mask_threshold=mask_threshold\n        # nms\u9608\u503c\n        self.box_threshold=box_threshold\n        # sensor\u7ed9\u5230AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.rgb888p_size=[ALIGN_UP(rgb888p_size[0],16),rgb888p_size[1]]\n        # \u89c6\u9891\u8f93\u51faVO\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.display_size=[ALIGN_UP(display_size[0],16),display_size[1]]\n        # debug_mode\u6a21\u5f0f\n        self.debug_mode=debug_mode\n        self.ocr_det=OCRDetectionApp(self.ocr_det_kmodel,model_input_size=self.det_input_size,mask_threshold=self.mask_threshold,box_threshold=self.box_threshold,rgb888p_size=self.rgb888p_size,display_size=self.display_size,debug_mode=0)\n        self.ocr_rec=OCRRecognitionApp(self.ocr_rec_kmodel,model_input_size=self.rec_input_size,dict_path=self.dict_path,rgb888p_size=self.rgb888p_size,display_size=self.display_size)\n        self.ocr_det.config_preprocess()\n\n    # run\u51fd\u6570\n    def run(self,input_np):\n        # \u5148\u8fdb\u884cOCR\u68c0\u6d4b\n        det_res=self.ocr_det.run(input_np)\n        boxes=[]\n        ocr_res=[]\n        for det in det_res:\n            # \u5bf9\u5f97\u5230\u7684\u6bcf\u4e2a\u68c0\u6d4b\u6846\u6267\u884cOCR\u8bc6\u522b\n            self.ocr_rec.config_preprocess(input_image_size=[det[0].shape[2],det[0].shape[1]],input_np=det[0])\n            ocr_str=self.ocr_rec.run(det[0])\n            ocr_res.append(ocr_str)\n            boxes.append(det[1])\n            gc.collect()\n        return boxes,ocr_res\n\n    # \u7ed8\u5236OCR\u68c0\u6d4b\u8bc6\u522b\u6548\u679c\n    def draw_result(self,pl,det_res,rec_res):\n        pl.osd_img.clear()\n        if det_res:\n            # \u5faa\u73af\u7ed8\u5236\u6240\u6709\u68c0\u6d4b\u5230\u7684\u6846\n            for j in range(len(det_res)):\n                # \u5c06\u539f\u56fe\u7684\u5750\u6807\u70b9\u8f6c\u6362\u6210\u663e\u793a\u7684\u5750\u6807\u70b9\uff0c\u5faa\u73af\u7ed8\u5236\u56db\u6761\u76f4\u7ebf\uff0c\u5f97\u5230\u4e00\u4e2a\u77e9\u5f62\u6846\n                for i in range(4):\n                    x1 = det_res[j][(i * 2)] / self.rgb888p_size[0] * self.display_size[0]\n                    y1 = det_res[j][(i * 2 + 1)] / self.rgb888p_size[1] * self.display_size[1]\n                    x2 = det_res[j][((i + 1) * 2) % 8] / self.rgb888p_size[0] * self.display_size[0]\n                    y2 = det_res[j][((i + 1) * 2 + 1) % 8] / self.rgb888p_size[1] * self.display_size[1]\n                    pl.osd_img.draw_line((int(x1), int(y1), int(x2), int(y2)), color=(255, 0, 0, 255),thickness=5)\n                pl.osd_img.draw_string_advanced(int(x1),int(y1),32,rec_res[j],color=(0,0,255))\n\n\nif __name__=="__main__":\n    # \u663e\u793a\u6a21\u5f0f\uff0c\u9ed8\u8ba4"hdmi",\u53ef\u4ee5\u9009\u62e9"hdmi"\u548c"lcd"\n    display_mode="hdmi"\n    if display_mode=="hdmi":\n        display_size=[1920,1080]\n    else:\n        display_size=[800,480]\n    # OCR\u68c0\u6d4b\u6a21\u578b\u8def\u5f84\n    ocr_det_kmodel_path="/sdcard/app/tests/kmodel/ocr_det_int16.kmodel"\n    # OCR\u8bc6\u522b\u6a21\u578b\u8def\u5f84\n    ocr_rec_kmodel_path="/sdcard/app/tests/kmodel/ocr_rec_int16.kmodel"\n    # \u5176\u4ed6\u53c2\u6570\n    dict_path="/sdcard/app/tests/utils/dict.txt"\n    rgb888p_size=[640,360]\n    ocr_det_input_size=[640,640]\n    ocr_rec_input_size=[512,32]\n    mask_threshold=0.25\n    box_threshold=0.3\n\n    # \u521d\u59cb\u5316PipeLine\uff0c\u53ea\u5173\u6ce8\u4f20\u7ed9AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u663e\u793a\u7684\u5206\u8fa8\u7387\n    pl=PipeLine(rgb888p_size=rgb888p_size,display_size=display_size,display_mode=display_mode)\n    pl.create()\n    ocr=OCRDetRec(ocr_det_kmodel_path,ocr_rec_kmodel_path,det_input_size=ocr_det_input_size,rec_input_size=ocr_rec_input_size,dict_path=dict_path,mask_threshold=mask_threshold,box_threshold=box_threshold,rgb888p_size=rgb888p_size,display_size=display_size)\n    try:\n        while True:\n            os.exitpoint()\n            with ScopedTiming("total",1):\n                img=pl.get_frame()                  # \u83b7\u53d6\u5f53\u524d\u5e27\n                det_res,rec_res=ocr.run(img)        # \u63a8\u7406\u5f53\u524d\u5e27\n                ocr.draw_result(pl,det_res,rec_res) # \u7ed8\u5236\u5f53\u524d\u5e27\u63a8\u7406\u7ed3\u679c\n                pl.show_image()                     # \u5c55\u793a\u5f53\u524d\u5e27\u63a8\u7406\u7ed3\u679c\n                gc.collect()\n    except Exception as e:\n        sys.print_exception(e)\n    finally:\n        ocr.ocr_det.deinit()\n        ocr.ocr_rec.deinit()\n        pl.destroy()\n'})}),"\n",(0,i.jsx)(n.h3,{id:"223-\u4eba\u4f53\u68c0\u6d4b",children:"2.23. \u4eba\u4f53\u68c0\u6d4b"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:'from libs.PipeLine import PipeLine, ScopedTiming\nfrom libs.AIBase import AIBase\nfrom libs.AI2D import Ai2d\nimport os\nimport ujson\nfrom media.media import *\nfrom time import *\nimport nncase_runtime as nn\nimport ulab.numpy as np\nimport time\nimport utime\nimport image\nimport random\nimport gc\nimport sys\nimport aicube\n\n# \u81ea\u5b9a\u4e49\u4eba\u4f53\u68c0\u6d4b\u7c7b\nclass PersonDetectionApp(AIBase):\n    def __init__(self,kmodel_path,model_input_size,labels,anchors,confidence_threshold=0.2,nms_threshold=0.5,nms_option=False,strides=[8,16,32],rgb888p_size=[224,224],display_size=[1920,1080],debug_mode=0):\n        super().__init__(kmodel_path,model_input_size,rgb888p_size,debug_mode)\n        self.kmodel_path=kmodel_path\n        # \u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.model_input_size=model_input_size\n        # \u6807\u7b7e\n        self.labels=labels\n        # \u68c0\u6d4banchors\u8bbe\u7f6e\n        self.anchors=anchors\n        # \u7279\u5f81\u56fe\u964d\u91c7\u6837\u500d\u6570\n        self.strides=strides\n        # \u7f6e\u4fe1\u5ea6\u9608\u503c\u8bbe\u7f6e\n        self.confidence_threshold=confidence_threshold\n        # nms\u9608\u503c\u8bbe\u7f6e\n        self.nms_threshold=nms_threshold\n        self.nms_option=nms_option\n        # sensor\u7ed9\u5230AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\n        self.rgb888p_size=[ALIGN_UP(rgb888p_size[0],16),rgb888p_size[1]]\n        # \u663e\u793a\u5206\u8fa8\u7387\n        self.display_size=[ALIGN_UP(display_size[0],16),display_size[1]]\n        self.debug_mode=debug_mode\n        # Ai2d\u5b9e\u4f8b\uff0c\u7528\u4e8e\u5b9e\u73b0\u6a21\u578b\u9884\u5904\u7406\n        self.ai2d=Ai2d(debug_mode)\n        # \u8bbe\u7f6eAi2d\u7684\u8f93\u5165\u8f93\u51fa\u683c\u5f0f\u548c\u7c7b\u578b\n        self.ai2d.set_ai2d_dtype(nn.ai2d_format.NCHW_FMT,nn.ai2d_format.NCHW_FMT,np.uint8, np.uint8)\n\n    # \u914d\u7f6e\u9884\u5904\u7406\u64cd\u4f5c\uff0c\u8fd9\u91cc\u4f7f\u7528\u4e86pad\u548cresize\uff0cAi2d\u652f\u6301crop/shift/pad/resize/affine\uff0c\u5177\u4f53\u4ee3\u7801\u8bf7\u6253\u5f00/sdcard/app/libs/AI2D.py\u67e5\u770b\n    def config_preprocess(self,input_image_size=None):\n        with ScopedTiming("set preprocess config",self.debug_mode > 0):\n            # \u521d\u59cb\u5316ai2d\u9884\u5904\u7406\u914d\u7f6e\uff0c\u9ed8\u8ba4\u4e3asensor\u7ed9\u5230AI\u7684\u5c3a\u5bf8\uff0c\u60a8\u53ef\u4ee5\u901a\u8fc7\u8bbe\u7f6einput_image_size\u81ea\u884c\u4fee\u6539\u8f93\u5165\u5c3a\u5bf8\n            ai2d_input_size=input_image_size if input_image_size else self.rgb888p_size\n            top,bottom,left,right=self.get_padding_param()\n            self.ai2d.pad([0,0,0,0,top,bottom,left,right], 0, [0,0,0])\n            self.ai2d.resize(nn.interp_method.tf_bilinear, nn.interp_mode.half_pixel)\n            self.ai2d.build([1,3,ai2d_input_size[1],ai2d_input_size[0]],[1,3,self.model_input_size[1],self.model_input_size[0]])\n\n    # \u81ea\u5b9a\u4e49\u5f53\u524d\u4efb\u52a1\u7684\u540e\u5904\u7406\n    def postprocess(self,results):\n        with ScopedTiming("postprocess",self.debug_mode > 0):\n            # \u8fd9\u91cc\u4f7f\u7528\u4e86aicube\u6a21\u578b\u7684\u540e\u5904\u7406\u63a5\u53e3anchorbasedet_post_preocess\n            dets = aicube.anchorbasedet_post_process(results[0], results[1], results[2], self.model_input_size, self.rgb888p_size, self.strides, len(self.labels), self.confidence_threshold, self.nms_threshold, self.anchors, self.nms_option)\n            return dets\n\n    # \u7ed8\u5236\u7ed3\u679c\n    def draw_result(self,pl,dets):\n        with ScopedTiming("display_draw",self.debug_mode >0):\n            if dets:\n                pl.osd_img.clear()\n                for det_box in dets:\n                    x1, y1, x2, y2 = det_box[2],det_box[3],det_box[4],det_box[5]\n                    w = float(x2 - x1) * self.display_size[0] // self.rgb888p_size[0]\n                    h = float(y2 - y1) * self.display_size[1] // self.rgb888p_size[1]\n                    x1 = int(x1 * self.display_size[0] // self.rgb888p_size[0])\n                    y1 = int(y1 * self.display_size[1] // self.rgb888p_size[1])\n                    x2 = int(x2 * self.display_size[0] // self.rgb888p_size[0])\n                    y2 = int(y2 * self.display_size[1] // self.rgb888p_size[1])\n                    if (h<(0.1*self.display_size[0])):\n                        continue\n                    if (w<(0.25*self.display_size[0]) and ((x1<(0.03*self.display_size[0])) or (x2>(0.97*self.display_size[0])))):\n                        continue\n                    if (w<(0.15*self.display_size[0]) and ((x1<(0.01*self.display_size[0])) or (x2>(0.99*self.display_size[0])))):\n                        continue\n                    pl.osd_img.draw_rectangle(x1 , y1 , int(w) , int(h), color=(255, 0, 255, 0), thickness = 2)\n                    pl.osd_img.draw_string_advanced( x1 , y1-50,32, " " + self.labels[det_box[0]] + " " + str(round(det_box[1],2)), color=(255,0, 255, 0))\n            else:\n                pl.osd_img.clear()\n\n    # \u8ba1\u7b97padding\u53c2\u6570\n    def get_padding_param(self):\n        dst_w = self.model_input_size[0]\n        dst_h = self.model_input_size[1]\n        input_width = self.rgb888p_size[0]\n        input_high = self.rgb888p_size[1]\n        ratio_w = dst_w / input_width\n        ratio_h = dst_h / input_high\n        if ratio_w < ratio_h:\n            ratio = ratio_w\n        else:\n            ratio = ratio_h\n        new_w = (int)(ratio * input_width)\n        new_h = (int)(ratio * input_high)\n        dw = (dst_w - new_w) / 2\n        dh = (dst_h - new_h) / 2\n        top = int(round(dh - 0.1))\n        bottom = int(round(dh + 0.1))\n        left = int(round(dw - 0.1))\n        right = int(round(dw - 0.1))\n        return  top, bottom, left, right\n\nif __name__=="__main__":\n    # \u663e\u793a\u6a21\u5f0f\uff0c\u9ed8\u8ba4"hdmi",\u53ef\u4ee5\u9009\u62e9"hdmi"\u548c"lcd"\n    display_mode="hdmi"\n    if display_mode=="hdmi":\n        display_size=[1920,1080]\n    else:\n        display_size=[800,480]\n    # \u6a21\u578b\u8def\u5f84\n    kmodel_path="/sdcard/app/tests/kmodel/person_detect_yolov5n.kmodel"\n    # \u5176\u5b83\u53c2\u6570\u8bbe\u7f6e\n    confidence_threshold = 0.2\n    nms_threshold = 0.6\n    rgb888p_size=[1920,1080]\n    labels = ["person"]\n    anchors = [10, 13, 16, 30, 33, 23, 30, 61, 62, 45, 59, 119, 116, 90, 156, 198, 373, 326]\n\n    # \u521d\u59cb\u5316PipeLine\n    pl=PipeLine(rgb888p_size=rgb888p_size,display_size=display_size,display_mode=display_mode)\n    pl.create()\n    # \u521d\u59cb\u5316\u81ea\u5b9a\u4e49\u4eba\u4f53\u68c0\u6d4b\u5b9e\u4f8b\n    person_det=PersonDetectionApp(kmodel_path,model_input_size=[640,640],labels=labels,anchors=anchors,confidence_threshold=confidence_threshold,nms_threshold=nms_threshold,nms_option=False,strides=[8,16,32],rgb888p_size=rgb888p_size,display_size=display_size,debug_mode=0)\n    person_det.config_preprocess()\n    try:\n        while True:\n            os.exitpoint()\n            with ScopedTiming("total",1):\n                # \u83b7\u53d6\u5f53\u524d\u5e27\u6570\u636e\n                img=pl.get_frame()\n                # \u63a8\u7406\u5f53\u524d\u5e27\n                res=person_det.run(img)\n                # \u7ed8\u5236\u7ed3\u679c\u5230PipeLine\u7684osd\u56fe\u50cf\n                person_det.draw_result(pl,res)\n                # \u663e\u793a\u5f53\u524d\u7684\u7ed8\u5236\u7ed3\u679c\n                pl.show_image()\n                gc.collect()\n    except Exception as e:\n        sys.print_exception(e)\n    finally:\n        person_det.deinit()\n        pl.destroy()\n'})}),"\n",(0,i.jsx)(n.h3,{id:"224-\u4eba\u4f53\u5173\u952e\u70b9\u68c0\u6d4b",children:"2.24. \u4eba\u4f53\u5173\u952e\u70b9\u68c0\u6d4b"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:'from libs.PipeLine import PipeLine, ScopedTiming\nfrom libs.AIBase import AIBase\nfrom libs.AI2D import Ai2d\nimport os\nimport ujson\nfrom media.media import *\nfrom time import *\nimport nncase_runtime as nn\nimport ulab.numpy as np\nimport time\nimport utime\nimport image\nimport random\nimport gc\nimport sys\nimport aidemo\n\n# \u81ea\u5b9a\u4e49\u4eba\u4f53\u5173\u952e\u70b9\u68c0\u6d4b\u7c7b\nclass PersonKeyPointApp(AIBase):\n    def __init__(self,kmodel_path,model_input_size,confidence_threshold=0.2,nms_threshold=0.5,rgb888p_size=[1280,720],display_size=[1920,1080],debug_mode=0):\n        super().__init__(kmodel_path,model_input_size,rgb888p_size,debug_mode)\n        self.kmodel_path=kmodel_path\n        # \u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.model_input_size=model_input_size\n        # \u7f6e\u4fe1\u5ea6\u9608\u503c\u8bbe\u7f6e\n        self.confidence_threshold=confidence_threshold\n        # nms\u9608\u503c\u8bbe\u7f6e\n        self.nms_threshold=nms_threshold\n        # sensor\u7ed9\u5230AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\n        self.rgb888p_size=[ALIGN_UP(rgb888p_size[0],16),rgb888p_size[1]]\n        # \u663e\u793a\u5206\u8fa8\u7387\n        self.display_size=[ALIGN_UP(display_size[0],16),display_size[1]]\n        self.debug_mode=debug_mode\n        #\u9aa8\u9abc\u4fe1\u606f\n        self.SKELETON = [(16, 14),(14, 12),(17, 15),(15, 13),(12, 13),(6,  12),(7,  13),(6,  7),(6,  8),(7,  9),(8,  10),(9,  11),(2,  3),(1,  2),(1,  3),(2,  4),(3,  5),(4,  6),(5,  7)]\n        #\u80a2\u4f53\u989c\u8272\n        self.LIMB_COLORS = [(255, 51,  153, 255),(255, 51,  153, 255),(255, 51,  153, 255),(255, 51,  153, 255),(255, 255, 51,  255),(255, 255, 51,  255),(255, 255, 51,  255),(255, 255, 128, 0),(255, 255, 128, 0),(255, 255, 128, 0),(255, 255, 128, 0),(255, 255, 128, 0),(255, 0,   255, 0),(255, 0,   255, 0),(255, 0,   255, 0),(255, 0,   255, 0),(255, 0,   255, 0),(255, 0,   255, 0),(255, 0,   255, 0)]\n        #\u5173\u952e\u70b9\u989c\u8272\uff0c\u517117\u4e2a\n        self.KPS_COLORS = [(255, 0,   255, 0),(255, 0,   255, 0),(255, 0,   255, 0),(255, 0,   255, 0),(255, 0,   255, 0),(255, 255, 128, 0),(255, 255, 128, 0),(255, 255, 128, 0),(255, 255, 128, 0),(255, 255, 128, 0),(255, 255, 128, 0),(255, 51,  153, 255),(255, 51,  153, 255),(255, 51,  153, 255),(255, 51,  153, 255),(255, 51,  153, 255),(255, 51,  153, 255)]\n\n        # Ai2d\u5b9e\u4f8b\uff0c\u7528\u4e8e\u5b9e\u73b0\u6a21\u578b\u9884\u5904\u7406\n        self.ai2d=Ai2d(debug_mode)\n        # \u8bbe\u7f6eAi2d\u7684\u8f93\u5165\u8f93\u51fa\u683c\u5f0f\u548c\u7c7b\u578b\n        self.ai2d.set_ai2d_dtype(nn.ai2d_format.NCHW_FMT,nn.ai2d_format.NCHW_FMT,np.uint8, np.uint8)\n\n    # \u914d\u7f6e\u9884\u5904\u7406\u64cd\u4f5c\uff0c\u8fd9\u91cc\u4f7f\u7528\u4e86pad\u548cresize\uff0cAi2d\u652f\u6301crop/shift/pad/resize/affine\uff0c\u5177\u4f53\u4ee3\u7801\u8bf7\u6253\u5f00/sdcard/app/libs/AI2D.py\u67e5\u770b\n    def config_preprocess(self,input_image_size=None):\n        with ScopedTiming("set preprocess config",self.debug_mode > 0):\n            # \u521d\u59cb\u5316ai2d\u9884\u5904\u7406\u914d\u7f6e\uff0c\u9ed8\u8ba4\u4e3asensor\u7ed9\u5230AI\u7684\u5c3a\u5bf8\uff0c\u60a8\u53ef\u4ee5\u901a\u8fc7\u8bbe\u7f6einput_image_size\u81ea\u884c\u4fee\u6539\u8f93\u5165\u5c3a\u5bf8\n            ai2d_input_size=input_image_size if input_image_size else self.rgb888p_size\n            top,bottom,left,right=self.get_padding_param()\n            self.ai2d.pad([0,0,0,0,top,bottom,left,right], 0, [0,0,0])\n            self.ai2d.resize(nn.interp_method.tf_bilinear, nn.interp_mode.half_pixel)\n            self.ai2d.build([1,3,ai2d_input_size[1],ai2d_input_size[0]],[1,3,self.model_input_size[1],self.model_input_size[0]])\n\n    # \u81ea\u5b9a\u4e49\u5f53\u524d\u4efb\u52a1\u7684\u540e\u5904\u7406\n    def postprocess(self,results):\n        with ScopedTiming("postprocess",self.debug_mode > 0):\n            # \u8fd9\u91cc\u4f7f\u7528\u4e86aidemo\u5e93\u7684person_kp_postprocess\u63a5\u53e3\n            results = aidemo.person_kp_postprocess(results[0],[self.rgb888p_size[1],self.rgb888p_size[0]],self.model_input_size,self.confidence_threshold,self.nms_threshold)\n            return results\n\n    #\u7ed8\u5236\u7ed3\u679c\uff0c\u7ed8\u5236\u4eba\u4f53\u5173\u952e\u70b9\n    def draw_result(self,pl,res):\n        with ScopedTiming("display_draw",self.debug_mode >0):\n            if res[0]:\n                pl.osd_img.clear()\n                kpses = res[1]\n                for i in range(len(res[0])):\n                    for k in range(17+2):\n                        if (k < 17):\n                            kps_x,kps_y,kps_s = round(kpses[i][k][0]),round(kpses[i][k][1]),kpses[i][k][2]\n                            kps_x1 = int(float(kps_x) * self.display_size[0] // self.rgb888p_size[0])\n                            kps_y1 = int(float(kps_y) * self.display_size[1] // self.rgb888p_size[1])\n                            if (kps_s > 0):\n                                pl.osd_img.draw_circle(kps_x1,kps_y1,5,self.KPS_COLORS[k],4)\n                        ske = self.SKELETON[k]\n                        pos1_x,pos1_y= round(kpses[i][ske[0]-1][0]),round(kpses[i][ske[0]-1][1])\n                        pos1_x_ = int(float(pos1_x) * self.display_size[0] // self.rgb888p_size[0])\n                        pos1_y_ = int(float(pos1_y) * self.display_size[1] // self.rgb888p_size[1])\n\n                        pos2_x,pos2_y = round(kpses[i][(ske[1] -1)][0]),round(kpses[i][(ske[1] -1)][1])\n                        pos2_x_ = int(float(pos2_x) * self.display_size[0] // self.rgb888p_size[0])\n                        pos2_y_ = int(float(pos2_y) * self.display_size[1] // self.rgb888p_size[1])\n\n                        pos1_s,pos2_s = kpses[i][(ske[0] -1)][2],kpses[i][(ske[1] -1)][2]\n                        if (pos1_s > 0.0 and pos2_s >0.0):\n                            pl.osd_img.draw_line(pos1_x_,pos1_y_,pos2_x_,pos2_y_,self.LIMB_COLORS[k],4)\n                    gc.collect()\n            else:\n                pl.osd_img.clear()\n\n    # \u8ba1\u7b97padding\u53c2\u6570\n    def get_padding_param(self):\n        dst_w = self.model_input_size[0]\n        dst_h = self.model_input_size[1]\n        input_width = self.rgb888p_size[0]\n        input_high = self.rgb888p_size[1]\n        ratio_w = dst_w / input_width\n        ratio_h = dst_h / input_high\n        if ratio_w < ratio_h:\n            ratio = ratio_w\n        else:\n            ratio = ratio_h\n        new_w = (int)(ratio * input_width)\n        new_h = (int)(ratio * input_high)\n        dw = (dst_w - new_w) / 2\n        dh = (dst_h - new_h) / 2\n        top = int(round(dh - 0.1))\n        bottom = int(round(dh + 0.1))\n        left = int(round(dw - 0.1))\n        right = int(round(dw - 0.1))\n        return  top, bottom, left, right\n\nif __name__=="__main__":\n    # \u663e\u793a\u6a21\u5f0f\uff0c\u9ed8\u8ba4"hdmi",\u53ef\u4ee5\u9009\u62e9"hdmi"\u548c"lcd"\n    display_mode="hdmi"\n    if display_mode=="hdmi":\n        display_size=[1920,1080]\n    else:\n        display_size=[800,480]\n    # \u6a21\u578b\u8def\u5f84\n    kmodel_path="/sdcard/app/tests/kmodel/yolov8n-pose.kmodel"\n    # \u5176\u5b83\u53c2\u6570\u8bbe\u7f6e\n    confidence_threshold = 0.2\n    nms_threshold = 0.5\n    rgb888p_size=[1920,1080]\n    # \u521d\u59cb\u5316PipeLine\n    pl=PipeLine(rgb888p_size=rgb888p_size,display_size=display_size,display_mode=display_mode)\n    pl.create()\n    # \u521d\u59cb\u5316\u81ea\u5b9a\u4e49\u4eba\u4f53\u5173\u952e\u70b9\u68c0\u6d4b\u5b9e\u4f8b\n    person_kp=PersonKeyPointApp(kmodel_path,model_input_size=[320,320],confidence_threshold=confidence_threshold,nms_threshold=nms_threshold,rgb888p_size=rgb888p_size,display_size=display_size,debug_mode=0)\n    person_kp.config_preprocess()\n    try:\n        while True:\n            os.exitpoint()\n            with ScopedTiming("total",1):\n                # \u83b7\u53d6\u5f53\u524d\u5e27\u6570\u636e\n                img=pl.get_frame()\n                # \u63a8\u7406\u5f53\u524d\u5e27\n                res=person_kp.run(img)\n                # \u7ed8\u5236\u7ed3\u679c\u5230PipeLine\u7684osd\u56fe\u50cf\n                person_kp.draw_result(pl,res)\n                # \u663e\u793a\u5f53\u524d\u7684\u7ed8\u5236\u7ed3\u679c\n                pl.show_image()\n                gc.collect()\n    except Exception as e:\n        sys.print_exception(e)\n    finally:\n        person_kp.deinit()\n        pl.destroy()\n'})}),"\n",(0,i.jsx)(n.h3,{id:"225-\u62fc\u56fe\u6e38\u620f",children:"2.25. \u62fc\u56fe\u6e38\u620f"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:'from libs.PipeLine import PipeLine, ScopedTiming\nfrom libs.AIBase import AIBase\nfrom libs.AI2D import Ai2d\nimport os\nimport ujson\nfrom media.media import *\nfrom time import *\nimport nncase_runtime as nn\nimport ulab.numpy as np\nimport time\nimport image\nimport aicube\nimport random\nimport gc\nimport sys\n\n# \u81ea\u5b9a\u4e49\u624b\u638c\u68c0\u6d4b\u4efb\u52a1\u7c7b\nclass HandDetApp(AIBase):\n    def __init__(self,kmodel_path,labels,model_input_size,anchors,confidence_threshold=0.2,nms_threshold=0.5,nms_option=False, strides=[8,16,32],rgb888p_size=[1920,1080],display_size=[1920,1080],debug_mode=0):\n        super().__init__(kmodel_path,model_input_size,rgb888p_size,debug_mode)\n        # kmodel\u8def\u5f84\n        self.kmodel_path=kmodel_path\n        self.labels=labels\n        # \u68c0\u6d4b\u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.model_input_size=model_input_size\n        # \u7f6e\u4fe1\u5ea6\u9608\u503c\n        self.confidence_threshold=confidence_threshold\n        # nms\u9608\u503c\n        self.nms_threshold=nms_threshold\n        self.anchors=anchors\n        self.strides = strides  # \u7279\u5f81\u4e0b\u91c7\u6837\u500d\u6570\n        self.nms_option = nms_option  # NMS\u9009\u9879\uff0c\u5982\u679c\u4e3aTrue\u505a\u7c7b\u95f4NMS,\u5982\u679c\u4e3aFalse\u505a\u7c7b\u5185NMS\n        # sensor\u7ed9\u5230AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.rgb888p_size=[ALIGN_UP(rgb888p_size[0],16),rgb888p_size[1]]\n        # \u89c6\u9891\u8f93\u51faVO\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.display_size=[ALIGN_UP(display_size[0],16),display_size[1]]\n        # debug\u6a21\u5f0f\n        self.debug_mode=debug_mode\n        self.ai2d=Ai2d(debug_mode)\n        self.ai2d.set_ai2d_dtype(nn.ai2d_format.NCHW_FMT,nn.ai2d_format.NCHW_FMT,np.uint8, np.uint8)\n\n    def config_preprocess(self,input_image_size=None):\n        with ScopedTiming("set preprocess config",self.debug_mode > 0):\n            # \u521d\u59cb\u5316ai2d\u9884\u5904\u7406\u914d\u7f6e\uff0c\u9ed8\u8ba4\u4e3asensor\u7ed9\u5230AI\u7684\u5c3a\u5bf8\uff0c\u53ef\u4ee5\u901a\u8fc7\u8bbe\u7f6einput_image_size\u81ea\u884c\u4fee\u6539\u8f93\u5165\u5c3a\u5bf8\n            ai2d_input_size = input_image_size if input_image_size else self.rgb888p_size\n            # \u8ba1\u7b97padding\u53c2\u6570\u5e76\u5e94\u7528pad\u64cd\u4f5c\uff0c\u4ee5\u786e\u4fdd\u8f93\u5165\u56fe\u50cf\u5c3a\u5bf8\u4e0e\u6a21\u578b\u8f93\u5165\u5c3a\u5bf8\u5339\u914d\n            top, bottom, left, right = self.get_padding_param()\n            self.ai2d.pad([0, 0, 0, 0, top, bottom, left, right], 0, [114, 114, 114])\n            # \u4f7f\u7528\u53cc\u7ebf\u6027\u63d2\u503c\u8fdb\u884cresize\u64cd\u4f5c\uff0c\u8c03\u6574\u56fe\u50cf\u5c3a\u5bf8\u4ee5\u7b26\u5408\u6a21\u578b\u8f93\u5165\u8981\u6c42\n            self.ai2d.resize(nn.interp_method.tf_bilinear, nn.interp_mode.half_pixel)\n            # \u6784\u5efa\u9884\u5904\u7406\u6d41\u7a0b\n            self.ai2d.build([1,3,ai2d_input_size[1],ai2d_input_size[0]],[1,3,self.model_input_size[1],self.model_input_size[0]])\n\n    # \u81ea\u5b9a\u4e49\u540e\u5904\u7406\uff0cresults\u662f\u6a21\u578b\u7684\u8f93\u51faarray\u5217\u8868\uff0c\u8fd9\u91cc\u4f7f\u7528\u4e86aicube\u5e93\u7684anchorbasedet_post_process\n    def postprocess(self,results):\n        with ScopedTiming("postprocess",self.debug_mode > 0):\n            dets = aicube.anchorbasedet_post_process(results[0], results[1], results[2], self.model_input_size, self.rgb888p_size, self.strides, len(self.labels), self.confidence_threshold, self.nms_threshold, self.anchors, self.nms_option)\n            # \u8fd4\u56de\u624b\u638c\u68c0\u6d4b\u7ed3\u679c\n            return dets\n\n    # \u8ba1\u7b97padding\u53c2\u6570\uff0c\u786e\u4fdd\u8f93\u5165\u56fe\u50cf\u5c3a\u5bf8\u4e0e\u6a21\u578b\u8f93\u5165\u5c3a\u5bf8\u5339\u914d\n    def get_padding_param(self):\n        # \u6839\u636e\u76ee\u6807\u5bbd\u5ea6\u548c\u9ad8\u5ea6\u8ba1\u7b97\u6bd4\u4f8b\u56e0\u5b50\n        dst_w = self.model_input_size[0]\n        dst_h = self.model_input_size[1]\n        input_width = self.rgb888p_size[0]\n        input_high = self.rgb888p_size[1]\n        ratio_w = dst_w / input_width\n        ratio_h = dst_h / input_high\n        # \u9009\u62e9\u8f83\u5c0f\u7684\u6bd4\u4f8b\u56e0\u5b50\uff0c\u4ee5\u786e\u4fdd\u56fe\u50cf\u5185\u5bb9\u5b8c\u6574\n        if ratio_w < ratio_h:\n            ratio = ratio_w\n        else:\n            ratio = ratio_h\n        # \u8ba1\u7b97\u65b0\u7684\u5bbd\u5ea6\u548c\u9ad8\u5ea6\n        new_w = int(ratio * input_width)\n        new_h = int(ratio * input_high)\n        # \u8ba1\u7b97\u5bbd\u5ea6\u548c\u9ad8\u5ea6\u7684\u5dee\u503c\uff0c\u5e76\u786e\u5b9apadding\u7684\u4f4d\u7f6e\n        dw = (dst_w - new_w) / 2\n        dh = (dst_h - new_h) / 2\n        top = int(round(dh - 0.1))\n        bottom = int(round(dh + 0.1))\n        left = int(round(dw - 0.1))\n        right = int(round(dw + 0.1))\n        return top, bottom, left, right\n\n# \u81ea\u5b9a\u4e49\u624b\u52bf\u5173\u952e\u70b9\u5206\u7c7b\u4efb\u52a1\u7c7b\nclass HandKPClassApp(AIBase):\n    def __init__(self,kmodel_path,model_input_size,rgb888p_size=[1920,1080],display_size=[1920,1080],debug_mode=0):\n        super().__init__(kmodel_path,model_input_size,rgb888p_size,debug_mode)\n        # kmodel\u8def\u5f84\n        self.kmodel_path=kmodel_path\n        # \u68c0\u6d4b\u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.model_input_size=model_input_size\n        # sensor\u7ed9\u5230AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.rgb888p_size=[ALIGN_UP(rgb888p_size[0],16),rgb888p_size[1]]\n        # \u89c6\u9891\u8f93\u51faVO\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.display_size=[ALIGN_UP(display_size[0],16),display_size[1]]\n        self.crop_params=[]\n        # debug\u6a21\u5f0f\n        self.debug_mode=debug_mode\n        self.ai2d=Ai2d(debug_mode)\n        self.ai2d.set_ai2d_dtype(nn.ai2d_format.NCHW_FMT,nn.ai2d_format.NCHW_FMT,np.uint8, np.uint8)\n\n    # \u914d\u7f6e\u9884\u5904\u7406\u64cd\u4f5c\uff0c\u8fd9\u91cc\u4f7f\u7528\u4e86crop\u548cresize\uff0cAi2d\u652f\u6301crop/shift/pad/resize/affine\n    def config_preprocess(self,det,input_image_size=None):\n        with ScopedTiming("set preprocess config",self.debug_mode > 0):\n            ai2d_input_size=input_image_size if input_image_size else self.rgb888p_size\n            self.crop_params = self.get_crop_param(det)\n            self.ai2d.crop(self.crop_params[0],self.crop_params[1],self.crop_params[2],self.crop_params[3])\n            self.ai2d.resize(nn.interp_method.tf_bilinear, nn.interp_mode.half_pixel)\n            self.ai2d.build([1,3,ai2d_input_size[1],ai2d_input_size[0]],[1,3,self.model_input_size[1],self.model_input_size[0]])\n\n    # \u81ea\u5b9a\u4e49\u540e\u5904\u7406\uff0cresults\u662f\u6a21\u578b\u7684\u8f93\u51faarray\u5217\u8868\n    def postprocess(self,results):\n        with ScopedTiming("postprocess",self.debug_mode > 0):\n            results=results[0].reshape(results[0].shape[0]*results[0].shape[1])\n            results_show = np.zeros(results.shape,dtype=np.int16)\n            results_show[0::2] = (results[0::2] * self.crop_params[3] + self.crop_params[0])\n            results_show[1::2] = (results[1::2] * self.crop_params[2] + self.crop_params[1])\n            return results_show\n\n    # \u8ba1\u7b97crop\u53c2\u6570\n    def get_crop_param(self,det_box):\n        x1, y1, x2, y2 = det_box[2],det_box[3],det_box[4],det_box[5]\n        w,h= int(x2 - x1),int(y2 - y1)\n        w_det = int(float(x2 - x1) * self.display_size[0] // self.rgb888p_size[0])\n        h_det = int(float(y2 - y1) * self.display_size[1] // self.rgb888p_size[1])\n        x_det = int(x1*self.display_size[0] // self.rgb888p_size[0])\n        y_det = int(y1*self.display_size[1] // self.rgb888p_size[1])\n        length = max(w, h)/2\n        cx = (x1+x2)/2\n        cy = (y1+y2)/2\n        ratio_num = 1.26*length\n        x1_kp = int(max(0,cx-ratio_num))\n        y1_kp = int(max(0,cy-ratio_num))\n        x2_kp = int(min(self.rgb888p_size[0]-1, cx+ratio_num))\n        y2_kp = int(min(self.rgb888p_size[1]-1, cy+ratio_num))\n        w_kp = int(x2_kp - x1_kp + 1)\n        h_kp = int(y2_kp - y1_kp + 1)\n        return [x1_kp, y1_kp, w_kp, h_kp]\n\n# \u62fc\u56fe\u6e38\u620f\u4efb\u52a1\u7c7b\nclass PuzzleGame:\n    def __init__(self,hand_det_kmodel,hand_kp_kmodel,det_input_size,kp_input_size,labels,anchors,confidence_threshold=0.25,nms_threshold=0.3,nms_option=False,strides=[8,16,32],rgb888p_size=[1280,720],display_size=[1920,1080],debug_mode=0):\n        # \u624b\u638c\u68c0\u6d4b\u6a21\u578b\u8def\u5f84\n        self.hand_det_kmodel=hand_det_kmodel\n        # \u624b\u638c\u5173\u952e\u70b9\u6a21\u578b\u8def\u5f84\n        self.hand_kp_kmodel=hand_kp_kmodel\n        # \u624b\u638c\u68c0\u6d4b\u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.det_input_size=det_input_size\n        # \u624b\u638c\u5173\u952e\u70b9\u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.kp_input_size=kp_input_size\n        self.labels=labels\n        # anchors\n        self.anchors=anchors\n        # \u7f6e\u4fe1\u5ea6\u9608\u503c\n        self.confidence_threshold=confidence_threshold\n        # nms\u9608\u503c\n        self.nms_threshold=nms_threshold\n        self.nms_option=nms_option\n        self.strides=strides\n        # sensor\u7ed9\u5230AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.rgb888p_size=[ALIGN_UP(rgb888p_size[0],16),rgb888p_size[1]]\n        # \u89c6\u9891\u8f93\u51faVO\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.display_size=[ALIGN_UP(display_size[0],16),display_size[1]]\n        # debug_mode\u6a21\u5f0f\n        self.debug_mode=debug_mode\n\n        self.level = 3                                                              # \u6e38\u620f\u7ea7\u522b \u76ee\u524d\u53ea\u652f\u6301\u8bbe\u7f6e\u4e3a 3\n        self.puzzle_width = self.display_size[1]                                    # \u8bbe\u5b9a \u62fc\u56fe\u5bbd\n        self.puzzle_height = self.display_size[1]                                   # \u8bbe\u5b9a \u62fc\u56fe\u9ad8\n        self.puzzle_ori_width = self.display_size[0] - self.puzzle_width - 50       # \u8bbe\u5b9a \u539f\u59cb\u62fc\u56fe\u5bbd\n        self.puzzle_ori_height = self.display_size[0] - self.puzzle_height - 50     # \u8bbe\u5b9a \u539f\u59cb\u62fc\u56fe\u9ad8\n\n        self.every_block_width = int(self.puzzle_width/self.level)                  # \u8bbe\u5b9a \u62fc\u56fe\u5757\u5bbd\n        self.every_block_height = int(self.puzzle_height/self.level)                # \u8bbe\u5b9a \u62fc\u56fe\u5757\u9ad8\n        self.ori_every_block_width = int(self.puzzle_ori_width/self.level)          # \u8bbe\u5b9a \u539f\u59cb\u62fc\u56fe\u5bbd\n        self.ori_every_block_height = int(self.puzzle_ori_height/self.level)        # \u8bbe\u5b9a \u539f\u59cb\u62fc\u56fe\u9ad8\n        self.ratio_num = self.every_block_width/360.0                               # \u5b57\u4f53\u6bd4\u4f8b\n        self.blank_x = 0                                                            # \u7a7a\u767d\u5757 \u89d2\u70b9x\n        self.blank_y = 0                                                            # \u7a7a\u767d\u5757 \u89d2\u70b9y\n        self.direction_vec = [-1,1,-1,1]                                            # \u7a7a\u767d\u5757\u56db\u79cd\u79fb\u52a8\u65b9\u5411\n        self.exact_division_x = 0                                                   # \u4ea4\u6362\u5757 \u89d2\u70b9x\n        self.exact_division_y = 0                                                   # \u4ea4\u6362\u5757 \u89d2\u70b9y\n        self.distance_tow_points = self.display_size[0]                             # \u4e24\u624b\u6307\u8ddd\u79bb\n        self.distance_thred = self.every_block_width*0.4                            # \u4e24\u624b\u6307\u8ddd\u79bb\u9608\u503c\n        self.osd_frame_tmp = np.zeros((self.display_size[1],self.display_size[0],4),dtype=np.uint8)\n        self.osd_frame_tmp_img = image.Image(self.display_size[0], self.display_size[1], image.ARGB8888,alloc=image.ALLOC_REF,data=self.osd_frame_tmp)\n        self.move_mat = np.zeros((self.every_block_height,self.every_block_width,4),dtype=np.uint8)\n        self.init_osd_frame()\n        self.hand_det=HandDetApp(self.hand_det_kmodel,self.labels,model_input_size=self.det_input_size,anchors=self.anchors,confidence_threshold=self.confidence_threshold,nms_threshold=self.nms_threshold,nms_option=self.nms_option,strides=self.strides,rgb888p_size=self.rgb888p_size,display_size=self.display_size,debug_mode=0)\n        self.hand_kp=HandKPClassApp(self.hand_kp_kmodel,model_input_size=self.kp_input_size,rgb888p_size=self.rgb888p_size,display_size=self.display_size)\n        self.hand_det.config_preprocess()\n\n    # \u521d\u59cb\u5316\u62fc\u56fe\u754c\u9762\uff0c\u7ed8\u5236\u4e24\u4e2a3*3\u7684\u62fc\u56fe\n    def init_osd_frame(self):\n        self.osd_frame_tmp[0:self.puzzle_height,0:self.puzzle_width,3] = 100\n        self.osd_frame_tmp[0:self.puzzle_height,0:self.puzzle_width,2] = 150\n        self.osd_frame_tmp[0:self.puzzle_height,0:self.puzzle_width,1] = 130\n        self.osd_frame_tmp[0:self.puzzle_height,0:self.puzzle_width,0] = 127\n        self.osd_frame_tmp[(self.display_size[1]-self.puzzle_ori_height)//2:(self.display_size[1]-self.puzzle_ori_height)//2+self.puzzle_ori_width,self.puzzle_width+25:self.puzzle_width+25+self.puzzle_ori_height,3] = 100\n        self.osd_frame_tmp[(self.display_size[1]-self.puzzle_ori_height)//2:(self.display_size[1]-self.puzzle_ori_height)//2+self.puzzle_ori_width,self.puzzle_width+25:self.puzzle_width+25+self.puzzle_ori_height,2] = 150\n        self.osd_frame_tmp[(self.display_size[1]-self.puzzle_ori_height)//2:(self.display_size[1]-self.puzzle_ori_height)//2+self.puzzle_ori_width,self.puzzle_width+25:self.puzzle_width+25+self.puzzle_ori_height,1] = 130\n        self.osd_frame_tmp[(self.display_size[1]-self.puzzle_ori_height)//2:(self.display_size[1]-self.puzzle_ori_height)//2+self.puzzle_ori_width,self.puzzle_width+25:self.puzzle_width+25+self.puzzle_ori_height,0] = 127\n        for i in range(self.level*self.level):\n            self.osd_frame_tmp_img.draw_rectangle((i%self.level)*self.every_block_width,(i//self.level)*self.every_block_height,self.every_block_width,self.every_block_height,(255,0,0,0),5)\n            self.osd_frame_tmp_img.draw_string_advanced((i%self.level)*self.every_block_width + 55,(i//self.level)*self.every_block_height + 45,int(60*self.ratio_num),str(i),color=(255,0,0,255))\n            self.osd_frame_tmp_img.draw_rectangle(self.puzzle_width+25 + (i%self.level)*self.ori_every_block_width,(self.display_size[1]-self.puzzle_ori_height)//2 + (i//self.level)*self.ori_every_block_height,self.ori_every_block_width,self.ori_every_block_height,(255,0,0,0),5)\n            self.osd_frame_tmp_img.draw_string_advanced(self.puzzle_width+25 + (i%self.level)*self.ori_every_block_width + 50,(self.display_size[1]-self.puzzle_ori_height)//2 + (i//self.level)*self.ori_every_block_height + 25,int(50*self.ratio_num),str(i),color=(255,0,0,255))\n        self.osd_frame_tmp[0:self.every_block_height,0:self.every_block_width,3] = 114\n        self.osd_frame_tmp[0:self.every_block_height,0:self.every_block_width,2] = 114\n        self.osd_frame_tmp[0:self.every_block_height,0:self.every_block_width,1] = 114\n        self.osd_frame_tmp[0:self.every_block_height,0:self.every_block_width,0] = 220\n        self.osd_frame_tmp[(self.display_size[1]-self.puzzle_ori_height)//2:(self.display_size[1]-self.puzzle_ori_height)//2+self.ori_every_block_width,self.puzzle_width+25:self.puzzle_width+25+self.ori_every_block_height,3] = 114\n        self.osd_frame_tmp[(self.display_size[1]-self.puzzle_ori_height)//2:(self.display_size[1]-self.puzzle_ori_height)//2+self.ori_every_block_width,self.puzzle_width+25:self.puzzle_width+25+self.ori_every_block_height,2] = 114\n        self.osd_frame_tmp[(self.display_size[1]-self.puzzle_ori_height)//2:(self.display_size[1]-self.puzzle_ori_height)//2+self.ori_every_block_width,self.puzzle_width+25:self.puzzle_width+25+self.ori_every_block_height,1] = 114\n        self.osd_frame_tmp[(self.display_size[1]-self.puzzle_ori_height)//2:(self.display_size[1]-self.puzzle_ori_height)//2+self.ori_every_block_width,self.puzzle_width+25:self.puzzle_width+25+self.ori_every_block_height,0] = 220\n\n        for i in range(self.level*10):\n            k230_random = int(random.random() * 100) % 4\n            blank_x_tmp = self.blank_x\n            blank_y_tmp = self.blank_y\n            if (k230_random < 2):\n                blank_x_tmp = self.blank_x + self.direction_vec[k230_random]\n            else:\n                blank_y_tmp = self.blank_y + self.direction_vec[k230_random]\n\n            if ((blank_x_tmp >= 0 and blank_x_tmp < self.level) and (blank_y_tmp >= 0 and blank_y_tmp < self.level) and (abs(self.blank_x - blank_x_tmp) <= 1 and abs(self.blank_y - blank_y_tmp) <= 1)):\n                move_rect = [blank_x_tmp*self.every_block_width,blank_y_tmp*self.every_block_height,self.every_block_width,self.every_block_height]\n                blank_rect = [self.blank_x*self.every_block_width,self.blank_y*self.every_block_height,self.every_block_width,self.every_block_height]\n                self.move_mat[:] = self.osd_frame_tmp[move_rect[1]:move_rect[1]+move_rect[3],move_rect[0]:move_rect[0]+move_rect[2],:]\n                self.osd_frame_tmp[move_rect[1]:move_rect[1]+move_rect[3],move_rect[0]:move_rect[0]+move_rect[2],:] = self.osd_frame_tmp[blank_rect[1]:blank_rect[1]+blank_rect[3],blank_rect[0]:blank_rect[0]+blank_rect[2],:]\n                self.osd_frame_tmp[blank_rect[1]:blank_rect[1]+blank_rect[3],blank_rect[0]:blank_rect[0]+blank_rect[2],:] = self.move_mat[:]\n                self.blank_x = blank_x_tmp\n                self.blank_y = blank_y_tmp\n\n    # run\u51fd\u6570\n    def run(self,input_np):\n        # \u5148\u8fdb\u884c\u624b\u638c\u68c0\u6d4b\n        det_boxes=self.hand_det.run(input_np)\n        det_res=[]\n        two_point = np.zeros((4),dtype=np.int16)\n        # \u5bf9\u4e8e\u6bcf\u4e00\u4e2a\u68c0\u6d4b\u5230\u7684\u624b\u638c\u505a\u7b5b\u9009\n        for det_box in det_boxes:\n            x1, y1, x2, y2 = det_box[2],det_box[3],det_box[4],det_box[5]\n            w,h= int(x2 - x1),int(y2 - y1)\n            if (h<(0.1*self.rgb888p_size[1])):\n                continue\n            if (w<(0.25*self.rgb888p_size[0]) and ((x1<(0.03*self.rgb888p_size[0])) or (x2>(0.97*self.rgb888p_size[0])))):\n                continue\n            if (w<(0.15*self.rgb888p_size[0]) and ((x1<(0.01*self.rgb888p_size[0])) or (x2>(0.99*self.rgb888p_size[0])))):\n                continue\n            det_res.append(det_box)\n        if len(det_res)!=0:\n            # \u5bf9\u7b2c\u4e00\u4e2a\u624b\u638c\u505a\u624b\u638c\u5173\u952e\u70b9\u68c0\u6d4b\n            det_box=det_res[0]\n            self.hand_kp.config_preprocess(det_box)\n            results_show=self.hand_kp.run(input_np)\n            two_point[0],two_point[1],two_point[2],two_point[3] = results_show[8],results_show[9],results_show[16+8],results_show[16+9]\n        return det_res,two_point\n\n    # \u7ed8\u5236\u6548\u679c\uff0c\u624b\u6307\u62c7\u6307\u548c\u4e2d\u6307\u4f4d\u7f6e\u5224\u65ad\u62fc\u56fe\u79fb\u52a8\u4f4d\u7f6e\uff0c\u5e76\u4e0e\u5468\u8fb9\u7a7a\u767d\u4f4d\u7f6e\u505a\u4ea4\u6362\n    def draw_result(self,pl,det_res,two_point):\n        pl.osd_img.clear()\n        if len(det_res)==1:\n            if (two_point[1] <= self.rgb888p_size[0]):\n                self.distance_tow_points = np.sqrt(pow((two_point[0]-two_point[2]),2) + pow((two_point[1] - two_point[3]),2))* 1.0 / self.rgb888p_size[0] * self.display_size[0]\n                self.exact_division_x = int((two_point[0] * 1.0 / self.rgb888p_size[0] * self.display_size[0])//self.every_block_width)\n                self.exact_division_y = int((two_point[1] * 1.0 / self.rgb888p_size[1] * self.display_size[1])//self.every_block_height)\n\n\n                if (self.distance_tow_points < self.distance_thred and self.exact_division_x >= 0 and self.exact_division_x < self.level and self.exact_division_y >= 0 and self.exact_division_y < self.level):\n                    if (abs(self.blank_x - self.exact_division_x) == 1 and abs(self.blank_y - self.exact_division_y) == 0):\n                        move_rect = [self.exact_division_x*self.every_block_width,self.exact_division_y*self.every_block_height,self.every_block_width,self.every_block_height]\n                        blank_rect = [self.blank_x*self.every_block_width,self.blank_y*self.every_block_height,self.every_block_width,self.every_block_height]\n\n                        self.move_mat[:] = self.osd_frame_tmp[move_rect[1]:move_rect[1]+move_rect[3],move_rect[0]:move_rect[0]+move_rect[2],:]\n                        self.osd_frame_tmp[move_rect[1]:move_rect[1]+move_rect[3],move_rect[0]:move_rect[0]+move_rect[2],:] = self.osd_frame_tmp[blank_rect[1]:blank_rect[1]+blank_rect[3],blank_rect[0]:blank_rect[0]+blank_rect[2],:]\n                        self.osd_frame_tmp[blank_rect[1]:blank_rect[1]+blank_rect[3],blank_rect[0]:blank_rect[0]+blank_rect[2],:] = self.move_mat[:]\n\n                        self.blank_x = self.exact_division_x\n                    elif (abs(self.blank_y - self.exact_division_y) == 1 and abs(self.blank_x - self.exact_division_x) == 0):\n                        move_rect = [self.exact_division_x*self.every_block_width,self.exact_division_y*self.every_block_height,self.every_block_width,self.every_block_height]\n                        blank_rect = [self.blank_x*self.every_block_width,self.blank_y*self.every_block_height,self.every_block_width,self.every_block_height]\n\n                        self.move_mat[:] = self.osd_frame_tmp[move_rect[1]:move_rect[1]+move_rect[3],move_rect[0]:move_rect[0]+move_rect[2],:]\n                        self.osd_frame_tmp[move_rect[1]:move_rect[1]+move_rect[3],move_rect[0]:move_rect[0]+move_rect[2],:] = self.osd_frame_tmp[blank_rect[1]:blank_rect[1]+blank_rect[3],blank_rect[0]:blank_rect[0]+blank_rect[2],:]\n                        self.osd_frame_tmp[blank_rect[1]:blank_rect[1]+blank_rect[3],blank_rect[0]:blank_rect[0]+blank_rect[2],:] = self.move_mat[:]\n\n                        self.blank_y = self.exact_division_y\n\n                    pl.osd_img.copy_from(self.osd_frame_tmp)\n                    x1 = int(two_point[0] * 1.0 * self.display_size[0] // self.rgb888p_size[0])\n                    y1 = int(two_point[1] * 1.0 * self.display_size[1] // self.rgb888p_size[1])\n                    pl.osd_img.draw_circle(x1, y1, 1, color=(255, 0, 255, 255),thickness=4,fill=False)\n                else:\n                    pl.osd_img.copy_from(self.osd_frame_tmp)\n                    x1 = int(two_point[0] * 1.0 * self.display_size[0] // self.rgb888p_size[0])\n                    y1 = int(two_point[1] * 1.0 * self.display_size[1] // self.rgb888p_size[1])\n                    pl.osd_img.draw_circle(x1, y1, 1, color=(255, 255, 255, 0),thickness=4,fill=False)\n        else:\n            pl.osd_img.copy_from(self.osd_frame_tmp)\n            pl.osd_img.draw_string_advanced((self.display_size[0]//2),(self.display_size[1]//2),32,"\u8bf7\u4fdd\u8bc1\u4e00\u53ea\u624b\u5165\u955c!",color=(255,0,0))\n\n\n\nif __name__=="__main__":\n    # \u663e\u793a\u6a21\u5f0f\uff0c\u9ed8\u8ba4"hdmi",\u53ef\u4ee5\u9009\u62e9"hdmi"\u548c"lcd"\n    display_mode="hdmi"\n    if display_mode=="hdmi":\n        display_size=[1920,1080]\n    else:\n        display_size=[800,480]\n    # \u624b\u638c\u68c0\u6d4b\u6a21\u578b\u8def\u5f84\n    hand_det_kmodel_path="/sdcard/app/tests/kmodel/hand_det.kmodel"\n    # \u624b\u638c\u5173\u952e\u70b9\u6a21\u578b\u8def\u5f84\n    hand_kp_kmodel_path="/sdcard/app/tests/kmodel/handkp_det.kmodel"\n    # \u5176\u4ed6\u53c2\u6570\n    anchors_path="/sdcard/app/tests/utils/prior_data_320.bin"\n    rgb888p_size=[1920,1080]\n    hand_det_input_size=[512,512]\n    hand_kp_input_size=[256,256]\n    confidence_threshold=0.2\n    nms_threshold=0.5\n    labels=["hand"]\n    anchors = [26,27, 53,52, 75,71, 80,99, 106,82, 99,134, 140,113, 161,172, 245,276]\n\n    # \u521d\u59cb\u5316PipeLine\uff0c\u53ea\u5173\u6ce8\u4f20\u7ed9AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u663e\u793a\u7684\u5206\u8fa8\u7387\n    pl=PipeLine(rgb888p_size=rgb888p_size,display_size=display_size,display_mode=display_mode)\n    pl.create()\n    pg=PuzzleGame(hand_det_kmodel_path,hand_kp_kmodel_path,det_input_size=hand_det_input_size,kp_input_size=hand_kp_input_size,labels=labels,anchors=anchors,confidence_threshold=confidence_threshold,nms_threshold=nms_threshold,nms_option=False,strides=[8,16,32],rgb888p_size=rgb888p_size,display_size=display_size)\n    try:\n        while True:\n            os.exitpoint()\n            with ScopedTiming("total",1):\n                img=pl.get_frame()                      # \u83b7\u53d6\u5f53\u524d\u5e27\n                det_res,two_point=pg.run(img)           # \u63a8\u7406\u5f53\u524d\u5e27\n                pg.draw_result(pl,det_res,two_point)    # \u7ed8\u5236\u5f53\u524d\u5e27\u63a8\u7406\u7ed3\u679c\n                pl.show_image()                         # \u5c55\u793a\u63a8\u7406\u7ed3\u679c\n                gc.collect()\n    except Exception as e:\n        sys.print_exception(e)\n    finally:\n        pg.hand_det.deinit()\n        pg.hand_kp.deinit()\n        pl.destroy()\n'})}),"\n",(0,i.jsx)(n.h3,{id:"226-yolov8\u5206\u5272",children:"2.26. yolov8\u5206\u5272"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:'from libs.PipeLine import PipeLine, ScopedTiming\nfrom libs.AIBase import AIBase\nfrom libs.AI2D import Ai2d\nimport os\nimport ujson\nfrom media.media import *\nfrom time import *\nimport nncase_runtime as nn\nimport ulab.numpy as np\nimport time\nimport utime\nimport image\nimport random\nimport gc\nimport sys\nimport aidemo\n\n# \u81ea\u5b9a\u4e49YOLOv8\u5206\u5272\u7c7b\nclass SegmentationApp(AIBase):\n    def __init__(self,kmodel_path,labels,model_input_size,confidence_threshold=0.2,nms_threshold=0.5,mask_threshold=0.5,rgb888p_size=[224,224],display_size=[1920,1080],debug_mode=0):\n        super().__init__(kmodel_path,model_input_size,rgb888p_size,debug_mode)\n        # \u6a21\u578b\u8def\u5f84\n        self.kmodel_path=kmodel_path\n        # \u5206\u5272\u7c7b\u522b\u6807\u7b7e\n        self.labels=labels\n        # \u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.model_input_size=model_input_size\n        # \u7f6e\u4fe1\u5ea6\u9608\u503c\n        self.confidence_threshold=confidence_threshold\n        # nms\u9608\u503c\n        self.nms_threshold=nms_threshold\n        # mask\u9608\u503c\n        self.mask_threshold=mask_threshold\n        # sensor\u7ed9\u5230AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\n        self.rgb888p_size=[ALIGN_UP(rgb888p_size[0],16),rgb888p_size[1]]\n        # \u663e\u793a\u5206\u8fa8\u7387\n        self.display_size=[ALIGN_UP(display_size[0],16),display_size[1]]\n        self.debug_mode=debug_mode\n        # \u68c0\u6d4b\u6846\u9884\u7f6e\u989c\u8272\u503c\n        self.color_four=[(255, 220, 20, 60), (255, 119, 11, 32), (255, 0, 0, 142), (255, 0, 0, 230),\n                         (255, 106, 0, 228), (255, 0, 60, 100), (255, 0, 80, 100), (255, 0, 0, 70),\n                         (255, 0, 0, 192), (255, 250, 170, 30), (255, 100, 170, 30), (255, 220, 220, 0),\n                         (255, 175, 116, 175), (255, 250, 0, 30), (255, 165, 42, 42), (255, 255, 77, 255),\n                         (255, 0, 226, 252), (255, 182, 182, 255), (255, 0, 82, 0), (255, 120, 166, 157)]\n        # \u5206\u5272\u7ed3\u679c\u7684numpy.array\uff0c\u7528\u4e8e\u7ed9\u5230aidemo\u540e\u5904\u7406\u63a5\u53e3\n        self.masks=np.zeros((1,self.display_size[1],self.display_size[0],4))\n        # Ai2d\u5b9e\u4f8b\uff0c\u7528\u4e8e\u5b9e\u73b0\u6a21\u578b\u9884\u5904\u7406\n        self.ai2d=Ai2d(debug_mode)\n        # \u8bbe\u7f6eAi2d\u7684\u8f93\u5165\u8f93\u51fa\u683c\u5f0f\u548c\u7c7b\u578b\n        self.ai2d.set_ai2d_dtype(nn.ai2d_format.NCHW_FMT,nn.ai2d_format.NCHW_FMT,np.uint8, np.uint8)\n\n    # \u914d\u7f6e\u9884\u5904\u7406\u64cd\u4f5c\uff0c\u8fd9\u91cc\u4f7f\u7528\u4e86pad\u548cresize\uff0cAi2d\u652f\u6301crop/shift/pad/resize/affine\uff0c\u5177\u4f53\u4ee3\u7801\u8bf7\u6253\u5f00/sdcard/app/libs/AI2D.py\u67e5\u770b\n    def config_preprocess(self,input_image_size=None):\n        with ScopedTiming("set preprocess config",self.debug_mode > 0):\n            # \u521d\u59cb\u5316ai2d\u9884\u5904\u7406\u914d\u7f6e\uff0c\u9ed8\u8ba4\u4e3asensor\u7ed9\u5230AI\u7684\u5c3a\u5bf8\uff0c\u60a8\u53ef\u4ee5\u901a\u8fc7\u8bbe\u7f6einput_image_size\u81ea\u884c\u4fee\u6539\u8f93\u5165\u5c3a\u5bf8\n            ai2d_input_size=input_image_size if input_image_size else self.rgb888p_size\n            top,bottom,left,right=self.get_padding_param()\n            self.ai2d.pad([0,0,0,0,top,bottom,left,right], 0, [114,114,114])\n            self.ai2d.resize(nn.interp_method.tf_bilinear, nn.interp_mode.half_pixel)\n            self.ai2d.build([1,3,ai2d_input_size[1],ai2d_input_size[0]],[1,3,self.model_input_size[1],self.model_input_size[0]])\n\n    # \u81ea\u5b9a\u4e49\u5f53\u524d\u4efb\u52a1\u7684\u540e\u5904\u7406\n    def postprocess(self,results):\n        with ScopedTiming("postprocess",self.debug_mode > 0):\n            # \u8fd9\u91cc\u4f7f\u7528\u4e86aidemo\u7684segment_postprocess\u63a5\u53e3\n            seg_res = aidemo.segment_postprocess(results,[self.rgb888p_size[1],self.rgb888p_size[0]],self.model_input_size,[self.display_size[1],self.display_size[0]],self.confidence_threshold,self.nms_threshold,self.mask_threshold,self.masks)\n            return seg_res\n\n    # \u7ed8\u5236\u7ed3\u679c\n    def draw_result(self,pl,seg_res):\n        with ScopedTiming("display_draw",self.debug_mode >0):\n            if seg_res[0]:\n                pl.osd_img.clear()\n                mask_img=image.Image(self.display_size[0], self.display_size[1], image.ARGB8888,alloc=image.ALLOC_REF,data=self.masks)\n                pl.osd_img.copy_from(mask_img)\n                dets,ids,scores = seg_res[0],seg_res[1],seg_res[2]\n                for i, det in enumerate(dets):\n                    x1, y1, w, h = map(lambda x: int(round(x, 0)), det)\n                    pl.osd_img.draw_string_advanced(x1,y1-50,32, " " + self.labels[int(ids[i])] + " " + str(round(scores[i],2)) , color=self.get_color(int(ids[i])))\n            else:\n                pl.osd_img.clear()\n\n    # \u8ba1\u7b97padding\u53c2\u6570\n    def get_padding_param(self):\n        dst_w = self.model_input_size[0]\n        dst_h = self.model_input_size[1]\n        ratio_w = float(dst_w) / self.rgb888p_size[0]\n        ratio_h = float(dst_h) / self.rgb888p_size[1]\n        if ratio_w < ratio_h:\n            ratio = ratio_w\n        else:\n            ratio = ratio_h\n        new_w = (int)(ratio * self.rgb888p_size[0])\n        new_h = (int)(ratio * self.rgb888p_size[1])\n        dw = (dst_w - new_w) / 2\n        dh = (dst_h - new_h) / 2\n        top = (int)(round(dh - 0.1))\n        bottom = (int)(round(dh + 0.1))\n        left = (int)(round(dw - 0.1))\n        right = (int)(round(dw + 0.1))\n        return  top, bottom, left, right\n\n    # \u6839\u636e\u5f53\u524d\u7c7b\u522b\u7d22\u5f15\u83b7\u53d6\u6846\u7684\u989c\u8272\n    def get_color(self, x):\n        idx=x%len(self.color_four)\n        return self.color_four[idx]\n\n\nif __name__=="__main__":\n    # \u663e\u793a\u6a21\u5f0f\uff0c\u9ed8\u8ba4"hdmi",\u53ef\u4ee5\u9009\u62e9"hdmi"\u548c"lcd"\n    display_mode="hdmi"\n    if display_mode=="hdmi":\n        display_size=[1920,1080]\n    else:\n        display_size=[800,480]\n    # \u6a21\u578b\u8def\u5f84\n    kmodel_path="/sdcard/app/tests/kmodel/yolov8n_seg_320.kmodel"\n    labels = ["person", "bicycle", "car", "motorcycle", "airplane", "bus", "train", "truck", "boat", "traffic light", "fire hydrant", "stop sign", "parking meter", "bench", "bird", "cat", "dog", "horse", "sheep", "cow", "elephant", "bear", "zebra", "giraffe", "backpack", "umbrella", "handbag", "tie", "suitcase", "frisbee", "skis", "snowboard", "sports ball", "kite", "baseball bat", "baseball glove", "skateboard", "surfboard", "tennis racket", "bottle", "wine glass", "cup", "fork", "knife", "spoon", "bowl", "banana", "apple", "sandwich", "orange", "broccoli", "carrot", "hot dog", "pizza", "donut", "cake", "chair", "couch", "potted plant", "bed", "dining table", "toilet", "tv", "laptop", "mouse", "remote", "keyboard", "cell phone", "microwave", "oven", "toaster", "sink", "refrigerator", "book", "clock", "vase", "scissors", "teddy bear", "hair drier", "toothbrush"]\n    #\u5176\u5b83\u53c2\u6570\u8bbe\u7f6e\n    confidence_threshold = 0.2\n    nms_threshold = 0.5\n    mask_threshold=0.5\n    rgb888p_size=[320,320]\n\n    # \u521d\u59cb\u5316PipeLine\n    pl=PipeLine(rgb888p_size=rgb888p_size,display_size=display_size,display_mode=display_mode)\n    pl.create()\n    # \u521d\u59cb\u5316\u81ea\u5b9a\u4e49YOLOV8\u5206\u5272\u793a\u4f8b\n    seg=SegmentationApp(kmodel_path,labels=labels,model_input_size=[320,320],confidence_threshold=confidence_threshold,nms_threshold=nms_threshold,mask_threshold=mask_threshold,rgb888p_size=rgb888p_size,display_size=display_size,debug_mode=0)\n    seg.config_preprocess()\n    try:\n        while True:\n            os.exitpoint()\n            with ScopedTiming("total",1):\n                # \u83b7\u53d6\u5f53\u524d\u5e27\u6570\u636e\n                img=pl.get_frame()\n                # \u63a8\u7406\u5f53\u524d\u5e27\n                seg_res=seg.run(img)\n                # \u7ed8\u5236\u7ed3\u679c\u5230PipeLine\u7684osd\u56fe\u50cf\n                seg.draw_result(pl,seg_res)\n                # \u663e\u793a\u5f53\u524d\u7684\u7ed8\u5236\u7ed3\u679c\n                pl.show_image()\n                gc.collect()\n    except Exception as e:\n        sys.print_exception(e)\n    finally:\n        seg.deinit()\n        pl.destroy()\n'})}),"\n",(0,i.jsx)(n.h3,{id:"227-\u81ea\u5b66\u4e60",children:"2.27. \u81ea\u5b66\u4e60"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:'from libs.PipeLine import PipeLine, ScopedTiming\nfrom libs.AIBase import AIBase\nfrom libs.AI2D import Ai2d\nimport os\nimport ujson\nfrom media.media import *\nfrom time import *\nimport nncase_runtime as nn\nimport ulab.numpy as np\nimport time\nimport utime\nimport image\nimport random\nimport gc\nimport sys\nimport aicube\n\n# \u81ea\u5b9a\u4e49\u81ea\u5b66\u4e60\u7c7b\nclass SelfLearningApp(AIBase):\n    def __init__(self,kmodel_path,model_input_size,labels,top_k,threshold,database_path,rgb888p_size=[224,224],display_size=[1920,1080],debug_mode=0):\n        super().__init__(kmodel_path,model_input_size,rgb888p_size,debug_mode)\n        self.kmodel_path=kmodel_path\n        # \u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.model_input_size=model_input_size\n        self.labels=labels\n        self.database_path=database_path\n        # sensor\u7ed9\u5230AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\n        self.rgb888p_size=[ALIGN_UP(rgb888p_size[0],16),rgb888p_size[1]]\n        # \u663e\u793a\u5206\u8fa8\u7387\n        self.display_size=[ALIGN_UP(display_size[0],16),display_size[1]]\n        self.debug_mode=debug_mode\n        # \u8bc6\u522b\u9608\u503c\n        self.threshold = threshold\n        # \u9009\u62e9top_k\u4e2a\u76f8\u4f3c\u5ea6\u5927\u4e8e\u9608\u503c\u7684\u7ed3\u679c\u7c7b\u522b\n        self.top_k = top_k\n        #\u5bf9\u5e94\u7c7b\u522b\u6ce8\u518c\u7279\u5f81\u6570\u91cf\n        self.features=[2,2]\n        #\u6ce8\u518c\u5355\u4e2a\u7279\u5f81\u4e2d\u9014\u95f4\u9694\u5e27\u6570\n        self.time_one=60\n        self.time_all = 0\n        self.time_now = 0\n        # \u7c7b\u522b\u7d22\u5f15\n        self.category_index = 0\n        # \u7279\u5f81\u5316\u90e8\u5206\u526a\u5207\u5bbd\u9ad8\n        self.crop_w = 400\n        self.crop_h = 400\n        # crop\u7684\u4f4d\u7f6e\n        self.crop_x = self.rgb888p_size[0] / 2.0 - self.crop_w / 2.0\n        self.crop_y = self.rgb888p_size[1] / 2.0 - self.crop_h / 2.0\n        self.crop_x_osd=0\n        self.crop_y_osd=0\n        self.crop_w_osd=0\n        self.crop_h_osd=0\n        # Ai2d\u5b9e\u4f8b\uff0c\u7528\u4e8e\u5b9e\u73b0\u6a21\u578b\u9884\u5904\u7406\n        self.ai2d=Ai2d(debug_mode)\n        # \u8bbe\u7f6eAi2d\u7684\u8f93\u5165\u8f93\u51fa\u683c\u5f0f\u548c\u7c7b\u578b\n        self.ai2d.set_ai2d_dtype(nn.ai2d_format.NCHW_FMT,nn.ai2d_format.NCHW_FMT,np.uint8, np.uint8)\n        self.data_init()\n\n    # \u914d\u7f6e\u9884\u5904\u7406\u64cd\u4f5c\uff0c\u8fd9\u91cc\u4f7f\u7528\u4e86crop\u548cresize\uff0cAi2d\u652f\u6301crop/shift/pad/resize/affine\uff0c\u5177\u4f53\u4ee3\u7801\u8bf7\u6253\u5f00/sdcard/app/libs/AI2D.py\u67e5\u770b\n    def config_preprocess(self,input_image_size=None):\n        with ScopedTiming("set preprocess config",self.debug_mode > 0):\n            # \u521d\u59cb\u5316ai2d\u9884\u5904\u7406\u914d\u7f6e\uff0c\u9ed8\u8ba4\u4e3asensor\u7ed9\u5230AI\u7684\u5c3a\u5bf8\uff0c\u60a8\u53ef\u4ee5\u901a\u8fc7\u8bbe\u7f6einput_image_size\u81ea\u884c\u4fee\u6539\u8f93\u5165\u5c3a\u5bf8\n            ai2d_input_size=input_image_size if input_image_size else self.rgb888p_size\n            self.ai2d.crop(int(self.crop_x),int(self.crop_y),int(self.crop_w),int(self.crop_h))\n            self.ai2d.resize(nn.interp_method.tf_bilinear, nn.interp_mode.half_pixel)\n            self.ai2d.build([1,3,ai2d_input_size[1],ai2d_input_size[0]],[1,3,self.model_input_size[1],self.model_input_size[0]])\n\n    # \u81ea\u5b9a\u4e49\u5f53\u524d\u4efb\u52a1\u7684\u540e\u5904\u7406\n    def postprocess(self,results):\n        with ScopedTiming("postprocess",self.debug_mode > 0):\n            return results[0][0]\n\n    # \u7ed8\u5236\u7ed3\u679c\uff0c\u7ed8\u5236\u7279\u5f81\u91c7\u96c6\u6846\u548c\u7279\u5f81\u5206\u7c7b\u6846\n    def draw_result(self,pl,feature):\n        pl.osd_img.clear()\n        with ScopedTiming("display_draw",self.debug_mode >0):\n            pl.osd_img.draw_rectangle(self.crop_x_osd,self.crop_y_osd, self.crop_w_osd, self.crop_h_osd, color=(255, 255, 0, 255), thickness = 4)\n            if (self.category_index < len(self.labels)):\n                self.time_now += 1\n                pl.osd_img.draw_string_advanced(50, self.crop_y_osd-50, 30,"\u8bf7\u5c06\u5f85\u6dfb\u52a0\u7c7b\u522b\u653e\u5165\u6846\u5185\u8fdb\u884c\u7279\u5f81\u91c7\u96c6\uff1a"+self.labels[self.category_index] + "_" + str(int(self.time_now-1) // self.time_one) + ".bin", color=(255,255,0,0))\n                with open(self.database_path + self.labels[self.category_index] + "_" + str(int(self.time_now-1) // self.time_one) + ".bin", \'wb\') as f:\n                    f.write(feature.tobytes())\n                if (self.time_now // self.time_one == self.features[self.category_index]):\n                    self.category_index += 1\n                    self.time_all -= self.time_now\n                    self.time_now = 0\n            else:\n                results_learn = []\n                list_features = os.listdir(self.database_path)\n                for feature_name in list_features:\n                    with open(self.database_path + feature_name, \'rb\') as f:\n                        data = f.read()\n                    save_vec = np.frombuffer(data, dtype=np.float)\n                    score = self.getSimilarity(feature, save_vec)\n                    if (score > self.threshold):\n                        res = feature_name.split("_")\n                        is_same = False\n                        for r in results_learn:\n                            if (r["category"] ==  res[0]):\n                                if (r["score"] < score):\n                                    r["bin_file"] = feature_name\n                                    r["score"] = score\n                                is_same = True\n                        if (not is_same):\n                            if(len(results_learn) < self.top_k):\n                                evec = {}\n                                evec["category"] = res[0]\n                                evec["score"] = score\n                                evec["bin_file"] = feature_name\n                                results_learn.append( evec )\n                                results_learn = sorted(results_learn, key=lambda x: -x["score"])\n                            else:\n                                if( score <= results_learn[self.top_k-1]["score"] ):\n                                    continue\n                                else:\n                                    evec = {}\n                                    evec["category"] = res[0]\n                                    evec["score"] = score\n                                    evec["bin_file"] = feature_name\n                                    results_learn.append( evec )\n                                    results_learn = sorted(results_learn, key=lambda x: -x["score"])\n                                    results_learn.pop()\n                draw_y = 200\n                for r in results_learn:\n                    pl.osd_img.draw_string_advanced( 50 , draw_y,50,r["category"] + " : " + str(r["score"]), color=(255,255,0,0))\n                    draw_y += 50\n\n    #\u6570\u636e\u521d\u59cb\u5316\n    def data_init(self):\n        os.mkdir(self.database_path)\n        self.crop_x_osd = int(self.crop_x / self.rgb888p_size[0] * self.display_size[0])\n        self.crop_y_osd = int(self.crop_y / self.rgb888p_size[1] * self.display_size[1])\n        self.crop_w_osd = int(self.crop_w / self.rgb888p_size[0] * self.display_size[0])\n        self.crop_h_osd = int(self.crop_h / self.rgb888p_size[1] * self.display_size[1])\n        for i in range(len(self.labels)):\n            for j in range(self.features[i]):\n                self.time_all += self.time_one\n\n    # \u83b7\u53d6\u4e24\u4e2a\u7279\u5f81\u5411\u91cf\u7684\u76f8\u4f3c\u5ea6\n    def getSimilarity(self,output_vec,save_vec):\n        tmp = sum(output_vec * save_vec)\n        mold_out = np.sqrt(sum(output_vec * output_vec))\n        mold_save = np.sqrt(sum(save_vec * save_vec))\n        return tmp / (mold_out * mold_save)\n\n\nif __name__=="__main__":\n    # \u663e\u793a\u6a21\u5f0f\uff0c\u9ed8\u8ba4"hdmi",\u53ef\u4ee5\u9009\u62e9"hdmi"\u548c"lcd"\n    display_mode="hdmi"\n    if display_mode=="hdmi":\n        display_size=[1920,1080]\n    else:\n        display_size=[800,480]\n    # \u6a21\u578b\u8def\u5f84\n    kmodel_path="/sdcard/app/tests/kmodel/recognition.kmodel"\n    database_path="/sdcard/app/tests/utils/features/"\n    # \u5176\u5b83\u53c2\u6570\u8bbe\u7f6e\n    rgb888p_size=[1920,1080]\n    model_input_size=[224,224]\n    labels=["\u82f9\u679c","\u9999\u8549"]\n    top_k=3\n    threshold=0.5\n\n    # \u521d\u59cb\u5316PipeLine\n    pl=PipeLine(rgb888p_size=rgb888p_size,display_size=display_size,display_mode=display_mode)\n    pl.create()\n    # \u521d\u59cb\u5316\u81ea\u5b66\u4e60\u5b9e\u4f8b\n    sl=SelfLearningApp(kmodel_path,model_input_size=model_input_size,labels=labels,top_k=top_k,threshold=threshold,database_path=database_path,rgb888p_size=rgb888p_size,display_size=display_size,debug_mode=0)\n    sl.config_preprocess()\n    try:\n        while True:\n            os.exitpoint()\n            with ScopedTiming("total",1):\n                # \u83b7\u53d6\u5f53\u524d\u5e27\u6570\u636e\n                img=pl.get_frame()\n                # \u63a8\u7406\u5f53\u524d\u5e27\n                res=sl.run(img)\n                # \u7ed8\u5236\u7ed3\u679c\u5230PipeLine\u7684osd\u56fe\u50cf\n                sl.draw_result(pl,res)\n                # \u663e\u793a\u5f53\u524d\u7684\u7ed8\u5236\u7ed3\u679c\n                pl.show_image()\n                gc.collect()\n    except Exception as e:\n        sys.print_exception(e)\n    finally:\n        # \u5220\u9664features\u6587\u4ef6\u5939\n        stat_info = os.stat(database_path)\n        if (stat_info[0] & 0x4000):\n            list_files = os.listdir(database_path)\n            for l in list_files:\n                os.remove(database_path + l)\n        os.rmdir(database_path)\n        sl.deinit()\n        pl.destroy()\n'})}),"\n",(0,i.jsx)(n.h3,{id:"228-\u5c40\u90e8\u653e\u5927\u5668",children:"2.28. \u5c40\u90e8\u653e\u5927\u5668"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:'from libs.PipeLine import PipeLine, ScopedTiming\nfrom libs.AIBase import AIBase\nfrom libs.AI2D import Ai2d\nimport os\nimport ujson\nfrom media.media import *\nfrom time import *\nimport nncase_runtime as nn\nimport ulab.numpy as np\nimport time\nimport image\nimport aicube\nimport random\nimport gc\nimport sys\n\n# \u81ea\u5b9a\u4e49\u624b\u638c\u68c0\u6d4b\u4efb\u52a1\u7c7b\nclass HandDetApp(AIBase):\n    def __init__(self,kmodel_path,labels,model_input_size,anchors,confidence_threshold=0.2,nms_threshold=0.5,nms_option=False, strides=[8,16,32],rgb888p_size=[1920,1080],display_size=[1920,1080],debug_mode=0):\n        super().__init__(kmodel_path,model_input_size,rgb888p_size,debug_mode)\n        # kmodel\u8def\u5f84\n        self.kmodel_path=kmodel_path\n        self.labels=labels\n        # \u68c0\u6d4b\u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.model_input_size=model_input_size\n        # \u7f6e\u4fe1\u5ea6\u9608\u503c\n        self.confidence_threshold=confidence_threshold\n        # nms\u9608\u503c\n        self.nms_threshold=nms_threshold\n        self.anchors=anchors\n        self.strides = strides  # \u7279\u5f81\u4e0b\u91c7\u6837\u500d\u6570\n        self.nms_option = nms_option  # NMS\u9009\u9879\uff0c\u5982\u679c\u4e3aTrue\u505a\u7c7b\u95f4NMS,\u5982\u679c\u4e3aFalse\u505a\u7c7b\u5185NMS\n        # sensor\u7ed9\u5230AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.rgb888p_size=[ALIGN_UP(rgb888p_size[0],16),rgb888p_size[1]]\n        # \u89c6\u9891\u8f93\u51faVO\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.display_size=[ALIGN_UP(display_size[0],16),display_size[1]]\n        # debug\u6a21\u5f0f\n        self.debug_mode=debug_mode\n        self.ai2d=Ai2d(debug_mode)\n        self.ai2d.set_ai2d_dtype(nn.ai2d_format.NCHW_FMT,nn.ai2d_format.NCHW_FMT,np.uint8, np.uint8)\n\n    def config_preprocess(self,input_image_size=None):\n        with ScopedTiming("set preprocess config",self.debug_mode > 0):\n            # \u521d\u59cb\u5316ai2d\u9884\u5904\u7406\u914d\u7f6e\uff0c\u9ed8\u8ba4\u4e3asensor\u7ed9\u5230AI\u7684\u5c3a\u5bf8\uff0c\u53ef\u4ee5\u901a\u8fc7\u8bbe\u7f6einput_image_size\u81ea\u884c\u4fee\u6539\u8f93\u5165\u5c3a\u5bf8\n            ai2d_input_size = input_image_size if input_image_size else self.rgb888p_size\n            # \u8ba1\u7b97padding\u53c2\u6570\u5e76\u5e94\u7528pad\u64cd\u4f5c\uff0c\u4ee5\u786e\u4fdd\u8f93\u5165\u56fe\u50cf\u5c3a\u5bf8\u4e0e\u6a21\u578b\u8f93\u5165\u5c3a\u5bf8\u5339\u914d\n            top, bottom, left, right = self.get_padding_param()\n            self.ai2d.pad([0, 0, 0, 0, top, bottom, left, right], 0, [114, 114, 114])\n            # \u4f7f\u7528\u53cc\u7ebf\u6027\u63d2\u503c\u8fdb\u884cresize\u64cd\u4f5c\uff0c\u8c03\u6574\u56fe\u50cf\u5c3a\u5bf8\u4ee5\u7b26\u5408\u6a21\u578b\u8f93\u5165\u8981\u6c42\n            self.ai2d.resize(nn.interp_method.tf_bilinear, nn.interp_mode.half_pixel)\n            # \u6784\u5efa\u9884\u5904\u7406\u6d41\u7a0b\n            self.ai2d.build([1,3,ai2d_input_size[1],ai2d_input_size[0]],[1,3,self.model_input_size[1],self.model_input_size[0]])\n\n    # \u81ea\u5b9a\u4e49\u540e\u5904\u7406\uff0cresults\u662f\u6a21\u578b\u8f93\u51fa\u7684array\u5217\u8868\uff0c\u8fd9\u91cc\u4f7f\u7528\u4e86aicube\u5e93\u7684anchorbasedet_post_process\u63a5\u53e3\n    def postprocess(self,results):\n        with ScopedTiming("postprocess",self.debug_mode > 0):\n            dets = aicube.anchorbasedet_post_process(results[0], results[1], results[2], self.model_input_size, self.rgb888p_size, self.strides, len(self.labels), self.confidence_threshold, self.nms_threshold, self.anchors, self.nms_option)\n            # \u8fd4\u56de\u624b\u638c\u68c0\u6d4b\u7ed3\u679c\n            return dets\n\n    # \u8ba1\u7b97padding\u53c2\u6570\uff0c\u786e\u4fdd\u8f93\u5165\u56fe\u50cf\u5c3a\u5bf8\u4e0e\u6a21\u578b\u8f93\u5165\u5c3a\u5bf8\u5339\u914d\n    def get_padding_param(self):\n        # \u6839\u636e\u76ee\u6807\u5bbd\u5ea6\u548c\u9ad8\u5ea6\u8ba1\u7b97\u6bd4\u4f8b\u56e0\u5b50\n        dst_w = self.model_input_size[0]\n        dst_h = self.model_input_size[1]\n        input_width = self.rgb888p_size[0]\n        input_high = self.rgb888p_size[1]\n        ratio_w = dst_w / input_width\n        ratio_h = dst_h / input_high\n        # \u9009\u62e9\u8f83\u5c0f\u7684\u6bd4\u4f8b\u56e0\u5b50\uff0c\u4ee5\u786e\u4fdd\u56fe\u50cf\u5185\u5bb9\u5b8c\u6574\n        if ratio_w < ratio_h:\n            ratio = ratio_w\n        else:\n            ratio = ratio_h\n        # \u8ba1\u7b97\u65b0\u7684\u5bbd\u5ea6\u548c\u9ad8\u5ea6\n        new_w = int(ratio * input_width)\n        new_h = int(ratio * input_high)\n        # \u8ba1\u7b97\u5bbd\u5ea6\u548c\u9ad8\u5ea6\u7684\u5dee\u503c\uff0c\u5e76\u786e\u5b9apadding\u7684\u4f4d\u7f6e\n        dw = (dst_w - new_w) / 2\n        dh = (dst_h - new_h) / 2\n        top = int(round(dh - 0.1))\n        bottom = int(round(dh + 0.1))\n        left = int(round(dw - 0.1))\n        right = int(round(dw + 0.1))\n        return top, bottom, left, right\n\n# \u81ea\u5b9a\u4e49\u624b\u52bf\u5173\u952e\u70b9\u5206\u7c7b\u4efb\u52a1\u7c7b\nclass HandKPClassApp(AIBase):\n    def __init__(self,kmodel_path,model_input_size,rgb888p_size=[1920,1080],display_size=[1920,1080],debug_mode=0):\n        super().__init__(kmodel_path,model_input_size,rgb888p_size,debug_mode)\n        # kmodel\u8def\u5f84\n        self.kmodel_path=kmodel_path\n        # \u68c0\u6d4b\u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.model_input_size=model_input_size\n        # sensor\u7ed9\u5230AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.rgb888p_size=[ALIGN_UP(rgb888p_size[0],16),rgb888p_size[1]]\n        # \u89c6\u9891\u8f93\u51faVO\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.display_size=[ALIGN_UP(display_size[0],16),display_size[1]]\n        self.crop_params=[]\n        # debug\u6a21\u5f0f\n        self.debug_mode=debug_mode\n        self.ai2d=Ai2d(debug_mode)\n        self.ai2d.set_ai2d_dtype(nn.ai2d_format.NCHW_FMT,nn.ai2d_format.NCHW_FMT,np.uint8, np.uint8)\n\n    # \u914d\u7f6e\u9884\u5904\u7406\u64cd\u4f5c\uff0c\u8fd9\u91cc\u4f7f\u7528\u4e86crop\u548cresize\uff0cAi2d\u652f\u6301crop/shift/pad/resize/affine\n    def config_preprocess(self,det,input_image_size=None):\n        with ScopedTiming("set preprocess config",self.debug_mode > 0):\n            # \u521d\u59cb\u5316ai2d\u9884\u5904\u7406\u914d\u7f6e\uff0c\u9ed8\u8ba4\u4e3asensor\u7ed9\u5230AI\u7684\u5c3a\u5bf8\uff0c\u53ef\u4ee5\u901a\u8fc7\u8bbe\u7f6einput_image_size\u81ea\u884c\u4fee\u6539\u8f93\u5165\u5c3a\u5bf8\n            ai2d_input_size=input_image_size if input_image_size else self.rgb888p_size\n            self.crop_params = self.get_crop_param(det)\n            self.ai2d.crop(self.crop_params[0],self.crop_params[1],self.crop_params[2],self.crop_params[3])\n            self.ai2d.resize(nn.interp_method.tf_bilinear, nn.interp_mode.half_pixel)\n            self.ai2d.build([1,3,ai2d_input_size[1],ai2d_input_size[0]],[1,3,self.model_input_size[1],self.model_input_size[0]])\n\n    # \u81ea\u5b9a\u4e49\u540e\u5904\u7406\uff0cresults\u662f\u6a21\u578b\u8f93\u51fa\u7684array\u5217\u8868\uff0c\u8fd4\u56de\u624b\u90e8\u5173\u952e\u70b9\n    def postprocess(self,results):\n        with ScopedTiming("postprocess",self.debug_mode > 0):\n            results=results[0].reshape(results[0].shape[0]*results[0].shape[1])\n            results_show = np.zeros(results.shape,dtype=np.int16)\n            results_show[0::2] = (results[0::2] * self.crop_params[3] + self.crop_params[0])\n            results_show[1::2] = (results[1::2] * self.crop_params[2] + self.crop_params[1])\n            return results_show\n\n    # \u8ba1\u7b97crop\u53c2\u6570\n    def get_crop_param(self,det_box):\n        x1, y1, x2, y2 = det_box[2],det_box[3],det_box[4],det_box[5]\n        w,h= int(x2 - x1),int(y2 - y1)\n        w_det = int(float(x2 - x1) * self.display_size[0] // self.rgb888p_size[0])\n        h_det = int(float(y2 - y1) * self.display_size[1] // self.rgb888p_size[1])\n        x_det = int(x1*self.display_size[0] // self.rgb888p_size[0])\n        y_det = int(y1*self.display_size[1] // self.rgb888p_size[1])\n        length = max(w, h)/2\n        cx = (x1+x2)/2\n        cy = (y1+y2)/2\n        ratio_num = 1.26*length\n        x1_kp = int(max(0,cx-ratio_num))\n        y1_kp = int(max(0,cy-ratio_num))\n        x2_kp = int(min(self.rgb888p_size[0]-1, cx+ratio_num))\n        y2_kp = int(min(self.rgb888p_size[1]-1, cy+ratio_num))\n        w_kp = int(x2_kp - x1_kp + 1)\n        h_kp = int(y2_kp - y1_kp + 1)\n        return [x1_kp, y1_kp, w_kp, h_kp]\n\nclass SpaceResize:\n    def __init__(self,hand_det_kmodel,hand_kp_kmodel,det_input_size,kp_input_size,labels,anchors,confidence_threshold=0.25,nms_threshold=0.3,nms_option=False,strides=[8,16,32],rgb888p_size=[1280,720],display_size=[1920,1080],debug_mode=0):\n        # \u624b\u638c\u68c0\u6d4b\u6a21\u578b\u8def\u5f84\n        self.hand_det_kmodel=hand_det_kmodel\n        # \u624b\u638c\u5173\u952e\u70b9\u6a21\u578b\u8def\u5f84\n        self.hand_kp_kmodel=hand_kp_kmodel\n        # \u624b\u638c\u68c0\u6d4b\u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.det_input_size=det_input_size\n        # \u624b\u638c\u5173\u952e\u70b9\u6a21\u578b\u8f93\u5165\u5206\u8fa8\u7387\n        self.kp_input_size=kp_input_size\n        self.labels=labels\n        # anchors\n        self.anchors=anchors\n        # \u7f6e\u4fe1\u5ea6\u9608\u503c\n        self.confidence_threshold=confidence_threshold\n        # nms\u9608\u503c\n        self.nms_threshold=nms_threshold\n        self.nms_option=nms_option\n        self.strides=strides\n        # sensor\u7ed9\u5230AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.rgb888p_size=[ALIGN_UP(rgb888p_size[0],16),rgb888p_size[1]]\n        # \u89c6\u9891\u8f93\u51faVO\u5206\u8fa8\u7387\uff0c\u5bbd16\u5b57\u8282\u5bf9\u9f50\n        self.display_size=[ALIGN_UP(display_size[0],16),display_size[1]]\n        # debug_mode\u6a21\u5f0f\n        self.debug_mode=debug_mode\n\n        self.first_start = True                                              # \u9996\u6b21\u624b\u638c\u5165\u955c\u53c2\u6570\n        self.two_point_left_x = 0                                            # \u4e2d\u6307\u98df\u6307\u5305\u62ec\u8303\u56f4 x\n        self.two_point_top_y = 0                                             # \u4e2d\u6307\u98df\u6307\u5305\u62ec\u8303\u56f4 y\n        self.two_point_mean_w = 0                                            # \u4e2d\u6307\u98df\u6307\u9996\u6b21\u5165\u955c\u5305\u62ec\u8303\u56f4 w\n        self.two_point_mean_h = 0                                            # \u4e2d\u6307\u98df\u6307\u9996\u6b21\u5165\u955c\u5305\u62ec\u8303\u56f4 h\n        self.two_point_crop_w = 0                                            # \u4e2d\u6307\u98df\u6307\u5305\u62ec\u8303\u56f4 w\n        self.two_point_crop_h = 0                                            # \u4e2d\u6307\u98df\u6307\u5305\u62ec\u8303\u56f4 h\n        self.osd_plot_x = 0                                                  # osd \u753b\u7f29\u653e\u56fe\u8d77\u59cb\u70b9 x\n        self.osd_plot_y = 0                                                  # osd \u753b\u7f29\u653e\u56fe\u8d77\u59cb\u70b9 y\n        self.ori_new_ratio = 0                                               # \u7f29\u653e\u6bd4\u4f8b\n        self.new_resize_w = 0                                                # \u7f29\u653e\u540e w\n        self.new_resize_h = 0                                                # \u7f29\u653e\u540e h\n        self.crop_area = 0                                                   # \u526a\u5207\u533a\u57df\n        self.rect_frame_x = 0                                                # osd\u7ed8\u753b\u8d77\u59cb\u70b9 x\n        self.rect_frame_y = 0                                                # osd\u7ed8\u753b\u8d77\u59cb\u70b9 y\n        self.masks = np.zeros((self.display_size[1],self.display_size[0],4),dtype=np.uint8)\n        self.mask_img=image.Image(self.display_size[0], self.display_size[1], image.ARGB8888,alloc=image.ALLOC_REF,data=self.masks)\n        self.hand_det=HandDetApp(self.hand_det_kmodel,self.labels,model_input_size=self.det_input_size,anchors=self.anchors,confidence_threshold=self.confidence_threshold,nms_threshold=self.nms_threshold,nms_option=self.nms_option,strides=self.strides,rgb888p_size=self.rgb888p_size,display_size=self.display_size,debug_mode=0)\n        self.hand_kp=HandKPClassApp(self.hand_kp_kmodel,model_input_size=self.kp_input_size,rgb888p_size=self.rgb888p_size,display_size=self.display_size)\n        self.ai2d=Ai2d(debug_mode)\n        self.ai2d.set_ai2d_dtype(nn.ai2d_format.NCHW_FMT,nn.ai2d_format.RGB_packed,np.uint8, np.uint8)\n        self.hand_det.config_preprocess()\n\n    # \u5bf9\u8f93\u5165\u6570\u636e\u505a\u9884\u5904\u7406\uff0c\u5bf9\u62c7\u6307\u548c\u4e2d\u6307\u90e8\u5206\u505a\u88c1\u526a\u5e76\u505aresize\n    def imgprocess(self,input_np,x,y,w,h,out_w,out_h):\n        self.ai2d.crop(x, y, w, h)\n        self.ai2d.resize(nn.interp_method.tf_bilinear, nn.interp_mode.half_pixel )\n        self.ai2d.build([1,3,self.rgb888p_size[1],self.rgb888p_size[0]],[1,out_h, out_w,3])\n        return self.ai2d.run(input_np).to_numpy()\n\n    # run\u51fd\u6570\n    def run(self,input_np):\n        # \u5148\u8fdb\u884c\u624b\u638c\u68c0\u6d4b\n        det_boxes=self.hand_det.run(input_np)\n        det_res=[]\n        two_point = np.zeros((4),dtype=np.int16)\n        for det_box in det_boxes:\n            # \u7b5b\u9009\u7b26\u5408\u8981\u6c42\u7684\u624b\u638c\n            x1, y1, x2, y2 = det_box[2],det_box[3],det_box[4],det_box[5]\n            w,h= int(x2 - x1),int(y2 - y1)\n            if (h<(0.1*self.rgb888p_size[1])):\n                continue\n            if (w<(0.25*self.rgb888p_size[0]) and ((x1<(0.03*self.rgb888p_size[0])) or (x2>(0.97*self.rgb888p_size[0])))):\n                continue\n            if (w<(0.15*self.rgb888p_size[0]) and ((x1<(0.01*self.rgb888p_size[0])) or (x2>(0.99*self.rgb888p_size[0])))):\n                continue\n            det_res.append(det_box)\n        if len(det_res)!=0:\n            # \u9009\u62e9\u7b2c\u4e00\u4e2a\u624b\u638c\u505a\u624b\u638c\u5173\u952e\u70b9\u8bc6\u522b\uff0c\u7136\u540e\u88c1\u526a\u62c7\u6307\u548c\u4e2d\u6307\u533a\u57df\u505aresize\u5e76\u66ff\u6362\u539f\u56fe\u4e2d\u7684\u90e8\u5206\n            det_box=det_res[0]\n            self.hand_kp.config_preprocess(det_box)\n            results_show=self.hand_kp.run(input_np)\n            two_point[0],two_point[1],two_point[2],two_point[3] = results_show[8],results_show[9],results_show[16+8],results_show[16+9]\n            if (self.first_start):\n                if (two_point[0] > 0 and two_point[0] < self.rgb888p_size[0] and two_point[2] > 0 and two_point[2] < self.rgb888p_size[0] and two_point[1] > 0 and two_point[1] < self.rgb888p_size[1] and two_point[3] > 0 and two_point[3] < self.rgb888p_size[1]):\n                    self.two_point_mean_w = np.sqrt(pow(two_point[0] - two_point[2],2) + pow(two_point[1] - two_point[3],2))*0.8\n                    self.two_point_mean_h = np.sqrt(pow(two_point[0] - two_point[2],2) + pow(two_point[1] - two_point[3],2))*0.8\n                    self.first_start = False\n            else:\n                self.mask_img.clear()\n                self.two_point_left_x = int(max((two_point[0] + two_point[2]) / 2 - self.two_point_mean_w / 2, 0))\n                self.two_point_top_y = int(max((two_point[1] + two_point[3]) / 2 - self.two_point_mean_h / 2, 0))\n                self.two_point_crop_w = int(min(min((two_point[0] + two_point[2]) / 2 - self.two_point_mean_w / 2 + self.two_point_mean_w , self.two_point_mean_w), self.rgb888p_size[0] - ((two_point[0] + two_point[2]) / 2 - self.two_point_mean_w / 2)))\n                self.two_point_crop_h = int(min(min((two_point[1] + two_point[3]) / 2 - self.two_point_mean_h / 2 + self.two_point_mean_h , self.two_point_mean_h), self.rgb888p_size[1] - ((two_point[1] + two_point[3]) / 2 - self.two_point_mean_h / 2)))\n                self.ori_new_ratio = np.sqrt(pow((two_point[0] - two_point[2]),2) + pow((two_point[1] - two_point[3]),2))*0.8 / self.two_point_mean_w\n                self.new_resize_w = min(int(self.two_point_crop_w * self.ori_new_ratio / self.rgb888p_size[0] * self.display_size[0]),600)\n                self.new_resize_h = min(int(self.two_point_crop_h * self.ori_new_ratio / self.rgb888p_size[1] * self.display_size[1]),600)\n                self.rect_frame_x = int(self.two_point_left_x * 1.0 / self.rgb888p_size[0] * self.display_size[0])\n                self.rect_frame_y = int(self.two_point_top_y * 1.0 / self.rgb888p_size[1] * self.display_size[1])\n                self.draw_w = min(self.new_resize_w,self.display_size[0]-self.rect_frame_x-1)\n                self.draw_h = min(self.new_resize_h,self.display_size[1]-self.rect_frame_y-1)\n                space_np_out = self.imgprocess(input_np, self.two_point_left_x, self.two_point_top_y, self.two_point_crop_w, self.two_point_crop_h, self.new_resize_w, self.new_resize_h)      # \u8fd0\u884c \u9694\u7a7a\u7f29\u653e\u68c0\u6d4b ai2d\n                self.masks[self.rect_frame_y:self.rect_frame_y + self.draw_h,self.rect_frame_x:self.rect_frame_x + self.draw_w,0] = 255\n                self.masks[self.rect_frame_y:self.rect_frame_y + self.draw_h,self.rect_frame_x:self.rect_frame_x + self.draw_w,1:4] = space_np_out[0][0:self.draw_h,0:self.draw_w,:]\n        return det_res\n\n    # \u7ed8\u5236\u6548\u679c\n    def draw_result(self,pl,det_res):\n        pl.osd_img.clear()\n        if len(det_res)==1:\n            pl.osd_img.copy_from(self.mask_img)\n        else:\n            pl.osd_img.draw_string_advanced((self.display_size[0]//2),(self.display_size[1]//2),32,"\u8bf7\u4fdd\u8bc1\u4e00\u53ea\u624b\u5165\u955c!",color=(255,0,0))\n\n\n\nif __name__=="__main__":\n    # \u663e\u793a\u6a21\u5f0f\uff0c\u9ed8\u8ba4"hdmi",\u53ef\u4ee5\u9009\u62e9"hdmi"\u548c"lcd"\n    display_mode="hdmi"\n    if display_mode=="hdmi":\n        display_size=[1920,1080]\n    else:\n        display_size=[800,480]\n    # \u624b\u638c\u68c0\u6d4b\u6a21\u578b\u8def\u5f84\n    hand_det_kmodel_path="/sdcard/app/tests/kmodel/hand_det.kmodel"\n    # \u624b\u638c\u5173\u952e\u70b9\u6a21\u578b\u8def\u5f84\n    hand_kp_kmodel_path="/sdcard/app/tests/kmodel/handkp_det.kmodel"\n    anchors_path="/sdcard/app/tests/utils/prior_data_320.bin"\n    rgb888p_size=[1920,1080]\n    hand_det_input_size=[512,512]\n    hand_kp_input_size=[256,256]\n    confidence_threshold=0.2\n    nms_threshold=0.5\n    labels=["hand"]\n    anchors = [26,27, 53,52, 75,71, 80,99, 106,82, 99,134, 140,113, 161,172, 245,276]\n\n    # \u521d\u59cb\u5316PipeLine\uff0c\u53ea\u5173\u6ce8\u4f20\u7ed9AI\u7684\u56fe\u50cf\u5206\u8fa8\u7387\uff0c\u663e\u793a\u7684\u5206\u8fa8\u7387\n    pl=PipeLine(rgb888p_size=rgb888p_size,display_size=display_size,display_mode=display_mode)\n    pl.create()\n    sr=SpaceResize(hand_det_kmodel_path,hand_kp_kmodel_path,det_input_size=hand_det_input_size,kp_input_size=hand_kp_input_size,labels=labels,anchors=anchors,confidence_threshold=confidence_threshold,nms_threshold=nms_threshold,nms_option=False,strides=[8,16,32],rgb888p_size=rgb888p_size,display_size=display_size)\n    try:\n        while True:\n            os.exitpoint()\n            with ScopedTiming("total",1):\n                img=pl.get_frame()          # \u83b7\u53d6\u5f53\u524d\u5e27\n                det_res=sr.run(img)         # \u63a8\u7406\u5f53\u524d\u5e27\n                sr.draw_result(pl,det_res)  # \u7ed8\u5236\u5f53\u524d\u5e27\u63a8\u7406\u7ed3\u679c\n                pl.show_image()             # \u5c55\u793a\u5f53\u524d\u5e27\n                gc.collect()\n    except Exception as e:\n        sys.print_exception(e)\n    finally:\n        sr.hand_det.deinit()\n        sr.hand_kp.deinit()\n        pl.destroy()\n'})}),"\n",(0,i.jsx)(n.h3,{id:"229-\u6587\u672c\u8f6c\u8bed\u97f3\u4e2d\u6587",children:"2.29. \u6587\u672c\u8f6c\u8bed\u97f3\uff08\u4e2d\u6587\uff09"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:'from libs.PipeLine import ScopedTiming\nfrom libs.AIBase import AIBase\nfrom libs.AI2D import Ai2d\nfrom media.pyaudio import *                     # \u97f3\u9891\u6a21\u5757\nfrom media.media import *                       # \u8f6f\u4ef6\u62bd\u8c61\u6a21\u5757\uff0c\u4e3b\u8981\u5c01\u88c5\u5a92\u4f53\u6570\u636e\u94fe\u8def\u4ee5\u53ca\u5a92\u4f53\u7f13\u51b2\u533a\nimport media.wave as wave                       # wav\u97f3\u9891\u5904\u7406\u6a21\u5757\nimport nncase_runtime as nn                     # nncase\u8fd0\u884c\u6a21\u5757\uff0c\u5c01\u88c5\u4e86kpu\uff08kmodel\u63a8\u7406\uff09\u548cai2d\uff08\u56fe\u7247\u9884\u5904\u7406\u52a0\u901f\uff09\u64cd\u4f5c\nimport ulab.numpy as np                         # \u7c7b\u4f3cpython numpy\u64cd\u4f5c\uff0c\u4f46\u4e5f\u4f1a\u6709\u4e00\u4e9b\u63a5\u53e3\u4e0d\u540c\nimport aidemo                                   # aidemo\u6a21\u5757\uff0c\u5c01\u88c5ai demo\u76f8\u5173\u524d\u5904\u7406\u3001\u540e\u5904\u7406\u7b49\u64cd\u4f5c\nimport time                                     # \u65f6\u95f4\u7edf\u8ba1\nimport struct                                   # \u5b57\u8282\u5b57\u7b26\u8f6c\u6362\u6a21\u5757\nimport gc                                       # \u5783\u573e\u56de\u6536\u6a21\u5757\nimport os,sys                                   # \u64cd\u4f5c\u7cfb\u7edf\u63a5\u53e3\u6a21\u5757\n\n# \u81ea\u5b9a\u4e49TTS\u4e2d\u6587\u7f16\u7801\u5668\u7c7b\uff0c\u7ee7\u627f\u81eaAIBase\u57fa\u7c7b\nclass EncoderApp(AIBase):\n    def __init__(self, kmodel_path,dict_path,phase_path,mapfile,debug_mode=0):\n        super().__init__(kmodel_path)  # \u8c03\u7528\u57fa\u7c7b\u7684\u6784\u9020\u51fd\u6570\n        self.kmodel_path = kmodel_path  # \u6a21\u578b\u6587\u4ef6\u8def\u5f84\n        self.debug_mode = debug_mode  # \u662f\u5426\u5f00\u542f\u8c03\u8bd5\u6a21\u5f0f\n        self.ttszh=aidemo.tts_zh_create(dict_path,phase_path,mapfile)\n        self.data=None\n        self.data_len=0\n        self.durition_sum=0\n\n    # \u81ea\u5b9a\u4e49\u7f16\u7801\u5668\u9884\u5904\u7406\uff0c\u8fd4\u56de\u6a21\u578b\u8f93\u5165tensor\u5217\u8868\n    def preprocess(self,text):\n        with ScopedTiming("encoder preprocess", self.debug_mode > 0):\n            preprocess_data=aidemo.tts_zh_preprocess(self.ttszh,text)\n            self.data=preprocess_data[0]\n            self.data_len=preprocess_data[1]\n            # \u521b\u5efa\u7f16\u7801\u5668\u6a21\u578b\u8f93\u5165\u5e76\u548c\u6a21\u578b\u7ed1\u5b9a\uff0c\u7f16\u7801\u5668\u5305\u542b\u4e24\u4e2a\u8f93\u5165\uff0c\u4e00\u4e2a\u662f\u6587\u5b57\u9884\u5904\u7406\u7684\u5e8f\u5217\u6570\u636e\uff0c\u4e00\u4e2a\u662fspeaker\u6570\u636e\n            # \u7f16\u7801\u5668\u5e8f\u5217\u6570\u636e\n            enc_seq_input_tensor = nn.from_numpy(np.array(self.data))\n            # \u7f16\u7801\u5668speaker\u6570\u636e\n            enc_speaker_input_tensor=nn.from_numpy(np.array([0.0]))\n            return [enc_speaker_input_tensor,enc_seq_input_tensor]\n\n    # \u81ea\u5b9a\u4e49\u7f16\u7801\u5668\u7684\u540e\u5904\u7406\uff0cresults\u662f\u6a21\u578b\u8f93\u51fandarray\u5217\u8868,\u7f16\u7801\u5668\u540e\u5904\u7406\u4e5f\u53ef\u4ee5\u89c6\u4e3a\u89e3\u7801\u5668\u7684\u524d\u5904\u7406\n    def postprocess(self, results):\n        with ScopedTiming("encoder postprocess", self.debug_mode > 0):\n            enc_output_0_np=results[0]\n            enc_output_1_np=results[1]\n            # \u7ed9\u7f16\u7801\u7ed3\u679c\u6dfb\u52a0\u6301\u7eed\u65f6\u95f4\u5c5e\u6027\uff0c\u6bcf\u4e2a\u97f3\u7d20\u7f16\u7801\u5411\u91cf\u6309\u7167\u6301\u7eed\u65f6\u95f4\u91cd\u590d\n            duritions=enc_output_1_np[0][:int(self.data_len[0])]\n            self.durition_sum=int(np.sum(duritions))\n            # \u89e3\u7801\u5668\u8f93\u5165\u7ef4\u5ea6\u4e3a\uff081,600,256\uff09,\u4e0d\u8db3\u90e8\u5206\u9700\u8981padding\n            max_value=13\n            while self.durition_sum>600:\n                for i in range(len(duritions)):\n                    if duritions[i]>max_value:\n                        duritions[i]=max_value\n                max_value=max_value-1\n                self.durition_sum=np.sum(duritions)\n            dec_input=np.zeros((1,600,256),dtype=np.float)\n            m_pad=600-self.durition_sum\n            k=0\n            for i in range(len(duritions)):\n                for j in range(int(duritions[i])):\n                    dec_input[0][k]=enc_output_0_np[0][i]\n                    k+=1\n            return dec_input,self.durition_sum\n\n# \u81ea\u5b9a\u4e49TTS\u4e2d\u6587\u89e3\u7801\u5668\u7c7b\uff0c\u7ee7\u627f\u81eaAIBase\u57fa\u7c7b\nclass DecoderApp(AIBase):\n    def __init__(self, kmodel_path, debug_mode=0):\n        super().__init__(kmodel_path)  # \u8c03\u7528\u57fa\u7c7b\u7684\u6784\u9020\u51fd\u6570\n        self.kmodel_path = kmodel_path  # \u6a21\u578b\u6587\u4ef6\u8def\u5f84\n        self.debug_mode = debug_mode  # \u662f\u5426\u5f00\u542f\u8c03\u8bd5\u6a21\u5f0f\n\n    # \u81ea\u5b9a\u4e49\u89e3\u7801\u5668\u9884\u5904\u7406\uff0c\u8fd4\u56de\u6a21\u578b\u8f93\u5165tensor\u5217\u8868\n    def preprocess(self,dec_input):\n        with ScopedTiming("decoder preprocess", self.debug_mode > 0):\n            dec_input_tensor=nn.from_numpy(dec_input)\n            return [dec_input_tensor]\n\n    # \u81ea\u5b9a\u4e49\u89e3\u7801\u5668\u540e\u5904\u7406\uff0cresults\u662f\u6a21\u578b\u8f93\u51fandarray\u5217\u8868\n    def postprocess(self, results):\n        with ScopedTiming("decoder postprocess", self.debug_mode > 0):\n            return results[0]\n\n# \u81ea\u5b9a\u4e49HifiGan\u58f0\u7801\u5668\u7c7b\uff0c\u7ee7\u627f\u81eaAIBase\u57fa\u7c7b\nclass HifiGanApp(AIBase):\n    def __init__(self, kmodel_path, debug_mode=0):\n        super().__init__(kmodel_path)  # \u8c03\u7528\u57fa\u7c7b\u7684\u6784\u9020\u51fd\u6570\n        self.kmodel_path = kmodel_path  # \u6a21\u578b\u6587\u4ef6\u8def\u5f84\n        self.debug_mode = debug_mode  # \u662f\u5426\u5f00\u542f\u8c03\u8bd5\u6a21\u5f0f\n        self.mel_data=[]\n        self.subvector_num=0\n        self.hifi_input=None\n\n    # \u81ea\u5b9a\u4e49\u58f0\u7801\u5668\u9884\u5904\u7406\uff0c\u8fd4\u56de\u6a21\u578b\u8f93\u5165tensor\u5217\u8868\n    def preprocess(self,dec_output_np,durition_sum):\n        with ScopedTiming("hifigan preprocess", self.debug_mode > 0):\n            self.subvector_num=durition_sum//100;\n            remaining=durition_sum%100;\n            if remaining>0:\n                self.subvector_num+=1\n            self.hifi_input=np.zeros((1,80,self.subvector_num*100),dtype=np.float)\n            for i in range(durition_sum):\n                self.hifi_input[:,:,i]=dec_output_np[:,:,i]\n\n    def run(self,dec_output_np,durition_sum):\n        self.preprocess(dec_output_np,durition_sum)\n        # \u4f9d\u6b21\u5bf9\u6bcf\u4e00\u4e2a\u5b50\u5411\u91cf\u8fdb\u884c\u58f0\u7801\u5668\u63a8\u7406\n        for i in range(self.subvector_num):\n            hifi_input_tmp=np.zeros((1,80,100),dtype=np.float)\n            for j in range(80):\n                for k in range(i*100,(i+1)*100):\n                    hifi_input_tmp[0][j][k-i*100]=self.hifi_input[0][j][k]\n            # \u8bbe\u7f6e\u6a21\u578b\u8f93\u5165\n            hifigan_input_tensor=nn.from_numpy(hifi_input_tmp)\n            # \u63a8\u7406\n            results=self.inference([hifigan_input_tensor])\n            self.postprocess(results)\n        return self.mel_data\n\n    # \u81ea\u5b9a\u4e49\u5f53\u524d\u4efb\u52a1\u7684\u540e\u5904\u7406\uff0cresults\u662f\u6a21\u578b\u8f93\u51fandarray\u5217\u8868\n    def postprocess(self, results):\n        with ScopedTiming("hifigan postprocess", self.debug_mode > 0):\n            # \u6c47\u603b\u8f93\u51fa\u6570\u636e\n            for j in range(25600):\n                self.mel_data.append(results[0][0][0][j])\n\n#\u81ea\u5b9a\u4e49\u4e2d\u6587TTS\u4efb\u52a1\u7c7b\nclass TTSZH:\n    def __init__(self,encoder_kmodel_path,decoder_kmodel_path,hifigan_kmodel_path,dict_path,phase_path,mapfile,save_wav_file,debug_mode):\n        self.save_wav_file=save_wav_file\n        self.debug_mode=debug_mode\n        self.encoder=EncoderApp(encoder_kmodel_path,dict_path,phase_path,mapfile,debug_mode)\n        self.decoder=DecoderApp(decoder_kmodel_path,debug_mode)\n        self.hifigan=HifiGanApp(hifigan_kmodel_path,debug_mode)\n\n    def run(self,text):\n        encoder_output_0,encoder_output_1=self.encoder.run(text)\n        decoder_output_0=self.decoder.run(encoder_output_0)\n        hifigan_output=self.hifigan.run(decoder_output_0,encoder_output_1)\n        # \u5c06\u751f\u6210\u7684\u97f3\u9891\u6570\u636e\u4fdd\u5b58\u4e3awav\u6587\u4ef6\n        save_data=hifigan_output[:encoder_output_1*256]\n        save_len=len(save_data)\n        aidemo.save_wav(save_data,save_len,self.save_wav_file,24000)\n        self.play_audio()\n\n    def play_audio(self):\n        with ScopedTiming("play audio", self.debug_mode > 0):\n            # \u6709\u5173\u97f3\u9891\u6d41\u7684\u5b8f\u53d8\u91cf\n            SAMPLE_RATE = 24000         # \u91c7\u6837\u738724000Hz,\u5373\u6bcf\u79d2\u91c7\u683724000\u6b21\n            CHANNELS = 1                # \u901a\u9053\u6570 1\u4e3a\u5355\u58f0\u9053\uff0c2\u4e3a\u7acb\u4f53\u58f0\n            FORMAT = paInt16            # \u97f3\u9891\u8f93\u5165\u8f93\u51fa\u683c\u5f0f paInt16\n            CHUNK = int(0.3 * 24000)    # \u6bcf\u6b21\u8bfb\u53d6\u97f3\u9891\u6570\u636e\u7684\u5e27\u6570\uff0c\u8bbe\u7f6e\u4e3a0.3s\u7684\u5e27\u657024000*0.3=7200\n            # \u521d\u59cb\u5316\u97f3\u9891\u6d41\n            p = PyAudio()\n            p.initialize(CHUNK)\n            ret = MediaManager.init()\n            if ret:\n                print("record_audio, buffer_init failed")\n            # \u7528\u4e8e\u64ad\u653e\u97f3\u9891\n            output_stream = p.open(format=FORMAT,channels=CHANNELS,rate=SAMPLE_RATE,output=True,frames_per_buffer=CHUNK)\n            wf = wave.open(self.save_wav_file, "rb")\n            wav_data = wf.read_frames(CHUNK)\n            while wav_data:\n                output_stream.write(wav_data)\n                wav_data = wf.read_frames(CHUNK)\n            time.sleep(2) # \u65f6\u95f4\u7f13\u51b2\uff0c\u7528\u4e8e\u64ad\u653e\u58f0\u97f3\n            wf.close()\n            output_stream.stop_stream()\n            output_stream.close()\n            p.terminate()\n            MediaManager.deinit()\n\n    def deinit(self):\n        aidemo.tts_zh_destroy(self.encoder.ttszh)\n        tts_zh.encoder.deinit()\n        tts_zh.decoder.deinit()\n        tts_zh.hifigan.deinit()\n\nif __name__ == "__main__":\n    os.exitpoint(os.EXITPOINT_ENABLE)\n    nn.shrink_memory_pool()\n    # \u8bbe\u7f6e\u6a21\u578b\u8def\u5f84\u548c\u5176\u4ed6\u53c2\u6570\n    # \u4e2d\u6587tts encoder\u6a21\u578b\n    encoder_kmodel_path = "/sdcard/app/tests/kmodel/zh_fastspeech_1_f32.kmodel"\n    # \u4e2d\u6587tts decoder\u6a21\u578b\n    decoder_kmodel_path = "/sdcard/app/tests/kmodel/zh_fastspeech_2.kmodel"\n    # \u4e2d\u6587tts \u58f0\u7801\u5668\u6a21\u578b\n    hifigan_kmodel_path="/sdcard/app/tests/kmodel/hifigan.kmodel"\n    # \u62fc\u97f3\u5b57\u5178\n    dict_path="/sdcard/app/tests/utils/pinyin.txt"\n    # \u6c49\u5b57\u8f6c\u62fc\u97f3\u5b57\u5178\u6587\u4ef6\n    phase_path="/sdcard/app/tests/utils/small_pinyin.txt"\n    # \u62fc\u97f3\u8f6c\u97f3\u7d20\u6620\u5c04\u6587\u4ef6\n    mapfile="/sdcard/app/tests/utils/phone_map.txt"\n    # \u8f93\u5165\u4e2d\u6587\u8bed\u53e5\n    text="\u5609\u6960\u79d1\u6280\u7814\u53d1\u4e86\u6700\u65b0\u6b3e\u7684\u82af\u7247"\n    # \u751f\u6210\u97f3\u9891\u5b58\u50a8\u8def\u5f84\n    save_wav_file = "/sdcard/app/tests/test.wav"\n\n    # \u521d\u59cb\u5316\u81ea\u5b9a\u4e49\u4e2d\u6587tts\u5b9e\u4f8b\n    tts_zh = TTSZH(encoder_kmodel_path,decoder_kmodel_path,hifigan_kmodel_path,dict_path,phase_path,mapfile,save_wav_file,debug_mode=0)\n    try:\n        with ScopedTiming("total",1):\n            tts_zh.run(text)\n            gc.collect()                        # \u5783\u573e\u56de\u6536\n    except Exception as e:\n        sys.print_exception(e)                  # \u6253\u5370\u5f02\u5e38\u4fe1\u606f\n    finally:\n        tts_zh.deinit()\n'})})]})}function o(e={}){const{wrapper:n}={...(0,_.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(a,{...e})}):a(e)}},24357:(e,n,s)=>{s.d(n,{A:()=>i});const i=s.p+"assets/images/framework-1721614688998-7-6993c6cf5f43b4b94b3cfcf42ae2d34a.png"},81546:(e,n,s)=>{s.d(n,{A:()=>i});const i=s.p+"assets/images/task_diff-1721614741114-10-fd8b2becab43c264215f15e3d211f510.png"},28453:(e,n,s)=>{s.d(n,{R:()=>d,x:()=>l});var i=s(96540);const _={},t=i.createContext(_);function d(e){const n=i.useContext(t);return i.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(_):e.components||_:d(e.components),i.createElement(t.Provider,{value:n},e.children)}}}]);